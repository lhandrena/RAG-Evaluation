# Aufgabe 3.2: Prompt-Optimierung

## Lernziele
- Verstehen des Einflusses von Prompt-Qualit√§t auf die RAG-Performance
- Wechselwirkung von Prompt-Qualit√§t und Modellgr√∂√üe analysieren

## Voraussetzungen
‚úÖ Aufgabe 3.1 (Modell-Optimierung) wurde abgeschlossen

## Teil 1: Prompt-Vergleich mit starkem Modell

### Schritt 1: Aktuellen Prompt analysieren
√ñffne die Datei [`augmented_generation_prompt.py`](../src/advanced_rag/backend/nodes/prompt_templates/augmented_generation_prompt.py):

```python
@staticmethod
def get_prompt() -> str:
    prompt_file = Path(__file__).parent / "generation_prompt_bad.txt"
    #prompt_file = Path(__file__).parent / "generation_prompt_good.txt"
    return prompt_file.read_text().strip()
```

Aktuell wird der "bad" Prompt verwendet. Schaue dir beide Prompts an:
- [`generation_prompt_bad.txt`](../src/advanced_rag/backend/nodes/prompt_templates/generation_prompt_bad.txt)
- [`generation_prompt_good.txt`](../src/advanced_rag/backend/nodes/prompt_templates/generation_prompt_good.txt)

**Reflexionsfrage:** Was sind die Hauptunterschiede zwischen den beiden Prompts?

### Schritt 2: Auf "good" Prompt wechseln
√Ñndere in [`augmented_generation_prompt.py`](../src/advanced_rag/backend/nodes/prompt_templates/augmented_generation_prompt.py):

```python
@staticmethod
def get_prompt() -> str:
    #prompt_file = Path(__file__).parent / "generation_prompt_bad.txt"
    prompt_file = Path(__file__).parent / "generation_prompt_good.txt"
    return prompt_file.read_text().strip()
```

Und w√§hle als Chat-Modell `gpt-5`.

### Schritt 3: Backend neu starten und evaluieren
- Backend neu starten
- Evaluation durchf√ºhren (evaluate_dataset.py)

### Schritt 4: Metriken vergleichen
Analysiere in Langfuse:
- Wie haben sich die Metriken durch den besseren Prompt ver√§ndert?
- Welche Metriken profitieren am meisten?
- Gibt es √ºberraschende Verschlechterungen?

## Teil 2: Prompt-Impact bei schw√§cherem Modell

### Schritt 5: Auf schw√§cheres Modell wechseln
√ñffne die [`.env`](../.env) Datei und wechsel auf `gpt-4.1-nano`

### Schritt 6: Erneute Evaluation mit gutem Prompt
- Backend neu starten
- Evaluation durchf√ºhren (evaluate_dataset.py)

### Schritt 7: Prompt-Impact vergleichen
Vergleiche in Langfuse die folgenden Konfigurationen:
1. **gpt-5 + bad prompt** (aus Aufgabe 3.1)
2. **gpt-5 + good prompt** (aus Teil 1)
3. **gpt-4.1-nano + good prompt** (aktuell)

**Analysefragen:**
- Wie gro√ü ist der Prompt-Impact bei gpt-5?
- Wie gro√ü ist der Prompt-Impact bei gpt-4.1-nano?

### Reflexionsfragen
1. Warum kann ein kleineres Modell mit besserem Prompt ein gr√∂√üeres Modell mit schlechtem Prompt √ºbertreffen?
2. Welche Rolle spielt die Prompt-Qualit√§t im Vergleich zur Modellgr√∂√üe?
3. Wann lohnt sich der Einsatz eines gr√∂√üeren Modells?

## Teil 3 (Optional): Rules aus dem Prompt entfernen

- √ñffne [`generation_prompt_good.txt`](../src/advanced_rag/backend/nodes/prompt_templates/generation_prompt_good.txt) und entferne die "Rules" am Ende des Prompts.
- Vergleiche gpt-5 mit und ohne "Rules"?


## Fazit: Erkenntnisse aus Aufgabe 3.1 und 3.2

### Modell-Optimierung (3.1)
- ‚úÖ Modelle (LLM & Embedding-Modell) zu variieren ist technisch einfach
- ‚ö†Ô∏è Gr√∂√üere LLMs sind nicht immer besser
- ‚úÖ Bei Embedding-Modellen lohnen sich gr√∂√üere Modelle meistens
- üí° LLMs bei RAG "formulieren" haupts√§chlich um - nicht √ºbertreiben

### Prompt-Optimierung (3.2)
- ‚úÖ Prompts zu variieren ist technisch einfach
- ‚ö†Ô∏è Impact ist oft begrenzt, aber messbar
- üí° Gute Prompts k√∂nnen schw√§chere Modelle kompensieren
- ‚ö†Ô∏è Bei Prompts nicht √ºbertreiben - Einfachheit ist oft besser

### Wichtige Erkenntnisse
1. **Kosten-Nutzen-Abw√§gung:** Gr√∂√üere Modelle kosten mehr, bringen aber nicht immer proportional bessere Ergebnisse
2. **LLM vs. Retrieval:** Die Retrieval-Qualit√§t ist oft wichtiger als das LLM
