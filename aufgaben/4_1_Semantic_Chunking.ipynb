{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chunking Strategien - Einfache Erklärung\n",
    "\n",
    "Dieses Notebook zeigt die drei verschiedenen Chunking-Strategien, die in unserem RAG-System verwendet werden:\n",
    "\n",
    "1. **Rekursives Chunking**: Teilt Text nach Zeichenanzahl mit Überlappung\n",
    "2. **Embedding-basiertes semantisches Chunking**: Teilt Text basierend auf semantischer Ähnlichkeit zwischen Satz-Embeddings\n",
    "3. **LLM-basiertes semantisches Chunking**: Nutzt ein LLM, um sinnvolle Abschnitte zu identifizieren\n",
    "\n",
    "Wir verwenden einen einfachen Beispieltext über Künstliche Intelligenz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beispiel_text = \"\"\"\n",
    "Künstliche Intelligenz (KI) ist ein Teilgebiet der Informatik. Sie befasst sich mit der Automatisierung intelligenten Verhaltens.\n",
    "Machine Learning ist eine wichtige Methode der KI. Dabei lernen Computer aus Daten, ohne explizit programmiert zu werden.\n",
    "Es gibt verschiedene Arten: überwachtes Lernen, unüberwachtes Lernen und verstärkendes Lernen. Deep Learning ist eine spezielle Form des Machine Learning.\n",
    "Es verwendet neuronale Netze mit vielen Schichten.\n",
    "Diese Netze können komplexe Muster in großen Datenmengen erkennen. Besonders erfolgreich ist Deep Learning in der Bildverarbeitung und Spracherkennung. Large Language Models (LLMs) sind eine neuere Entwicklung.\n",
    "Sie basieren auf der Transformer-Architektur.\n",
    "Modelle wie GPT können Text generieren, übersetzen und Fragen beantworten. Sie werden mit riesigen Textmengen trainiert.\n",
    "Die Anwendungen von KI sind vielfältig.\n",
    "In der Medizin hilft KI bei der Diagnose von Krankheiten. In der Industrie optimiert sie Produktionsprozesse. Im Alltag nutzen wir KI in Sprachassistenten und Empfehlungssystemen.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Embedding-basiertes semantisches Chunking\n",
    "\n",
    "**Was ist embedding-basiertes semantisches Chunking?**\n",
    "\n",
    "Diese Methode teilt Text basierend auf **semantischer Ähnlichkeit** statt auf Zeichenanzahl. Der Algorithmus:\n",
    "\n",
    "1. Berechnet **Embeddings** für jeden Satz\n",
    "2. Vergleicht die **Ähnlichkeit** aufeinanderfolgender Sätze\n",
    "3. Erstellt eine **Grenze** (neuer Chunk), wenn die Ähnlichkeit stark sinkt\n",
    "\n",
    "**Wichtige Parameter:**\n",
    "\n",
    "- `breakpoint_threshold_type`: Wie wird entschieden, wann ein neuer Chunk beginnt?\n",
    "  - **\"percentile\"**: Nutzt Perzentile der Ähnlichkeiten (z.B. 50 = Median)\n",
    "  - **\"standard_deviation\"**: Nutzt Standardabweichung (z.B. 1.5σ)\n",
    "  - **\"interquartile\"**: Nutzt Interquartilsabstand (robuster gegen Ausreißer)\n",
    "  \n",
    "- `breakpoint_threshold_amount`: Der Schwellenwert\n",
    "  - Bei \"percentile\": 50 = Median, 75 = oberes Quartil\n",
    "  - Bei \"standard_deviation\": 1.0 = 1σ, 2.0 = 2σ\n",
    "  - Höhere Werte = weniger Chunks (nur bei sehr großen Unterschieden trennen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "import os\n",
    "\n",
    "def test_semantic_chunking(threshold_type=\"percentile\", threshold_amount=50):\n",
    "    embedding_model = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "    # Erstelle Semantic Chunker\n",
    "    semantic_splitter = None\n",
    "\n",
    "    semantic_chunks = semantic_splitter.split_text(beispiel_text)\n",
    "\n",
    "    print(f\"Anzahl der Chunks: {len(semantic_chunks)}\")\n",
    "    print(f\"Parameter: threshold_type='{threshold_type}', threshold_amount={threshold_amount}\\n\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    for i, chunk in enumerate(semantic_chunks, 1):\n",
    "        print(f\"\\nChunk {i} ({len(chunk)} Zeichen):\")\n",
    "        print(\"-\" * 80)\n",
    "        print(chunk)\n",
    "        print(\"-\" * 80)\n",
    "\n",
    "# Standard-Einstellungen\n",
    "test_semantic_chunking()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beobachtung:\n",
    "\n",
    "Die Chunks folgen thematischen Grenzen. Zum Beispiel bleiben alle Sätze über \"Machine Learning\" zusammen, auch wenn der Chunk größer wird."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Übungen: Semantic Chunking verstehen und optimieren\n",
    "\n",
    "### Aufgabe 1: Threshold-Type verstehen\n",
    "\n",
    "Experimentiere mit verschiedenen `breakpoint_threshold_type` Optionen und beobachte, wie sich die Anzahl und Größe der Chunks verändert."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experiment 1: Percentile mit niedrigem Wert (25. Perzentil)\n",
    "\n",
    "Was passiert, wenn wir einen niedrigen Perzentil-Wert verwenden? Das bedeutet, wir trennen bereits bei relativ kleinen Ähnlichkeitsunterschieden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Teste mit niedrigem Perzentil (mehr Chunks)\n",
    "test_semantic_chunking(threshold_type=\"percentile\", threshold_amount=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experiment 2: Percentile mit hohem Wert (75. Perzentil)\n",
    "\n",
    "Was passiert, wenn wir einen hohen Perzentil-Wert verwenden? Das bedeutet, wir trennen nur bei sehr großen Ähnlichkeitsunterschieden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Teste mit hohem Perzentil (weniger Chunks)\n",
    "test_semantic_chunking(threshold_type=\"percentile\", threshold_amount=75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Teile Text in Sätze auf (vereinfacht - in echt komplexer)\n",
    "sentences = [s.strip() for s in beispiel_text.split('\\n') if s.strip()]\n",
    "\n",
    "# Erstelle Embeddings für alle Sätze\n",
    "embedding_model = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "embeddings = embedding_model.embed_documents(sentences)\n",
    "\n",
    "# Berechne Ähnlichkeiten zwischen aufeinanderfolgenden Sätzen\n",
    "similarities = []\n",
    "for i in range(len(embeddings) - 1):\n",
    "    sim = cosine_similarity([embeddings[i]], [embeddings[i+1]])[0][0]\n",
    "    similarities.append(sim)\n",
    "\n",
    "# Visualisiere\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.plot(range(len(similarities)), similarities, marker='o', linewidth=2, markersize=8)\n",
    "plt.axhline(y=np.percentile(similarities, 50), color='r', linestyle='--', label='50. Perzentil (Median)')\n",
    "plt.axhline(y=np.percentile(similarities, 25), color='orange', linestyle='--', label='25. Perzentil')\n",
    "plt.axhline(y=np.percentile(similarities, 75), color='green', linestyle='--', label='75. Perzentil')\n",
    "plt.xlabel('Satz-Paar Index', fontsize=12)\n",
    "plt.ylabel('Cosine Similarity', fontsize=12)\n",
    "plt.title('Semantische Ähnlichkeit zwischen aufeinanderfolgenden Sätzen', fontsize=14)\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nStatistiken:\")\n",
    "print(f\"Min Ähnlichkeit: {min(similarities):.3f}\")\n",
    "print(f\"Max Ähnlichkeit: {max(similarities):.3f}\")\n",
    "print(f\"Median (50. Perzentil): {np.percentile(similarities, 50):.3f}\")\n",
    "print(f\"25. Perzentil: {np.percentile(similarities, 25):.3f}\")\n",
    "print(f\"75. Perzentil: {np.percentile(similarities, 75):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aufgabe 2: Visualisierung der semantischen Ähnlichkeiten\n",
    "\n",
    "Um besser zu verstehen, wie Semantic Chunking funktioniert, können wir die Ähnlichkeiten zwischen aufeinanderfolgenden Sätzen visualisieren. Die **Täler** in der Grafik zeigen, wo das Chunking Grenzen setzt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Teste mit hohem Perzentil (weniger Chunks)\n",
    "test_semantic_chunking(threshold_type=\"percentile\", threshold_amount=75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. LLM-basiertes semantisches Chunking\n",
    "\n",
    "**Was ist der Unterschied zwischen embedding-basiertem und LLM-basiertem semantischem Chunking?**\n",
    "\n",
    "Beide Methoden nutzen semantische Information, aber auf unterschiedliche Weise:\n",
    "\n",
    "### Embedding-basiertes semantisches Chunking:\n",
    "- **Wie:** Berechnet mathematische Vektoren (Embeddings) für Sätze und vergleicht deren Ähnlichkeit\n",
    "- **Entscheidung:** Automatisch basierend auf Ähnlichkeitsschwellenwerten\n",
    "- **Vorteile:** \n",
    "  - Schnell und deterministisch\n",
    "  - Keine API-Kosten pro Chunking-Vorgang\n",
    "  - Gut für große Textmengen\n",
    "- **Nachteile:**\n",
    "  - Nur mathematische Ähnlichkeit, kein tiefes Verständnis\n",
    "  - Kann wichtige thematische Übergänge verpassen\n",
    "  - Keine Berücksichtigung von Kontext oder Hierarchien\n",
    "\n",
    "### LLM-basiertes semantisches Chunking:\n",
    "- **Wie:** Ein Large Language Model analysiert den Text und identifiziert thematische Abschnitte\n",
    "- **Entscheidung:** Intelligente Analyse durch das LLM basierend auf Sprach- und Kontextverständnis\n",
    "- **Vorteile:**\n",
    "  - Versteht Kontext, Hierarchien und implizite Zusammenhänge\n",
    "  - Kann Überschriften/Themen generieren\n",
    "  - Berücksichtigt Dokumentstruktur\n",
    "- **Nachteile:**\n",
    "  - Langsamer (API-Aufrufe)\n",
    "  - Teurer (LLM-Nutzung)\n",
    "  - Nicht deterministisch (kann bei gleichen Input leicht variieren)\n",
    "\n",
    "**Wann welche Methode?**\n",
    "- **Embedding-basiert**: Große Datenmengen, Geschwindigkeit wichtig, einfache Texte\n",
    "- **LLM-basiert**: Komplexe Dokumente, Qualität wichtiger als Geschwindigkeit, strukturierte Ausgabe gewünscht"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Vereinfachtes Beispiel - In der echten Implementierung komplexer\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "# Prompt für das LLM\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Du bist ein Experte für Textanalyse. Teile den folgenden Text in thematisch zusammenhängende Abschnitte auf.\"),\n",
    "    (\"user\", \"\"\"Analysiere diesen Text und teile ihn in 3-5 thematische Abschnitte auf.\n",
    "    \n",
    "Gib für jeden Abschnitt die Überschrift und den zugehörigen Text zurück.\n",
    "\n",
    "Format:\n",
    "ABSCHNITT 1: [Überschrift]\n",
    "[Text des Abschnitts]\n",
    "\n",
    "ABSCHNITT 2: [Überschrift]\n",
    "[Text des Abschnitts]\n",
    "\n",
    "...\n",
    "\n",
    "Text:\n",
    "{text}\"\"\")\n",
    "])\n",
    "\n",
    "chain = prompt | llm\n",
    "result = chain.invoke({\"text\": beispiel_text})\n",
    "\n",
    "print(\"LLM-basierte Chunking-Analyse:\")\n",
    "print(\"=\"*80)\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beobachtung:\n",
    "\n",
    "Das LLM erkennt nicht nur thematische Grenzen, sondern kann auch:\n",
    "- Hierarchien verstehen (z.B. \"Deep Learning ist Teil von Machine Learning\")\n",
    "- Kontextabhängige Entscheidungen treffen\n",
    "- Überschriften/Themen für Chunks generieren"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aufgabe 3: Vergleich der Chunking-Strategien\n",
    "\n",
    "Nachdem du die beiden semantischen Chunking-Methoden kennengelernt hast, möchtest du herausfinden, welche Strategie die bessere Retrieval-Qualität für dein RAG-System liefert: **Embedding-basiertes** oder **LLM-basiertes semantisches Chunking**.\n",
    "\n",
    "### Schritt-für-Schritt Anleitung zur Evaluation\n",
    "\n",
    "#### 1. Embedding-basiertes semantisches Chunking evaluieren\n",
    "\n",
    "Öffne die `.env` Datei und setze:\n",
    "\n",
    "```bash\n",
    "CHUNKING_STRATEGY=SEMANTIC\n",
    "SEMANTIC_BREAKPOINT_TYPE=percentile\n",
    "SEMANTIC_BREAKPOINT_AMOUNT=50\n",
    "```\n",
    "\n",
    "#### 2. Backend neu starten\n",
    "\n",
    "Damit die neuen Chunking-Parameter wirksam werden, muss das Backend neugestartet werden:\n",
    "\n",
    "```bash\n",
    "uv run --env-file .env python -m src.advanced_rag.backend.main\n",
    "```\n",
    "\n",
    "**Hinweis:** Das Backend muss laufen bleiben während der Evaluation!\n",
    "\n",
    "#### 3. Evaluation-Dataset ausführen\n",
    "\n",
    "Öffne ein neues Terminal (das Backend läuft im ersten Terminal) und führe aus:\n",
    "\n",
    "```bash\n",
    "uv run --env-file .env python src/advanced_rag/evaluation/evaluate_dataset.py\n",
    "```\n",
    "\n",
    "Dies führt alle Test-Queries gegen dein RAG-System aus und speichert die Ergebnisse in Langfuse.\n",
    "\n",
    "#### 4. Ergebnisse in Langfuse notieren\n",
    "\n",
    "1. Öffne das Langfuse Dashboard (URL aus deiner `.env` Datei: `LANGFUSE_HOST`)\n",
    "2. Navigiere zum entsprechenden Projekt\n",
    "3. Suche nach dem neuesten Evaluation-Run\n",
    "4. Notiere die Metriken:\n",
    "   - **Context Precision**: Wie relevant sind die abgerufenen Chunks?\n",
    "   - **Context Recall**: Werden alle relevanten Informationen gefunden?\n",
    "   - **Answer Relevancy**: Wie gut beantwortet das System die Frage?\n",
    "\n",
    "#### 5. LLM-basiertes semantisches Chunking evaluieren\n",
    "\n",
    "Wiederhole die Schritte 1-4 mit der LLM-basierten Strategie:\n",
    "\n",
    "In der `.env` Datei setze:\n",
    "\n",
    "```bash\n",
    "CHUNKING_STRATEGY=SEMANTIC_LLM\n",
    "```\n",
    "\n",
    "Backend neu starten und Evaluation erneut durchführen.\n",
    "\n",
    "#### 6. Ergebnisse vergleichen\n",
    "\n",
    "**Embedding-basiertes semantisches Chunking:**\n",
    "- Schnell und kostengünstig\n",
    "- Basiert auf Ähnlichkeit zwischen Satz-Embeddings\n",
    "- Gut für große Datenmengen\n",
    "\n",
    "**LLM-basiertes semantisches Chunking:**\n",
    "- Langsamer und teurer (nutzt LLM-API)\n",
    "- Besseres Kontextverständnis\n",
    "- Kann thematische Zusammenhänge und Hierarchien erkennen\n",
    "\n",
    "**Notiere dir die Scores:**\n",
    "\n",
    "| Strategie | Context Precision | Context Recall | Answer Relevancy |\n",
    "|-----------|-------------------|----------------|------------------|\n",
    "| Embedding-basiert  | ?                 | ?              | ?                |\n",
    "| LLM-basiert | ?              | ?              | ?                |\n",
    "\n",
    "**Frage zum Nachdenken:** Rechtfertigt die bessere Qualität des LLM-basierten Ansatzes die höheren Kosten und längere Verarbeitungszeit für deinen Use-Case?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "advanced-rag (3.12.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
