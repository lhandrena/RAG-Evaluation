# Source: https://x.com/techNmak/status/1980156741066133683?t=RGBXF-br-TSAGLqWa71ybg&s=19

# Tech with Mak on X: "Your Vector RAG Blueprint. Here’s a clear 9-step pipeline to build a modern Vector RAG system from scratch. 1./ 𝐈𝐧𝐠𝐞𝐬𝐭 &amp; 𝐏𝐫𝐞𝐩𝐫𝐨𝐜𝐞𝐬𝐬 𝐃𝐚𝐭𝐚 ➞ Start with tools like web scraping libraries/services (e.g., Firecrawl), data connectors (e.g., for databases, APIs), https://t.co/bkJHoGOWvL" / X

PostSee new postsConversationTech with Mak@techNmakYour Vector RAG Blueprint.

Here’s a clear 9-step pipeline to build a modern Vector RAG system from scratch.

1./ 𝐈𝐧𝐠𝐞𝐬𝐭 & 𝐏𝐫𝐞𝐩𝐫𝐨𝐜𝐞𝐬𝐬 𝐃𝐚𝐭𝐚
➞ Start with tools like web scraping libraries/services (e.g., Firecrawl), data connectors (e.g., for databases, APIs), or dedicated ingestion and preprocessing platforms (e.g., Unstructured(.)io) to collect and clean your data before chunking or embedding begins.

2./ 𝐒𝐩𝐥𝐢𝐭 𝐈𝐧𝐭𝐨 𝐂𝐡𝐮𝐧𝐤𝐬
➞ Use libraries like LangChain or LlamaIndex to break documents into manageable, meaningful pieces, essential for context preservation and optimal retrieval.
➞ Consider various chunking strategies (e.g., fixed-size, semantic, recursive).

3./ 𝐆𝐞𝐧𝐞𝐫𝐚𝐭𝐞 𝐄𝐦𝐛𝐞𝐝𝐝𝐢𝐧𝐠𝐬
➞ Transform your chunks into dense vector representations using state-of-the-art embedding models like text-embedding-ada-002, Cohere Embed v3, BGE-M3, or llama-text-embed-v2.

4./ 𝐒𝐭𝐨𝐫𝐞 𝐢𝐧 𝐕𝐞𝐜𝐭𝐨𝐫 𝐃𝐁 & 𝐈𝐧𝐝𝐞𝐱
➞ Store vectors in specialized vector databases like Pinecone, Weaviate, Qdrant, Milvus - created by Zilliz, or pgvector.
➞ You can also use traditional databases like Elastic MongoDB for document storage, leveraging their vector search capabilities if available and suitable.

5./ 𝐑𝐞𝐭𝐫𝐢𝐞𝐯𝐞 𝐈𝐧𝐟𝐨𝐫𝐦𝐚𝐭𝐢𝐨𝐧
➞ Retrieve relevant context using dense vector search (similarity search), sparse retrieval (e.g., BM25, SPLADE), or sophisticated hybrid fusion methods (e.g., RRF, reciprocal rank fusion) via frameworks like LangChain, LlamaIndex, or Haystack. Implement re-ranking (e.g., using bge-reranker or Cohere Rerank) for improved precision.

6./ 𝐎𝐫𝐜𝐡𝐞𝐬𝐭𝐫𝐚𝐭𝐞 𝐭𝐡𝐞 𝐏𝐢𝐩𝐞𝐥𝐢𝐧𝐞
➞ Build your workflow and manage the flow of information between components using orchestration frameworks like LangChain, LlamaIndex, or dedicated workflow automation platforms like n8n or cloud services like Google Cloud Vertex AI Pipelines.

7./ 𝐒𝐞𝐥𝐞𝐜𝐭 𝐋𝐋𝐌𝐬 𝐟𝐨𝐫 𝐆𝐞𝐧𝐞𝐫𝐚𝐭𝐢𝐨𝐧
➞ Integrate your preferred Large Language Models (LLMs) such as Claude, GPT (e.g., GPT-4o), Gemini, Llama 3, DeepSeek, or Mistral via direct APIs or through AI gateways and routing services like Portkey, Eden, or OpenRouter for consistent access and management.

8./ 𝐀𝐝𝐝 𝐎𝐛𝐬𝐞𝐫𝐯𝐚𝐛𝐢𝐥𝐢𝐭𝐲
➞ Monitor and troubleshoot your RAG system using dedicated observability platforms like Langfuse, PromptLayer, Helicone (YC W23), or Arize AI to track prompt performance, token usage, latency, system health, and model outputs.

9./ 𝐄𝐯𝐚𝐥𝐮𝐚𝐭𝐞 & 𝐈𝐦𝐩𝐫𝐨𝐯𝐞
➞ Continuously test and refine retrieval and generation outputs using automated evaluation metrics (e.g., faithfulness, answer relevance, context recall/precision), A/B tests, human feedback loops, and fine-tuning (if necessary) for better quality and performance.

This workflow breaks down every stage of a successful Vector RAG pipeline.

Save this guide, it’s your starting point.

----

Follow@techNmakLast editedOpens edit history8:18 AM · Oct 20, 2025·38KViews51177773784Read 51 repliesNew to X?Sign up now to get your own personalized timeline!Sign up with AppleCreate accountBy signing up, you agree to theTerms of ServiceandPrivacy Policy, includingCookie Use.Trending nowWhat’s happeningTrending in GermanyBäume2,250 postsTrendingGovernance39.4K postsTrendingToken209K postsBusiness & finance · Trending#NemesisShow more

PostSee new postsConversationTech with Mak@techNmakYour Vector RAG Blueprint.

Here’s a clear 9-step pipeline to build a modern Vector RAG system from scratch.

1./ 𝐈𝐧𝐠𝐞𝐬𝐭 & 𝐏𝐫𝐞𝐩𝐫𝐨𝐜𝐞𝐬𝐬 𝐃𝐚𝐭𝐚
➞ Start with tools like web scraping libraries/services (e.g., Firecrawl), data connectors (e.g., for databases, APIs), or dedicated ingestion and preprocessing platforms (e.g., Unstructured(.)io) to collect and clean your data before chunking or embedding begins.

2./ 𝐒𝐩𝐥𝐢𝐭 𝐈𝐧𝐭𝐨 𝐂𝐡𝐮𝐧𝐤𝐬
➞ Use libraries like LangChain or LlamaIndex to break documents into manageable, meaningful pieces, essential for context preservation and optimal retrieval.
➞ Consider various chunking strategies (e.g., fixed-size, semantic, recursive).

3./ 𝐆𝐞𝐧𝐞𝐫𝐚𝐭𝐞 𝐄𝐦𝐛𝐞𝐝𝐝𝐢𝐧𝐠𝐬
➞ Transform your chunks into dense vector representations using state-of-the-art embedding models like text-embedding-ada-002, Cohere Embed v3, BGE-M3, or llama-text-embed-v2.

4./ 𝐒𝐭𝐨𝐫𝐞 𝐢𝐧 𝐕𝐞𝐜𝐭𝐨𝐫 𝐃𝐁 & 𝐈𝐧𝐝𝐞𝐱
➞ Store vectors in specialized vector databases like Pinecone, Weaviate, Qdrant, Milvus - created by Zilliz, or pgvector.
➞ You can also use traditional databases like Elastic MongoDB for document storage, leveraging their vector search capabilities if available and suitable.

5./ 𝐑𝐞𝐭𝐫𝐢𝐞𝐯𝐞 𝐈𝐧𝐟𝐨𝐫𝐦𝐚𝐭𝐢𝐨𝐧
➞ Retrieve relevant context using dense vector search (similarity search), sparse retrieval (e.g., BM25, SPLADE), or sophisticated hybrid fusion methods (e.g., RRF, reciprocal rank fusion) via frameworks like LangChain, LlamaIndex, or Haystack. Implement re-ranking (e.g., using bge-reranker or Cohere Rerank) for improved precision.

6./ 𝐎𝐫𝐜𝐡𝐞𝐬𝐭𝐫𝐚𝐭𝐞 𝐭𝐡𝐞 𝐏𝐢𝐩𝐞𝐥𝐢𝐧𝐞
➞ Build your workflow and manage the flow of information between components using orchestration frameworks like LangChain, LlamaIndex, or dedicated workflow automation platforms like n8n or cloud services like Google Cloud Vertex AI Pipelines.

7./ 𝐒𝐞𝐥𝐞𝐜𝐭 𝐋𝐋𝐌𝐬 𝐟𝐨𝐫 𝐆𝐞𝐧𝐞𝐫𝐚𝐭𝐢𝐨𝐧
➞ Integrate your preferred Large Language Models (LLMs) such as Claude, GPT (e.g., GPT-4o), Gemini, Llama 3, DeepSeek, or Mistral via direct APIs or through AI gateways and routing services like Portkey, Eden, or OpenRouter for consistent access and management.

8./ 𝐀𝐝𝐝 𝐎𝐛𝐬𝐞𝐫𝐯𝐚𝐛𝐢𝐥𝐢𝐭𝐲
➞ Monitor and troubleshoot your RAG system using dedicated observability platforms like Langfuse, PromptLayer, Helicone (YC W23), or Arize AI to track prompt performance, token usage, latency, system health, and model outputs.

9./ 𝐄𝐯𝐚𝐥𝐮𝐚𝐭𝐞 & 𝐈𝐦𝐩𝐫𝐨𝐯𝐞
➞ Continuously test and refine retrieval and generation outputs using automated evaluation metrics (e.g., faithfulness, answer relevance, context recall/precision), A/B tests, human feedback loops, and fine-tuning (if necessary) for better quality and performance.

This workflow breaks down every stage of a successful Vector RAG pipeline.

Save this guide, it’s your starting point.

----

Follow@techNmakLast editedOpens edit history8:18 AM · Oct 20, 2025·38KViews51177773784Read 51 repliesNew to X?Sign up now to get your own personalized timeline!Sign up with AppleCreate accountBy signing up, you agree to theTerms of ServiceandPrivacy Policy, includingCookie Use.Trending nowWhat’s happeningTrending in GermanyBäume2,250 postsTrendingGovernance39.4K postsTrendingToken209K postsBusiness & finance · Trending#NemesisShow more

PostSee new postsConversationTech with Mak@techNmakYour Vector RAG Blueprint.

Here’s a clear 9-step pipeline to build a modern Vector RAG system from scratch.

1./ 𝐈𝐧𝐠𝐞𝐬𝐭 & 𝐏𝐫𝐞𝐩𝐫𝐨𝐜𝐞𝐬𝐬 𝐃𝐚𝐭𝐚
➞ Start with tools like web scraping libraries/services (e.g., Firecrawl), data connectors (e.g., for databases, APIs), or dedicated ingestion and preprocessing platforms (e.g., Unstructured(.)io) to collect and clean your data before chunking or embedding begins.

2./ 𝐒𝐩𝐥𝐢𝐭 𝐈𝐧𝐭𝐨 𝐂𝐡𝐮𝐧𝐤𝐬
➞ Use libraries like LangChain or LlamaIndex to break documents into manageable, meaningful pieces, essential for context preservation and optimal retrieval.
➞ Consider various chunking strategies (e.g., fixed-size, semantic, recursive).

3./ 𝐆𝐞𝐧𝐞𝐫𝐚𝐭𝐞 𝐄𝐦𝐛𝐞𝐝𝐝𝐢𝐧𝐠𝐬
➞ Transform your chunks into dense vector representations using state-of-the-art embedding models like text-embedding-ada-002, Cohere Embed v3, BGE-M3, or llama-text-embed-v2.

4./ 𝐒𝐭𝐨𝐫𝐞 𝐢𝐧 𝐕𝐞𝐜𝐭𝐨𝐫 𝐃𝐁 & 𝐈𝐧𝐝𝐞𝐱
➞ Store vectors in specialized vector databases like Pinecone, Weaviate, Qdrant, Milvus - created by Zilliz, or pgvector.
➞ You can also use traditional databases like Elastic MongoDB for document storage, leveraging their vector search capabilities if available and suitable.

5./ 𝐑𝐞𝐭𝐫𝐢𝐞𝐯𝐞 𝐈𝐧𝐟𝐨𝐫𝐦𝐚𝐭𝐢𝐨𝐧
➞ Retrieve relevant context using dense vector search (similarity search), sparse retrieval (e.g., BM25, SPLADE), or sophisticated hybrid fusion methods (e.g., RRF, reciprocal rank fusion) via frameworks like LangChain, LlamaIndex, or Haystack. Implement re-ranking (e.g., using bge-reranker or Cohere Rerank) for improved precision.

6./ 𝐎𝐫𝐜𝐡𝐞𝐬𝐭𝐫𝐚𝐭𝐞 𝐭𝐡𝐞 𝐏𝐢𝐩𝐞𝐥𝐢𝐧𝐞
➞ Build your workflow and manage the flow of information between components using orchestration frameworks like LangChain, LlamaIndex, or dedicated workflow automation platforms like n8n or cloud services like Google Cloud Vertex AI Pipelines.

7./ 𝐒𝐞𝐥𝐞𝐜𝐭 𝐋𝐋𝐌𝐬 𝐟𝐨𝐫 𝐆𝐞𝐧𝐞𝐫𝐚𝐭𝐢𝐨𝐧
➞ Integrate your preferred Large Language Models (LLMs) such as Claude, GPT (e.g., GPT-4o), Gemini, Llama 3, DeepSeek, or Mistral via direct APIs or through AI gateways and routing services like Portkey, Eden, or OpenRouter for consistent access and management.

8./ 𝐀𝐝𝐝 𝐎𝐛𝐬𝐞𝐫𝐯𝐚𝐛𝐢𝐥𝐢𝐭𝐲
➞ Monitor and troubleshoot your RAG system using dedicated observability platforms like Langfuse, PromptLayer, Helicone (YC W23), or Arize AI to track prompt performance, token usage, latency, system health, and model outputs.

9./ 𝐄𝐯𝐚𝐥𝐮𝐚𝐭𝐞 & 𝐈𝐦𝐩𝐫𝐨𝐯𝐞
➞ Continuously test and refine retrieval and generation outputs using automated evaluation metrics (e.g., faithfulness, answer relevance, context recall/precision), A/B tests, human feedback loops, and fine-tuning (if necessary) for better quality and performance.

This workflow breaks down every stage of a successful Vector RAG pipeline.

Save this guide, it’s your starting point.

----

Follow@techNmakLast editedOpens edit history8:18 AM · Oct 20, 2025·38KViews51177773784Read 51 repliesNew to X?Sign up now to get your own personalized timeline!Sign up with AppleCreate accountBy signing up, you agree to theTerms of ServiceandPrivacy Policy, includingCookie Use.Trending nowWhat’s happeningTrending in GermanyBäume2,250 postsTrendingGovernance39.4K postsTrendingToken209K postsBusiness & finance · Trending#NemesisShow more

PostSee new postsConversationTech with Mak@techNmakYour Vector RAG Blueprint.

Here’s a clear 9-step pipeline to build a modern Vector RAG system from scratch.

1./ 𝐈𝐧𝐠𝐞𝐬𝐭 & 𝐏𝐫𝐞𝐩𝐫𝐨𝐜𝐞𝐬𝐬 𝐃𝐚𝐭𝐚
➞ Start with tools like web scraping libraries/services (e.g., Firecrawl), data connectors (e.g., for databases, APIs), or dedicated ingestion and preprocessing platforms (e.g., Unstructured(.)io) to collect and clean your data before chunking or embedding begins.

2./ 𝐒𝐩𝐥𝐢𝐭 𝐈𝐧𝐭𝐨 𝐂𝐡𝐮𝐧𝐤𝐬
➞ Use libraries like LangChain or LlamaIndex to break documents into manageable, meaningful pieces, essential for context preservation and optimal retrieval.
➞ Consider various chunking strategies (e.g., fixed-size, semantic, recursive).

3./ 𝐆𝐞𝐧𝐞𝐫𝐚𝐭𝐞 𝐄𝐦𝐛𝐞𝐝𝐝𝐢𝐧𝐠𝐬
➞ Transform your chunks into dense vector representations using state-of-the-art embedding models like text-embedding-ada-002, Cohere Embed v3, BGE-M3, or llama-text-embed-v2.

4./ 𝐒𝐭𝐨𝐫𝐞 𝐢𝐧 𝐕𝐞𝐜𝐭𝐨𝐫 𝐃𝐁 & 𝐈𝐧𝐝𝐞𝐱
➞ Store vectors in specialized vector databases like Pinecone, Weaviate, Qdrant, Milvus - created by Zilliz, or pgvector.
➞ You can also use traditional databases like Elastic MongoDB for document storage, leveraging their vector search capabilities if available and suitable.

5./ 𝐑𝐞𝐭𝐫𝐢𝐞𝐯𝐞 𝐈𝐧𝐟𝐨𝐫𝐦𝐚𝐭𝐢𝐨𝐧
➞ Retrieve relevant context using dense vector search (similarity search), sparse retrieval (e.g., BM25, SPLADE), or sophisticated hybrid fusion methods (e.g., RRF, reciprocal rank fusion) via frameworks like LangChain, LlamaIndex, or Haystack. Implement re-ranking (e.g., using bge-reranker or Cohere Rerank) for improved precision.

6./ 𝐎𝐫𝐜𝐡𝐞𝐬𝐭𝐫𝐚𝐭𝐞 𝐭𝐡𝐞 𝐏𝐢𝐩𝐞𝐥𝐢𝐧𝐞
➞ Build your workflow and manage the flow of information between components using orchestration frameworks like LangChain, LlamaIndex, or dedicated workflow automation platforms like n8n or cloud services like Google Cloud Vertex AI Pipelines.

7./ 𝐒𝐞𝐥𝐞𝐜𝐭 𝐋𝐋𝐌𝐬 𝐟𝐨𝐫 𝐆𝐞𝐧𝐞𝐫𝐚𝐭𝐢𝐨𝐧
➞ Integrate your preferred Large Language Models (LLMs) such as Claude, GPT (e.g., GPT-4o), Gemini, Llama 3, DeepSeek, or Mistral via direct APIs or through AI gateways and routing services like Portkey, Eden, or OpenRouter for consistent access and management.

8./ 𝐀𝐝𝐝 𝐎𝐛𝐬𝐞𝐫𝐯𝐚𝐛𝐢𝐥𝐢𝐭𝐲
➞ Monitor and troubleshoot your RAG system using dedicated observability platforms like Langfuse, PromptLayer, Helicone (YC W23), or Arize AI to track prompt performance, token usage, latency, system health, and model outputs.

9./ 𝐄𝐯𝐚𝐥𝐮𝐚𝐭𝐞 & 𝐈𝐦𝐩𝐫𝐨𝐯𝐞
➞ Continuously test and refine retrieval and generation outputs using automated evaluation metrics (e.g., faithfulness, answer relevance, context recall/precision), A/B tests, human feedback loops, and fine-tuning (if necessary) for better quality and performance.

This workflow breaks down every stage of a successful Vector RAG pipeline.

Save this guide, it’s your starting point.

----

Follow@techNmakLast editedOpens edit history8:18 AM · Oct 20, 2025·38KViews51177773784Read 51 replies

PostSee new postsConversationTech with Mak@techNmakYour Vector RAG Blueprint.

Here’s a clear 9-step pipeline to build a modern Vector RAG system from scratch.

1./ 𝐈𝐧𝐠𝐞𝐬𝐭 & 𝐏𝐫𝐞𝐩𝐫𝐨𝐜𝐞𝐬𝐬 𝐃𝐚𝐭𝐚
➞ Start with tools like web scraping libraries/services (e.g., Firecrawl), data connectors (e.g., for databases, APIs), or dedicated ingestion and preprocessing platforms (e.g., Unstructured(.)io) to collect and clean your data before chunking or embedding begins.

2./ 𝐒𝐩𝐥𝐢𝐭 𝐈𝐧𝐭𝐨 𝐂𝐡𝐮𝐧𝐤𝐬
➞ Use libraries like LangChain or LlamaIndex to break documents into manageable, meaningful pieces, essential for context preservation and optimal retrieval.
➞ Consider various chunking strategies (e.g., fixed-size, semantic, recursive).

3./ 𝐆𝐞𝐧𝐞𝐫𝐚𝐭𝐞 𝐄𝐦𝐛𝐞𝐝𝐝𝐢𝐧𝐠𝐬
➞ Transform your chunks into dense vector representations using state-of-the-art embedding models like text-embedding-ada-002, Cohere Embed v3, BGE-M3, or llama-text-embed-v2.

4./ 𝐒𝐭𝐨𝐫𝐞 𝐢𝐧 𝐕𝐞𝐜𝐭𝐨𝐫 𝐃𝐁 & 𝐈𝐧𝐝𝐞𝐱
➞ Store vectors in specialized vector databases like Pinecone, Weaviate, Qdrant, Milvus - created by Zilliz, or pgvector.
➞ You can also use traditional databases like Elastic MongoDB for document storage, leveraging their vector search capabilities if available and suitable.

5./ 𝐑𝐞𝐭𝐫𝐢𝐞𝐯𝐞 𝐈𝐧𝐟𝐨𝐫𝐦𝐚𝐭𝐢𝐨𝐧
➞ Retrieve relevant context using dense vector search (similarity search), sparse retrieval (e.g., BM25, SPLADE), or sophisticated hybrid fusion methods (e.g., RRF, reciprocal rank fusion) via frameworks like LangChain, LlamaIndex, or Haystack. Implement re-ranking (e.g., using bge-reranker or Cohere Rerank) for improved precision.

6./ 𝐎𝐫𝐜𝐡𝐞𝐬𝐭𝐫𝐚𝐭𝐞 𝐭𝐡𝐞 𝐏𝐢𝐩𝐞𝐥𝐢𝐧𝐞
➞ Build your workflow and manage the flow of information between components using orchestration frameworks like LangChain, LlamaIndex, or dedicated workflow automation platforms like n8n or cloud services like Google Cloud Vertex AI Pipelines.

7./ 𝐒𝐞𝐥𝐞𝐜𝐭 𝐋𝐋𝐌𝐬 𝐟𝐨𝐫 𝐆𝐞𝐧𝐞𝐫𝐚𝐭𝐢𝐨𝐧
➞ Integrate your preferred Large Language Models (LLMs) such as Claude, GPT (e.g., GPT-4o), Gemini, Llama 3, DeepSeek, or Mistral via direct APIs or through AI gateways and routing services like Portkey, Eden, or OpenRouter for consistent access and management.

8./ 𝐀𝐝𝐝 𝐎𝐛𝐬𝐞𝐫𝐯𝐚𝐛𝐢𝐥𝐢𝐭𝐲
➞ Monitor and troubleshoot your RAG system using dedicated observability platforms like Langfuse, PromptLayer, Helicone (YC W23), or Arize AI to track prompt performance, token usage, latency, system health, and model outputs.

9./ 𝐄𝐯𝐚𝐥𝐮𝐚𝐭𝐞 & 𝐈𝐦𝐩𝐫𝐨𝐯𝐞
➞ Continuously test and refine retrieval and generation outputs using automated evaluation metrics (e.g., faithfulness, answer relevance, context recall/precision), A/B tests, human feedback loops, and fine-tuning (if necessary) for better quality and performance.

This workflow breaks down every stage of a successful Vector RAG pipeline.

Save this guide, it’s your starting point.

----

Follow@techNmakLast editedOpens edit history8:18 AM · Oct 20, 2025·38KViews51177773784Read 51 replies

Tech with Mak@techNmakYour Vector RAG Blueprint.

Here’s a clear 9-step pipeline to build a modern Vector RAG system from scratch.

1./ 𝐈𝐧𝐠𝐞𝐬𝐭 & 𝐏𝐫𝐞𝐩𝐫𝐨𝐜𝐞𝐬𝐬 𝐃𝐚𝐭𝐚
➞ Start with tools like web scraping libraries/services (e.g., Firecrawl), data connectors (e.g., for databases, APIs), or dedicated ingestion and preprocessing platforms (e.g., Unstructured(.)io) to collect and clean your data before chunking or embedding begins.

2./ 𝐒𝐩𝐥𝐢𝐭 𝐈𝐧𝐭𝐨 𝐂𝐡𝐮𝐧𝐤𝐬
➞ Use libraries like LangChain or LlamaIndex to break documents into manageable, meaningful pieces, essential for context preservation and optimal retrieval.
➞ Consider various chunking strategies (e.g., fixed-size, semantic, recursive).

3./ 𝐆𝐞𝐧𝐞𝐫𝐚𝐭𝐞 𝐄𝐦𝐛𝐞𝐝𝐝𝐢𝐧𝐠𝐬
➞ Transform your chunks into dense vector representations using state-of-the-art embedding models like text-embedding-ada-002, Cohere Embed v3, BGE-M3, or llama-text-embed-v2.

4./ 𝐒𝐭𝐨𝐫𝐞 𝐢𝐧 𝐕𝐞𝐜𝐭𝐨𝐫 𝐃𝐁 & 𝐈𝐧𝐝𝐞𝐱
➞ Store vectors in specialized vector databases like Pinecone, Weaviate, Qdrant, Milvus - created by Zilliz, or pgvector.
➞ You can also use traditional databases like Elastic MongoDB for document storage, leveraging their vector search capabilities if available and suitable.

5./ 𝐑𝐞𝐭𝐫𝐢𝐞𝐯𝐞 𝐈𝐧𝐟𝐨𝐫𝐦𝐚𝐭𝐢𝐨𝐧
➞ Retrieve relevant context using dense vector search (similarity search), sparse retrieval (e.g., BM25, SPLADE), or sophisticated hybrid fusion methods (e.g., RRF, reciprocal rank fusion) via frameworks like LangChain, LlamaIndex, or Haystack. Implement re-ranking (e.g., using bge-reranker or Cohere Rerank) for improved precision.

6./ 𝐎𝐫𝐜𝐡𝐞𝐬𝐭𝐫𝐚𝐭𝐞 𝐭𝐡𝐞 𝐏𝐢𝐩𝐞𝐥𝐢𝐧𝐞
➞ Build your workflow and manage the flow of information between components using orchestration frameworks like LangChain, LlamaIndex, or dedicated workflow automation platforms like n8n or cloud services like Google Cloud Vertex AI Pipelines.

7./ 𝐒𝐞𝐥𝐞𝐜𝐭 𝐋𝐋𝐌𝐬 𝐟𝐨𝐫 𝐆𝐞𝐧𝐞𝐫𝐚𝐭𝐢𝐨𝐧
➞ Integrate your preferred Large Language Models (LLMs) such as Claude, GPT (e.g., GPT-4o), Gemini, Llama 3, DeepSeek, or Mistral via direct APIs or through AI gateways and routing services like Portkey, Eden, or OpenRouter for consistent access and management.

8./ 𝐀𝐝𝐝 𝐎𝐛𝐬𝐞𝐫𝐯𝐚𝐛𝐢𝐥𝐢𝐭𝐲
➞ Monitor and troubleshoot your RAG system using dedicated observability platforms like Langfuse, PromptLayer, Helicone (YC W23), or Arize AI to track prompt performance, token usage, latency, system health, and model outputs.

9./ 𝐄𝐯𝐚𝐥𝐮𝐚𝐭𝐞 & 𝐈𝐦𝐩𝐫𝐨𝐯𝐞
➞ Continuously test and refine retrieval and generation outputs using automated evaluation metrics (e.g., faithfulness, answer relevance, context recall/precision), A/B tests, human feedback loops, and fine-tuning (if necessary) for better quality and performance.

This workflow breaks down every stage of a successful Vector RAG pipeline.

Save this guide, it’s your starting point.

----

Follow@techNmakLast editedOpens edit history8:18 AM · Oct 20, 2025·38KViews51177773784Read 51 replies

Tech with Mak@techNmakYour Vector RAG Blueprint.

Here’s a clear 9-step pipeline to build a modern Vector RAG system from scratch.

1./ 𝐈𝐧𝐠𝐞𝐬𝐭 & 𝐏𝐫𝐞𝐩𝐫𝐨𝐜𝐞𝐬𝐬 𝐃𝐚𝐭𝐚
➞ Start with tools like web scraping libraries/services (e.g., Firecrawl), data connectors (e.g., for databases, APIs), or dedicated ingestion and preprocessing platforms (e.g., Unstructured(.)io) to collect and clean your data before chunking or embedding begins.

2./ 𝐒𝐩𝐥𝐢𝐭 𝐈𝐧𝐭𝐨 𝐂𝐡𝐮𝐧𝐤𝐬
➞ Use libraries like LangChain or LlamaIndex to break documents into manageable, meaningful pieces, essential for context preservation and optimal retrieval.
➞ Consider various chunking strategies (e.g., fixed-size, semantic, recursive).

3./ 𝐆𝐞𝐧𝐞𝐫𝐚𝐭𝐞 𝐄𝐦𝐛𝐞𝐝𝐝𝐢𝐧𝐠𝐬
➞ Transform your chunks into dense vector representations using state-of-the-art embedding models like text-embedding-ada-002, Cohere Embed v3, BGE-M3, or llama-text-embed-v2.

4./ 𝐒𝐭𝐨𝐫𝐞 𝐢𝐧 𝐕𝐞𝐜𝐭𝐨𝐫 𝐃𝐁 & 𝐈𝐧𝐝𝐞𝐱
➞ Store vectors in specialized vector databases like Pinecone, Weaviate, Qdrant, Milvus - created by Zilliz, or pgvector.
➞ You can also use traditional databases like Elastic MongoDB for document storage, leveraging their vector search capabilities if available and suitable.

5./ 𝐑𝐞𝐭𝐫𝐢𝐞𝐯𝐞 𝐈𝐧𝐟𝐨𝐫𝐦𝐚𝐭𝐢𝐨𝐧
➞ Retrieve relevant context using dense vector search (similarity search), sparse retrieval (e.g., BM25, SPLADE), or sophisticated hybrid fusion methods (e.g., RRF, reciprocal rank fusion) via frameworks like LangChain, LlamaIndex, or Haystack. Implement re-ranking (e.g., using bge-reranker or Cohere Rerank) for improved precision.

6./ 𝐎𝐫𝐜𝐡𝐞𝐬𝐭𝐫𝐚𝐭𝐞 𝐭𝐡𝐞 𝐏𝐢𝐩𝐞𝐥𝐢𝐧𝐞
➞ Build your workflow and manage the flow of information between components using orchestration frameworks like LangChain, LlamaIndex, or dedicated workflow automation platforms like n8n or cloud services like Google Cloud Vertex AI Pipelines.

7./ 𝐒𝐞𝐥𝐞𝐜𝐭 𝐋𝐋𝐌𝐬 𝐟𝐨𝐫 𝐆𝐞𝐧𝐞𝐫𝐚𝐭𝐢𝐨𝐧
➞ Integrate your preferred Large Language Models (LLMs) such as Claude, GPT (e.g., GPT-4o), Gemini, Llama 3, DeepSeek, or Mistral via direct APIs or through AI gateways and routing services like Portkey, Eden, or OpenRouter for consistent access and management.

8./ 𝐀𝐝𝐝 𝐎𝐛𝐬𝐞𝐫𝐯𝐚𝐛𝐢𝐥𝐢𝐭𝐲
➞ Monitor and troubleshoot your RAG system using dedicated observability platforms like Langfuse, PromptLayer, Helicone (YC W23), or Arize AI to track prompt performance, token usage, latency, system health, and model outputs.

9./ 𝐄𝐯𝐚𝐥𝐮𝐚𝐭𝐞 & 𝐈𝐦𝐩𝐫𝐨𝐯𝐞
➞ Continuously test and refine retrieval and generation outputs using automated evaluation metrics (e.g., faithfulness, answer relevance, context recall/precision), A/B tests, human feedback loops, and fine-tuning (if necessary) for better quality and performance.

This workflow breaks down every stage of a successful Vector RAG pipeline.

Save this guide, it’s your starting point.

----

Follow@techNmakLast editedOpens edit history8:18 AM · Oct 20, 2025·38KViews51177773784Read 51 replies

Tech with Mak@techNmakYour Vector RAG Blueprint.

Here’s a clear 9-step pipeline to build a modern Vector RAG system from scratch.

1./ 𝐈𝐧𝐠𝐞𝐬𝐭 & 𝐏𝐫𝐞𝐩𝐫𝐨𝐜𝐞𝐬𝐬 𝐃𝐚𝐭𝐚
➞ Start with tools like web scraping libraries/services (e.g., Firecrawl), data connectors (e.g., for databases, APIs), or dedicated ingestion and preprocessing platforms (e.g., Unstructured(.)io) to collect and clean your data before chunking or embedding begins.

2./ 𝐒𝐩𝐥𝐢𝐭 𝐈𝐧𝐭𝐨 𝐂𝐡𝐮𝐧𝐤𝐬
➞ Use libraries like LangChain or LlamaIndex to break documents into manageable, meaningful pieces, essential for context preservation and optimal retrieval.
➞ Consider various chunking strategies (e.g., fixed-size, semantic, recursive).

3./ 𝐆𝐞𝐧𝐞𝐫𝐚𝐭𝐞 𝐄𝐦𝐛𝐞𝐝𝐝𝐢𝐧𝐠𝐬
➞ Transform your chunks into dense vector representations using state-of-the-art embedding models like text-embedding-ada-002, Cohere Embed v3, BGE-M3, or llama-text-embed-v2.

4./ 𝐒𝐭𝐨𝐫𝐞 𝐢𝐧 𝐕𝐞𝐜𝐭𝐨𝐫 𝐃𝐁 & 𝐈𝐧𝐝𝐞𝐱
➞ Store vectors in specialized vector databases like Pinecone, Weaviate, Qdrant, Milvus - created by Zilliz, or pgvector.
➞ You can also use traditional databases like Elastic MongoDB for document storage, leveraging their vector search capabilities if available and suitable.

5./ 𝐑𝐞𝐭𝐫𝐢𝐞𝐯𝐞 𝐈𝐧𝐟𝐨𝐫𝐦𝐚𝐭𝐢𝐨𝐧
➞ Retrieve relevant context using dense vector search (similarity search), sparse retrieval (e.g., BM25, SPLADE), or sophisticated hybrid fusion methods (e.g., RRF, reciprocal rank fusion) via frameworks like LangChain, LlamaIndex, or Haystack. Implement re-ranking (e.g., using bge-reranker or Cohere Rerank) for improved precision.

6./ 𝐎𝐫𝐜𝐡𝐞𝐬𝐭𝐫𝐚𝐭𝐞 𝐭𝐡𝐞 𝐏𝐢𝐩𝐞𝐥𝐢𝐧𝐞
➞ Build your workflow and manage the flow of information between components using orchestration frameworks like LangChain, LlamaIndex, or dedicated workflow automation platforms like n8n or cloud services like Google Cloud Vertex AI Pipelines.

7./ 𝐒𝐞𝐥𝐞𝐜𝐭 𝐋𝐋𝐌𝐬 𝐟𝐨𝐫 𝐆𝐞𝐧𝐞𝐫𝐚𝐭𝐢𝐨𝐧
➞ Integrate your preferred Large Language Models (LLMs) such as Claude, GPT (e.g., GPT-4o), Gemini, Llama 3, DeepSeek, or Mistral via direct APIs or through AI gateways and routing services like Portkey, Eden, or OpenRouter for consistent access and management.

8./ 𝐀𝐝𝐝 𝐎𝐛𝐬𝐞𝐫𝐯𝐚𝐛𝐢𝐥𝐢𝐭𝐲
➞ Monitor and troubleshoot your RAG system using dedicated observability platforms like Langfuse, PromptLayer, Helicone (YC W23), or Arize AI to track prompt performance, token usage, latency, system health, and model outputs.

9./ 𝐄𝐯𝐚𝐥𝐮𝐚𝐭𝐞 & 𝐈𝐦𝐩𝐫𝐨𝐯𝐞
➞ Continuously test and refine retrieval and generation outputs using automated evaluation metrics (e.g., faithfulness, answer relevance, context recall/precision), A/B tests, human feedback loops, and fine-tuning (if necessary) for better quality and performance.

This workflow breaks down every stage of a successful Vector RAG pipeline.

Save this guide, it’s your starting point.

----

Follow@techNmakLast editedOpens edit history8:18 AM · Oct 20, 2025·38KViews51177773784Read 51 replies

Tech with Mak@techNmakYour Vector RAG Blueprint.

Here’s a clear 9-step pipeline to build a modern Vector RAG system from scratch.

1./ 𝐈𝐧𝐠𝐞𝐬𝐭 & 𝐏𝐫𝐞𝐩𝐫𝐨𝐜𝐞𝐬𝐬 𝐃𝐚𝐭𝐚
➞ Start with tools like web scraping libraries/services (e.g., Firecrawl), data connectors (e.g., for databases, APIs), or dedicated ingestion and preprocessing platforms (e.g., Unstructured(.)io) to collect and clean your data before chunking or embedding begins.

2./ 𝐒𝐩𝐥𝐢𝐭 𝐈𝐧𝐭𝐨 𝐂𝐡𝐮𝐧𝐤𝐬
➞ Use libraries like LangChain or LlamaIndex to break documents into manageable, meaningful pieces, essential for context preservation and optimal retrieval.
➞ Consider various chunking strategies (e.g., fixed-size, semantic, recursive).

3./ 𝐆𝐞𝐧𝐞𝐫𝐚𝐭𝐞 𝐄𝐦𝐛𝐞𝐝𝐝𝐢𝐧𝐠𝐬
➞ Transform your chunks into dense vector representations using state-of-the-art embedding models like text-embedding-ada-002, Cohere Embed v3, BGE-M3, or llama-text-embed-v2.

4./ 𝐒𝐭𝐨𝐫𝐞 𝐢𝐧 𝐕𝐞𝐜𝐭𝐨𝐫 𝐃𝐁 & 𝐈𝐧𝐝𝐞𝐱
➞ Store vectors in specialized vector databases like Pinecone, Weaviate, Qdrant, Milvus - created by Zilliz, or pgvector.
➞ You can also use traditional databases like Elastic MongoDB for document storage, leveraging their vector search capabilities if available and suitable.

5./ 𝐑𝐞𝐭𝐫𝐢𝐞𝐯𝐞 𝐈𝐧𝐟𝐨𝐫𝐦𝐚𝐭𝐢𝐨𝐧
➞ Retrieve relevant context using dense vector search (similarity search), sparse retrieval (e.g., BM25, SPLADE), or sophisticated hybrid fusion methods (e.g., RRF, reciprocal rank fusion) via frameworks like LangChain, LlamaIndex, or Haystack. Implement re-ranking (e.g., using bge-reranker or Cohere Rerank) for improved precision.

6./ 𝐎𝐫𝐜𝐡𝐞𝐬𝐭𝐫𝐚𝐭𝐞 𝐭𝐡𝐞 𝐏𝐢𝐩𝐞𝐥𝐢𝐧𝐞
➞ Build your workflow and manage the flow of information between components using orchestration frameworks like LangChain, LlamaIndex, or dedicated workflow automation platforms like n8n or cloud services like Google Cloud Vertex AI Pipelines.

7./ 𝐒𝐞𝐥𝐞𝐜𝐭 𝐋𝐋𝐌𝐬 𝐟𝐨𝐫 𝐆𝐞𝐧𝐞𝐫𝐚𝐭𝐢𝐨𝐧
➞ Integrate your preferred Large Language Models (LLMs) such as Claude, GPT (e.g., GPT-4o), Gemini, Llama 3, DeepSeek, or Mistral via direct APIs or through AI gateways and routing services like Portkey, Eden, or OpenRouter for consistent access and management.

8./ 𝐀𝐝𝐝 𝐎𝐛𝐬𝐞𝐫𝐯𝐚𝐛𝐢𝐥𝐢𝐭𝐲
➞ Monitor and troubleshoot your RAG system using dedicated observability platforms like Langfuse, PromptLayer, Helicone (YC W23), or Arize AI to track prompt performance, token usage, latency, system health, and model outputs.

9./ 𝐄𝐯𝐚𝐥𝐮𝐚𝐭𝐞 & 𝐈𝐦𝐩𝐫𝐨𝐯𝐞
➞ Continuously test and refine retrieval and generation outputs using automated evaluation metrics (e.g., faithfulness, answer relevance, context recall/precision), A/B tests, human feedback loops, and fine-tuning (if necessary) for better quality and performance.

This workflow breaks down every stage of a successful Vector RAG pipeline.

Save this guide, it’s your starting point.

----

Follow@techNmakLast editedOpens edit history8:18 AM · Oct 20, 2025·38KViews51177773784Read 51 replies

Tech with Mak@techNmakYour Vector RAG Blueprint.

Here’s a clear 9-step pipeline to build a modern Vector RAG system from scratch.

1./ 𝐈𝐧𝐠𝐞𝐬𝐭 & 𝐏𝐫𝐞𝐩𝐫𝐨𝐜𝐞𝐬𝐬 𝐃𝐚𝐭𝐚
➞ Start with tools like web scraping libraries/services (e.g., Firecrawl), data connectors (e.g., for databases, APIs), or dedicated ingestion and preprocessing platforms (e.g., Unstructured(.)io) to collect and clean your data before chunking or embedding begins.

2./ 𝐒𝐩𝐥𝐢𝐭 𝐈𝐧𝐭𝐨 𝐂𝐡𝐮𝐧𝐤𝐬
➞ Use libraries like LangChain or LlamaIndex to break documents into manageable, meaningful pieces, essential for context preservation and optimal retrieval.
➞ Consider various chunking strategies (e.g., fixed-size, semantic, recursive).

3./ 𝐆𝐞𝐧𝐞𝐫𝐚𝐭𝐞 𝐄𝐦𝐛𝐞𝐝𝐝𝐢𝐧𝐠𝐬
➞ Transform your chunks into dense vector representations using state-of-the-art embedding models like text-embedding-ada-002, Cohere Embed v3, BGE-M3, or llama-text-embed-v2.

4./ 𝐒𝐭𝐨𝐫𝐞 𝐢𝐧 𝐕𝐞𝐜𝐭𝐨𝐫 𝐃𝐁 & 𝐈𝐧𝐝𝐞𝐱
➞ Store vectors in specialized vector databases like Pinecone, Weaviate, Qdrant, Milvus - created by Zilliz, or pgvector.
➞ You can also use traditional databases like Elastic MongoDB for document storage, leveraging their vector search capabilities if available and suitable.

5./ 𝐑𝐞𝐭𝐫𝐢𝐞𝐯𝐞 𝐈𝐧𝐟𝐨𝐫𝐦𝐚𝐭𝐢𝐨𝐧
➞ Retrieve relevant context using dense vector search (similarity search), sparse retrieval (e.g., BM25, SPLADE), or sophisticated hybrid fusion methods (e.g., RRF, reciprocal rank fusion) via frameworks like LangChain, LlamaIndex, or Haystack. Implement re-ranking (e.g., using bge-reranker or Cohere Rerank) for improved precision.

6./ 𝐎𝐫𝐜𝐡𝐞𝐬𝐭𝐫𝐚𝐭𝐞 𝐭𝐡𝐞 𝐏𝐢𝐩𝐞𝐥𝐢𝐧𝐞
➞ Build your workflow and manage the flow of information between components using orchestration frameworks like LangChain, LlamaIndex, or dedicated workflow automation platforms like n8n or cloud services like Google Cloud Vertex AI Pipelines.

7./ 𝐒𝐞𝐥𝐞𝐜𝐭 𝐋𝐋𝐌𝐬 𝐟𝐨𝐫 𝐆𝐞𝐧𝐞𝐫𝐚𝐭𝐢𝐨𝐧
➞ Integrate your preferred Large Language Models (LLMs) such as Claude, GPT (e.g., GPT-4o), Gemini, Llama 3, DeepSeek, or Mistral via direct APIs or through AI gateways and routing services like Portkey, Eden, or OpenRouter for consistent access and management.

8./ 𝐀𝐝𝐝 𝐎𝐛𝐬𝐞𝐫𝐯𝐚𝐛𝐢𝐥𝐢𝐭𝐲
➞ Monitor and troubleshoot your RAG system using dedicated observability platforms like Langfuse, PromptLayer, Helicone (YC W23), or Arize AI to track prompt performance, token usage, latency, system health, and model outputs.

9./ 𝐄𝐯𝐚𝐥𝐮𝐚𝐭𝐞 & 𝐈𝐦𝐩𝐫𝐨𝐯𝐞
➞ Continuously test and refine retrieval and generation outputs using automated evaluation metrics (e.g., faithfulness, answer relevance, context recall/precision), A/B tests, human feedback loops, and fine-tuning (if necessary) for better quality and performance.

This workflow breaks down every stage of a successful Vector RAG pipeline.

Save this guide, it’s your starting point.

----

Follow@techNmakLast editedOpens edit history8:18 AM · Oct 20, 2025·38KViews51177773784Read 51 replies

Tech with Mak@techNmakYour Vector RAG Blueprint.

Here’s a clear 9-step pipeline to build a modern Vector RAG system from scratch.

1./ 𝐈𝐧𝐠𝐞𝐬𝐭 & 𝐏𝐫𝐞𝐩𝐫𝐨𝐜𝐞𝐬𝐬 𝐃𝐚𝐭𝐚
➞ Start with tools like web scraping libraries/services (e.g., Firecrawl), data connectors (e.g., for databases, APIs), or dedicated ingestion and preprocessing platforms (e.g., Unstructured(.)io) to collect and clean your data before chunking or embedding begins.

2./ 𝐒𝐩𝐥𝐢𝐭 𝐈𝐧𝐭𝐨 𝐂𝐡𝐮𝐧𝐤𝐬
➞ Use libraries like LangChain or LlamaIndex to break documents into manageable, meaningful pieces, essential for context preservation and optimal retrieval.
➞ Consider various chunking strategies (e.g., fixed-size, semantic, recursive).

3./ 𝐆𝐞𝐧𝐞𝐫𝐚𝐭𝐞 𝐄𝐦𝐛𝐞𝐝𝐝𝐢𝐧𝐠𝐬
➞ Transform your chunks into dense vector representations using state-of-the-art embedding models like text-embedding-ada-002, Cohere Embed v3, BGE-M3, or llama-text-embed-v2.

4./ 𝐒𝐭𝐨𝐫𝐞 𝐢𝐧 𝐕𝐞𝐜𝐭𝐨𝐫 𝐃𝐁 & 𝐈𝐧𝐝𝐞𝐱
➞ Store vectors in specialized vector databases like Pinecone, Weaviate, Qdrant, Milvus - created by Zilliz, or pgvector.
➞ You can also use traditional databases like Elastic MongoDB for document storage, leveraging their vector search capabilities if available and suitable.

5./ 𝐑𝐞𝐭𝐫𝐢𝐞𝐯𝐞 𝐈𝐧𝐟𝐨𝐫𝐦𝐚𝐭𝐢𝐨𝐧
➞ Retrieve relevant context using dense vector search (similarity search), sparse retrieval (e.g., BM25, SPLADE), or sophisticated hybrid fusion methods (e.g., RRF, reciprocal rank fusion) via frameworks like LangChain, LlamaIndex, or Haystack. Implement re-ranking (e.g., using bge-reranker or Cohere Rerank) for improved precision.

6./ 𝐎𝐫𝐜𝐡𝐞𝐬𝐭𝐫𝐚𝐭𝐞 𝐭𝐡𝐞 𝐏𝐢𝐩𝐞𝐥𝐢𝐧𝐞
➞ Build your workflow and manage the flow of information between components using orchestration frameworks like LangChain, LlamaIndex, or dedicated workflow automation platforms like n8n or cloud services like Google Cloud Vertex AI Pipelines.

7./ 𝐒𝐞𝐥𝐞𝐜𝐭 𝐋𝐋𝐌𝐬 𝐟𝐨𝐫 𝐆𝐞𝐧𝐞𝐫𝐚𝐭𝐢𝐨𝐧
➞ Integrate your preferred Large Language Models (LLMs) such as Claude, GPT (e.g., GPT-4o), Gemini, Llama 3, DeepSeek, or Mistral via direct APIs or through AI gateways and routing services like Portkey, Eden, or OpenRouter for consistent access and management.

8./ 𝐀𝐝𝐝 𝐎𝐛𝐬𝐞𝐫𝐯𝐚𝐛𝐢𝐥𝐢𝐭𝐲
➞ Monitor and troubleshoot your RAG system using dedicated observability platforms like Langfuse, PromptLayer, Helicone (YC W23), or Arize AI to track prompt performance, token usage, latency, system health, and model outputs.

9./ 𝐄𝐯𝐚𝐥𝐮𝐚𝐭𝐞 & 𝐈𝐦𝐩𝐫𝐨𝐯𝐞
➞ Continuously test and refine retrieval and generation outputs using automated evaluation metrics (e.g., faithfulness, answer relevance, context recall/precision), A/B tests, human feedback loops, and fine-tuning (if necessary) for better quality and performance.

This workflow breaks down every stage of a successful Vector RAG pipeline.

Save this guide, it’s your starting point.

----

Follow@techNmakLast editedOpens edit history8:18 AM · Oct 20, 2025·38KViews51177773784Read 51 replies

Tech with Mak@techNmakYour Vector RAG Blueprint.

Here’s a clear 9-step pipeline to build a modern Vector RAG system from scratch.

1./ 𝐈𝐧𝐠𝐞𝐬𝐭 & 𝐏𝐫𝐞𝐩𝐫𝐨𝐜𝐞𝐬𝐬 𝐃𝐚𝐭𝐚
➞ Start with tools like web scraping libraries/services (e.g., Firecrawl), data connectors (e.g., for databases, APIs), or dedicated ingestion and preprocessing platforms (e.g., Unstructured(.)io) to collect and clean your data before chunking or embedding begins.

2./ 𝐒𝐩𝐥𝐢𝐭 𝐈𝐧𝐭𝐨 𝐂𝐡𝐮𝐧𝐤𝐬
➞ Use libraries like LangChain or LlamaIndex to break documents into manageable, meaningful pieces, essential for context preservation and optimal retrieval.
➞ Consider various chunking strategies (e.g., fixed-size, semantic, recursive).

3./ 𝐆𝐞𝐧𝐞𝐫𝐚𝐭𝐞 𝐄𝐦𝐛𝐞𝐝𝐝𝐢𝐧𝐠𝐬
➞ Transform your chunks into dense vector representations using state-of-the-art embedding models like text-embedding-ada-002, Cohere Embed v3, BGE-M3, or llama-text-embed-v2.

4./ 𝐒𝐭𝐨𝐫𝐞 𝐢𝐧 𝐕𝐞𝐜𝐭𝐨𝐫 𝐃𝐁 & 𝐈𝐧𝐝𝐞𝐱
➞ Store vectors in specialized vector databases like Pinecone, Weaviate, Qdrant, Milvus - created by Zilliz, or pgvector.
➞ You can also use traditional databases like Elastic MongoDB for document storage, leveraging their vector search capabilities if available and suitable.

5./ 𝐑𝐞𝐭𝐫𝐢𝐞𝐯𝐞 𝐈𝐧𝐟𝐨𝐫𝐦𝐚𝐭𝐢𝐨𝐧
➞ Retrieve relevant context using dense vector search (similarity search), sparse retrieval (e.g., BM25, SPLADE), or sophisticated hybrid fusion methods (e.g., RRF, reciprocal rank fusion) via frameworks like LangChain, LlamaIndex, or Haystack. Implement re-ranking (e.g., using bge-reranker or Cohere Rerank) for improved precision.

6./ 𝐎𝐫𝐜𝐡𝐞𝐬𝐭𝐫𝐚𝐭𝐞 𝐭𝐡𝐞 𝐏𝐢𝐩𝐞𝐥𝐢𝐧𝐞
➞ Build your workflow and manage the flow of information between components using orchestration frameworks like LangChain, LlamaIndex, or dedicated workflow automation platforms like n8n or cloud services like Google Cloud Vertex AI Pipelines.

7./ 𝐒𝐞𝐥𝐞𝐜𝐭 𝐋𝐋𝐌𝐬 𝐟𝐨𝐫 𝐆𝐞𝐧𝐞𝐫𝐚𝐭𝐢𝐨𝐧
➞ Integrate your preferred Large Language Models (LLMs) such as Claude, GPT (e.g., GPT-4o), Gemini, Llama 3, DeepSeek, or Mistral via direct APIs or through AI gateways and routing services like Portkey, Eden, or OpenRouter for consistent access and management.

8./ 𝐀𝐝𝐝 𝐎𝐛𝐬𝐞𝐫𝐯𝐚𝐛𝐢𝐥𝐢𝐭𝐲
➞ Monitor and troubleshoot your RAG system using dedicated observability platforms like Langfuse, PromptLayer, Helicone (YC W23), or Arize AI to track prompt performance, token usage, latency, system health, and model outputs.

9./ 𝐄𝐯𝐚𝐥𝐮𝐚𝐭𝐞 & 𝐈𝐦𝐩𝐫𝐨𝐯𝐞
➞ Continuously test and refine retrieval and generation outputs using automated evaluation metrics (e.g., faithfulness, answer relevance, context recall/precision), A/B tests, human feedback loops, and fine-tuning (if necessary) for better quality and performance.

This workflow breaks down every stage of a successful Vector RAG pipeline.

Save this guide, it’s your starting point.

----

Follow@techNmakLast editedOpens edit history8:18 AM · Oct 20, 2025·38KViews51177773784Read 51 replies

Tech with Mak@techNmak

Tech with Mak@techNmak

Tech with Mak@techNmak

Tech with Mak@techNmak

Tech with Mak@techNmak

Tech with Mak@techNmak

Tech with Mak@techNmak

Your Vector RAG Blueprint.

Here’s a clear 9-step pipeline to build a modern Vector RAG system from scratch.

1./ 𝐈𝐧𝐠𝐞𝐬𝐭 & 𝐏𝐫𝐞𝐩𝐫𝐨𝐜𝐞𝐬𝐬 𝐃𝐚𝐭𝐚
➞ Start with tools like web scraping libraries/services (e.g., Firecrawl), data connectors (e.g., for databases, APIs), or dedicated ingestion and preprocessing platforms (e.g., Unstructured(.)io) to collect and clean your data before chunking or embedding begins.

2./ 𝐒𝐩𝐥𝐢𝐭 𝐈𝐧𝐭𝐨 𝐂𝐡𝐮𝐧𝐤𝐬
➞ Use libraries like LangChain or LlamaIndex to break documents into manageable, meaningful pieces, essential for context preservation and optimal retrieval.
➞ Consider various chunking strategies (e.g., fixed-size, semantic, recursive).

3./ 𝐆𝐞𝐧𝐞𝐫𝐚𝐭𝐞 𝐄𝐦𝐛𝐞𝐝𝐝𝐢𝐧𝐠𝐬
➞ Transform your chunks into dense vector representations using state-of-the-art embedding models like text-embedding-ada-002, Cohere Embed v3, BGE-M3, or llama-text-embed-v2.

4./ 𝐒𝐭𝐨𝐫𝐞 𝐢𝐧 𝐕𝐞𝐜𝐭𝐨𝐫 𝐃𝐁 & 𝐈𝐧𝐝𝐞𝐱
➞ Store vectors in specialized vector databases like Pinecone, Weaviate, Qdrant, Milvus - created by Zilliz, or pgvector.
➞ You can also use traditional databases like Elastic MongoDB for document storage, leveraging their vector search capabilities if available and suitable.

5./ 𝐑𝐞𝐭𝐫𝐢𝐞𝐯𝐞 𝐈𝐧𝐟𝐨𝐫𝐦𝐚𝐭𝐢𝐨𝐧
➞ Retrieve relevant context using dense vector search (similarity search), sparse retrieval (e.g., BM25, SPLADE), or sophisticated hybrid fusion methods (e.g., RRF, reciprocal rank fusion) via frameworks like LangChain, LlamaIndex, or Haystack. Implement re-ranking (e.g., using bge-reranker or Cohere Rerank) for improved precision.

6./ 𝐎𝐫𝐜𝐡𝐞𝐬𝐭𝐫𝐚𝐭𝐞 𝐭𝐡𝐞 𝐏𝐢𝐩𝐞𝐥𝐢𝐧𝐞
➞ Build your workflow and manage the flow of information between components using orchestration frameworks like LangChain, LlamaIndex, or dedicated workflow automation platforms like n8n or cloud services like Google Cloud Vertex AI Pipelines.

7./ 𝐒𝐞𝐥𝐞𝐜𝐭 𝐋𝐋𝐌𝐬 𝐟𝐨𝐫 𝐆𝐞𝐧𝐞𝐫𝐚𝐭𝐢𝐨𝐧
➞ Integrate your preferred Large Language Models (LLMs) such as Claude, GPT (e.g., GPT-4o), Gemini, Llama 3, DeepSeek, or Mistral via direct APIs or through AI gateways and routing services like Portkey, Eden, or OpenRouter for consistent access and management.

8./ 𝐀𝐝𝐝 𝐎𝐛𝐬𝐞𝐫𝐯𝐚𝐛𝐢𝐥𝐢𝐭𝐲
➞ Monitor and troubleshoot your RAG system using dedicated observability platforms like Langfuse, PromptLayer, Helicone (YC W23), or Arize AI to track prompt performance, token usage, latency, system health, and model outputs.

9./ 𝐄𝐯𝐚𝐥𝐮𝐚𝐭𝐞 & 𝐈𝐦𝐩𝐫𝐨𝐯𝐞
➞ Continuously test and refine retrieval and generation outputs using automated evaluation metrics (e.g., faithfulness, answer relevance, context recall/precision), A/B tests, human feedback loops, and fine-tuning (if necessary) for better quality and performance.

This workflow breaks down every stage of a successful Vector RAG pipeline.

Save this guide, it’s your starting point.

----

Follow@techNmakLast editedOpens edit history8:18 AM · Oct 20, 2025·38KViews51177773784Read 51 replies

Your Vector RAG Blueprint.

Here’s a clear 9-step pipeline to build a modern Vector RAG system from scratch.

1./ 𝐈𝐧𝐠𝐞𝐬𝐭 & 𝐏𝐫𝐞𝐩𝐫𝐨𝐜𝐞𝐬𝐬 𝐃𝐚𝐭𝐚
➞ Start with tools like web scraping libraries/services (e.g., Firecrawl), data connectors (e.g., for databases, APIs), or dedicated ingestion and preprocessing platforms (e.g., Unstructured(.)io) to collect and clean your data before chunking or embedding begins.

2./ 𝐒𝐩𝐥𝐢𝐭 𝐈𝐧𝐭𝐨 𝐂𝐡𝐮𝐧𝐤𝐬
➞ Use libraries like LangChain or LlamaIndex to break documents into manageable, meaningful pieces, essential for context preservation and optimal retrieval.
➞ Consider various chunking strategies (e.g., fixed-size, semantic, recursive).

3./ 𝐆𝐞𝐧𝐞𝐫𝐚𝐭𝐞 𝐄𝐦𝐛𝐞𝐝𝐝𝐢𝐧𝐠𝐬
➞ Transform your chunks into dense vector representations using state-of-the-art embedding models like text-embedding-ada-002, Cohere Embed v3, BGE-M3, or llama-text-embed-v2.

4./ 𝐒𝐭𝐨𝐫𝐞 𝐢𝐧 𝐕𝐞𝐜𝐭𝐨𝐫 𝐃𝐁 & 𝐈𝐧𝐝𝐞𝐱
➞ Store vectors in specialized vector databases like Pinecone, Weaviate, Qdrant, Milvus - created by Zilliz, or pgvector.
➞ You can also use traditional databases like Elastic MongoDB for document storage, leveraging their vector search capabilities if available and suitable.

5./ 𝐑𝐞𝐭𝐫𝐢𝐞𝐯𝐞 𝐈𝐧𝐟𝐨𝐫𝐦𝐚𝐭𝐢𝐨𝐧
➞ Retrieve relevant context using dense vector search (similarity search), sparse retrieval (e.g., BM25, SPLADE), or sophisticated hybrid fusion methods (e.g., RRF, reciprocal rank fusion) via frameworks like LangChain, LlamaIndex, or Haystack. Implement re-ranking (e.g., using bge-reranker or Cohere Rerank) for improved precision.

6./ 𝐎𝐫𝐜𝐡𝐞𝐬𝐭𝐫𝐚𝐭𝐞 𝐭𝐡𝐞 𝐏𝐢𝐩𝐞𝐥𝐢𝐧𝐞
➞ Build your workflow and manage the flow of information between components using orchestration frameworks like LangChain, LlamaIndex, or dedicated workflow automation platforms like n8n or cloud services like Google Cloud Vertex AI Pipelines.

7./ 𝐒𝐞𝐥𝐞𝐜𝐭 𝐋𝐋𝐌𝐬 𝐟𝐨𝐫 𝐆𝐞𝐧𝐞𝐫𝐚𝐭𝐢𝐨𝐧
➞ Integrate your preferred Large Language Models (LLMs) such as Claude, GPT (e.g., GPT-4o), Gemini, Llama 3, DeepSeek, or Mistral via direct APIs or through AI gateways and routing services like Portkey, Eden, or OpenRouter for consistent access and management.

8./ 𝐀𝐝𝐝 𝐎𝐛𝐬𝐞𝐫𝐯𝐚𝐛𝐢𝐥𝐢𝐭𝐲
➞ Monitor and troubleshoot your RAG system using dedicated observability platforms like Langfuse, PromptLayer, Helicone (YC W23), or Arize AI to track prompt performance, token usage, latency, system health, and model outputs.

9./ 𝐄𝐯𝐚𝐥𝐮𝐚𝐭𝐞 & 𝐈𝐦𝐩𝐫𝐨𝐯𝐞
➞ Continuously test and refine retrieval and generation outputs using automated evaluation metrics (e.g., faithfulness, answer relevance, context recall/precision), A/B tests, human feedback loops, and fine-tuning (if necessary) for better quality and performance.

This workflow breaks down every stage of a successful Vector RAG pipeline.

Save this guide, it’s your starting point.

----

Follow@techNmak

Your Vector RAG Blueprint.

Here’s a clear 9-step pipeline to build a modern Vector RAG system from scratch.

1./ 𝐈𝐧𝐠𝐞𝐬𝐭 & 𝐏𝐫𝐞𝐩𝐫𝐨𝐜𝐞𝐬𝐬 𝐃𝐚𝐭𝐚
➞ Start with tools like web scraping libraries/services (e.g., Firecrawl), data connectors (e.g., for databases, APIs), or dedicated ingestion and preprocessing platforms (e.g., Unstructured(.)io) to collect and clean your data before chunking or embedding begins.

2./ 𝐒𝐩𝐥𝐢𝐭 𝐈𝐧𝐭𝐨 𝐂𝐡𝐮𝐧𝐤𝐬
➞ Use libraries like LangChain or LlamaIndex to break documents into manageable, meaningful pieces, essential for context preservation and optimal retrieval.
➞ Consider various chunking strategies (e.g., fixed-size, semantic, recursive).

3./ 𝐆𝐞𝐧𝐞𝐫𝐚𝐭𝐞 𝐄𝐦𝐛𝐞𝐝𝐝𝐢𝐧𝐠𝐬
➞ Transform your chunks into dense vector representations using state-of-the-art embedding models like text-embedding-ada-002, Cohere Embed v3, BGE-M3, or llama-text-embed-v2.

4./ 𝐒𝐭𝐨𝐫𝐞 𝐢𝐧 𝐕𝐞𝐜𝐭𝐨𝐫 𝐃𝐁 & 𝐈𝐧𝐝𝐞𝐱
➞ Store vectors in specialized vector databases like Pinecone, Weaviate, Qdrant, Milvus - created by Zilliz, or pgvector.
➞ You can also use traditional databases like Elastic MongoDB for document storage, leveraging their vector search capabilities if available and suitable.

5./ 𝐑𝐞𝐭𝐫𝐢𝐞𝐯𝐞 𝐈𝐧𝐟𝐨𝐫𝐦𝐚𝐭𝐢𝐨𝐧
➞ Retrieve relevant context using dense vector search (similarity search), sparse retrieval (e.g., BM25, SPLADE), or sophisticated hybrid fusion methods (e.g., RRF, reciprocal rank fusion) via frameworks like LangChain, LlamaIndex, or Haystack. Implement re-ranking (e.g., using bge-reranker or Cohere Rerank) for improved precision.

6./ 𝐎𝐫𝐜𝐡𝐞𝐬𝐭𝐫𝐚𝐭𝐞 𝐭𝐡𝐞 𝐏𝐢𝐩𝐞𝐥𝐢𝐧𝐞
➞ Build your workflow and manage the flow of information between components using orchestration frameworks like LangChain, LlamaIndex, or dedicated workflow automation platforms like n8n or cloud services like Google Cloud Vertex AI Pipelines.

7./ 𝐒𝐞𝐥𝐞𝐜𝐭 𝐋𝐋𝐌𝐬 𝐟𝐨𝐫 𝐆𝐞𝐧𝐞𝐫𝐚𝐭𝐢𝐨𝐧
➞ Integrate your preferred Large Language Models (LLMs) such as Claude, GPT (e.g., GPT-4o), Gemini, Llama 3, DeepSeek, or Mistral via direct APIs or through AI gateways and routing services like Portkey, Eden, or OpenRouter for consistent access and management.

8./ 𝐀𝐝𝐝 𝐎𝐛𝐬𝐞𝐫𝐯𝐚𝐛𝐢𝐥𝐢𝐭𝐲
➞ Monitor and troubleshoot your RAG system using dedicated observability platforms like Langfuse, PromptLayer, Helicone (YC W23), or Arize AI to track prompt performance, token usage, latency, system health, and model outputs.

9./ 𝐄𝐯𝐚𝐥𝐮𝐚𝐭𝐞 & 𝐈𝐦𝐩𝐫𝐨𝐯𝐞
➞ Continuously test and refine retrieval and generation outputs using automated evaluation metrics (e.g., faithfulness, answer relevance, context recall/precision), A/B tests, human feedback loops, and fine-tuning (if necessary) for better quality and performance.

This workflow breaks down every stage of a successful Vector RAG pipeline.

Save this guide, it’s your starting point.

----

Follow@techNmak

Your Vector RAG Blueprint.

Here’s a clear 9-step pipeline to build a modern Vector RAG system from scratch.

1./ 𝐈𝐧𝐠𝐞𝐬𝐭 & 𝐏𝐫𝐞𝐩𝐫𝐨𝐜𝐞𝐬𝐬 𝐃𝐚𝐭𝐚
➞ Start with tools like web scraping libraries/services (e.g., Firecrawl), data connectors (e.g., for databases, APIs), or dedicated ingestion and preprocessing platforms (e.g., Unstructured(.)io) to collect and clean your data before chunking or embedding begins.

2./ 𝐒𝐩𝐥𝐢𝐭 𝐈𝐧𝐭𝐨 𝐂𝐡𝐮𝐧𝐤𝐬
➞ Use libraries like LangChain or LlamaIndex to break documents into manageable, meaningful pieces, essential for context preservation and optimal retrieval.
➞ Consider various chunking strategies (e.g., fixed-size, semantic, recursive).

3./ 𝐆𝐞𝐧𝐞𝐫𝐚𝐭𝐞 𝐄𝐦𝐛𝐞𝐝𝐝𝐢𝐧𝐠𝐬
➞ Transform your chunks into dense vector representations using state-of-the-art embedding models like text-embedding-ada-002, Cohere Embed v3, BGE-M3, or llama-text-embed-v2.

4./ 𝐒𝐭𝐨𝐫𝐞 𝐢𝐧 𝐕𝐞𝐜𝐭𝐨𝐫 𝐃𝐁 & 𝐈𝐧𝐝𝐞𝐱
➞ Store vectors in specialized vector databases like Pinecone, Weaviate, Qdrant, Milvus - created by Zilliz, or pgvector.
➞ You can also use traditional databases like Elastic MongoDB for document storage, leveraging their vector search capabilities if available and suitable.

5./ 𝐑𝐞𝐭𝐫𝐢𝐞𝐯𝐞 𝐈𝐧𝐟𝐨𝐫𝐦𝐚𝐭𝐢𝐨𝐧
➞ Retrieve relevant context using dense vector search (similarity search), sparse retrieval (e.g., BM25, SPLADE), or sophisticated hybrid fusion methods (e.g., RRF, reciprocal rank fusion) via frameworks like LangChain, LlamaIndex, or Haystack. Implement re-ranking (e.g., using bge-reranker or Cohere Rerank) for improved precision.

6./ 𝐎𝐫𝐜𝐡𝐞𝐬𝐭𝐫𝐚𝐭𝐞 𝐭𝐡𝐞 𝐏𝐢𝐩𝐞𝐥𝐢𝐧𝐞
➞ Build your workflow and manage the flow of information between components using orchestration frameworks like LangChain, LlamaIndex, or dedicated workflow automation platforms like n8n or cloud services like Google Cloud Vertex AI Pipelines.

7./ 𝐒𝐞𝐥𝐞𝐜𝐭 𝐋𝐋𝐌𝐬 𝐟𝐨𝐫 𝐆𝐞𝐧𝐞𝐫𝐚𝐭𝐢𝐨𝐧
➞ Integrate your preferred Large Language Models (LLMs) such as Claude, GPT (e.g., GPT-4o), Gemini, Llama 3, DeepSeek, or Mistral via direct APIs or through AI gateways and routing services like Portkey, Eden, or OpenRouter for consistent access and management.

8./ 𝐀𝐝𝐝 𝐎𝐛𝐬𝐞𝐫𝐯𝐚𝐛𝐢𝐥𝐢𝐭𝐲
➞ Monitor and troubleshoot your RAG system using dedicated observability platforms like Langfuse, PromptLayer, Helicone (YC W23), or Arize AI to track prompt performance, token usage, latency, system health, and model outputs.

9./ 𝐄𝐯𝐚𝐥𝐮𝐚𝐭𝐞 & 𝐈𝐦𝐩𝐫𝐨𝐯𝐞
➞ Continuously test and refine retrieval and generation outputs using automated evaluation metrics (e.g., faithfulness, answer relevance, context recall/precision), A/B tests, human feedback loops, and fine-tuning (if necessary) for better quality and performance.

This workflow breaks down every stage of a successful Vector RAG pipeline.

Save this guide, it’s your starting point.

----

Follow@techNmak

Your Vector RAG Blueprint.

Here’s a clear 9-step pipeline to build a modern Vector RAG system from scratch.

1./ 𝐈𝐧𝐠𝐞𝐬𝐭 & 𝐏𝐫𝐞𝐩𝐫𝐨𝐜𝐞𝐬𝐬 𝐃𝐚𝐭𝐚
➞ Start with tools like web scraping libraries/services (e.g., Firecrawl), data connectors (e.g., for databases, APIs), or dedicated ingestion and preprocessing platforms (e.g., Unstructured(.)io) to collect and clean your data before chunking or embedding begins.

2./ 𝐒𝐩𝐥𝐢𝐭 𝐈𝐧𝐭𝐨 𝐂𝐡𝐮𝐧𝐤𝐬
➞ Use libraries like LangChain or LlamaIndex to break documents into manageable, meaningful pieces, essential for context preservation and optimal retrieval.
➞ Consider various chunking strategies (e.g., fixed-size, semantic, recursive).

3./ 𝐆𝐞𝐧𝐞𝐫𝐚𝐭𝐞 𝐄𝐦𝐛𝐞𝐝𝐝𝐢𝐧𝐠𝐬
➞ Transform your chunks into dense vector representations using state-of-the-art embedding models like text-embedding-ada-002, Cohere Embed v3, BGE-M3, or llama-text-embed-v2.

4./ 𝐒𝐭𝐨𝐫𝐞 𝐢𝐧 𝐕𝐞𝐜𝐭𝐨𝐫 𝐃𝐁 & 𝐈𝐧𝐝𝐞𝐱
➞ Store vectors in specialized vector databases like Pinecone, Weaviate, Qdrant, Milvus - created by Zilliz, or pgvector.
➞ You can also use traditional databases like Elastic MongoDB for document storage, leveraging their vector search capabilities if available and suitable.

5./ 𝐑𝐞𝐭𝐫𝐢𝐞𝐯𝐞 𝐈𝐧𝐟𝐨𝐫𝐦𝐚𝐭𝐢𝐨𝐧
➞ Retrieve relevant context using dense vector search (similarity search), sparse retrieval (e.g., BM25, SPLADE), or sophisticated hybrid fusion methods (e.g., RRF, reciprocal rank fusion) via frameworks like LangChain, LlamaIndex, or Haystack. Implement re-ranking (e.g., using bge-reranker or Cohere Rerank) for improved precision.

6./ 𝐎𝐫𝐜𝐡𝐞𝐬𝐭𝐫𝐚𝐭𝐞 𝐭𝐡𝐞 𝐏𝐢𝐩𝐞𝐥𝐢𝐧𝐞
➞ Build your workflow and manage the flow of information between components using orchestration frameworks like LangChain, LlamaIndex, or dedicated workflow automation platforms like n8n or cloud services like Google Cloud Vertex AI Pipelines.

7./ 𝐒𝐞𝐥𝐞𝐜𝐭 𝐋𝐋𝐌𝐬 𝐟𝐨𝐫 𝐆𝐞𝐧𝐞𝐫𝐚𝐭𝐢𝐨𝐧
➞ Integrate your preferred Large Language Models (LLMs) such as Claude, GPT (e.g., GPT-4o), Gemini, Llama 3, DeepSeek, or Mistral via direct APIs or through AI gateways and routing services like Portkey, Eden, or OpenRouter for consistent access and management.

8./ 𝐀𝐝𝐝 𝐎𝐛𝐬𝐞𝐫𝐯𝐚𝐛𝐢𝐥𝐢𝐭𝐲
➞ Monitor and troubleshoot your RAG system using dedicated observability platforms like Langfuse, PromptLayer, Helicone (YC W23), or Arize AI to track prompt performance, token usage, latency, system health, and model outputs.

9./ 𝐄𝐯𝐚𝐥𝐮𝐚𝐭𝐞 & 𝐈𝐦𝐩𝐫𝐨𝐯𝐞
➞ Continuously test and refine retrieval and generation outputs using automated evaluation metrics (e.g., faithfulness, answer relevance, context recall/precision), A/B tests, human feedback loops, and fine-tuning (if necessary) for better quality and performance.

This workflow breaks down every stage of a successful Vector RAG pipeline.

Save this guide, it’s your starting point.

----

Follow

Last editedOpens edit history8:18 AM · Oct 20, 2025·38KViews

Last editedOpens edit history8:18 AM · Oct 20, 2025·38KViews

Last editedOpens edit history8:18 AM · Oct 20, 2025·38KViews

Last editedOpens edit history8:18 AM · Oct 20, 2025·38KViews

Last editedOpens edit history8:18 AM · Oct 20, 2025

New to X?Sign up now to get your own personalized timeline!Sign up with AppleCreate accountBy signing up, you agree to theTerms of ServiceandPrivacy Policy, includingCookie Use.Trending nowWhat’s happeningTrending in GermanyBäume2,250 postsTrendingGovernance39.4K postsTrendingToken209K postsBusiness & finance · Trending#NemesisShow more

New to X?Sign up now to get your own personalized timeline!Sign up with AppleCreate accountBy signing up, you agree to theTerms of ServiceandPrivacy Policy, includingCookie Use.Trending nowWhat’s happeningTrending in GermanyBäume2,250 postsTrendingGovernance39.4K postsTrendingToken209K postsBusiness & finance · Trending#NemesisShow more

New to X?Sign up now to get your own personalized timeline!Sign up with AppleCreate accountBy signing up, you agree to theTerms of ServiceandPrivacy Policy, includingCookie Use.Trending nowWhat’s happeningTrending in GermanyBäume2,250 postsTrendingGovernance39.4K postsTrendingToken209K postsBusiness & finance · Trending#NemesisShow more

New to X?Sign up now to get your own personalized timeline!Sign up with AppleCreate accountBy signing up, you agree to theTerms of ServiceandPrivacy Policy, includingCookie Use.Trending nowWhat’s happeningTrending in GermanyBäume2,250 postsTrendingGovernance39.4K postsTrendingToken209K postsBusiness & finance · Trending#NemesisShow more

New to X?Sign up now to get your own personalized timeline!Sign up with AppleCreate accountBy signing up, you agree to theTerms of ServiceandPrivacy Policy, includingCookie Use.Trending nowWhat’s happeningTrending in GermanyBäume2,250 postsTrendingGovernance39.4K postsTrendingToken209K postsBusiness & finance · Trending#NemesisShow more

New to X?Sign up now to get your own personalized timeline!Sign up with AppleCreate accountBy signing up, you agree to theTerms of ServiceandPrivacy Policy, includingCookie Use.Trending nowWhat’s happeningTrending in GermanyBäume2,250 postsTrendingGovernance39.4K postsTrendingToken209K postsBusiness & finance · Trending#NemesisShow more

New to X?Sign up now to get your own personalized timeline!Sign up with AppleCreate accountBy signing up, you agree to theTerms of ServiceandPrivacy Policy, includingCookie Use.

Sign up now to get your own personalized timeline!

Sign up now to get your own personalized timeline!

Sign up with AppleCreate account

By signing up, you agree to theTerms of ServiceandPrivacy Policy, includingCookie Use.

Trending nowWhat’s happeningTrending in GermanyBäume2,250 postsTrendingGovernance39.4K postsTrendingToken209K postsBusiness & finance · Trending#NemesisShow more

What’s happeningTrending in GermanyBäume2,250 postsTrendingGovernance39.4K postsTrendingToken209K postsBusiness & finance · Trending#NemesisShow more

What’s happeningTrending in GermanyBäume2,250 postsTrendingGovernance39.4K postsTrendingToken209K postsBusiness & finance · Trending#NemesisShow more

Trending in GermanyBäume2,250 posts

Trending in GermanyBäume2,250 posts

Trending in GermanyBäume2,250 posts

Trending in GermanyBäume2,250 posts

TrendingGovernance39.4K posts

TrendingGovernance39.4K posts

TrendingGovernance39.4K posts

TrendingGovernance39.4K posts

TrendingToken209K posts

TrendingToken209K posts

TrendingToken209K posts

TrendingToken209K posts

Business & finance · Trending#Nemesis

Business & finance · Trending#Nemesis

Business & finance · Trending#Nemesis

Business & finance · Trending#Nemesis

Business & finance · Trending

Business & finance · Trending

Business & finance · Trending

