{
  "id": "1ea7e550-1432-42a7-b48b-d8f63ad68364",
  "question": "Hallo, ich bin eine neugierige Studentin und lese gerade ein komplexes Übersichtsartikel über große Sprachmodelle. Kannst du mir erklären, was der Zweck der Kettenüberlegung beim Prompting ist?",
  "reference_answer": "Die Kettenüberlegung kann eingesetzt werden, um das Lernen im Kontext zu verbessern, indem eine Reihe von Zwischenschritten des Denkens in die Prompts einbezogen werden.",
  "reference_context": "Document 1650: Unnamed: 0: 1650\ntext: In addition, chain-of-thought prompting [33] can be employed to enhance in-context learning by involv- ing a series of intermediate reasoning steps in prompts. Furthermore, planning [432] is proposed for solving complex tasks, which first breaks them down into smaller sub-tasks and then generates a plan of action to solve these sub-tasks one by one. We summarize representative work for these prompting approaches in Table 11. Next, we will elaborate on the details of the four techniques.\n\n6.1 Prompting\n\nAs discussed in previous work [36], prompting is the major approach to utilizing LLMs for solving various tasks. Since the quality of prompts will largely influence the perfor- mance of LLMs in specific tasks, there have been a series of studies proposed to generate suitable task prompts through manual creation or automatic optimization, which will be introduced in this section.\n\n6.1.1 Prompt Creation\n\nThe process of manually creating a suitable prompt is also called prompt engineering [445, 446]. A well-designed prompt is very helpful to elicit the abilities of LLMs for accomplish- ing specific tasks. In this part, we will first introduce the key components of prompts and discuss several principles for prompt design. Then, we evaluate ChatGPT with differ- ent prompts to show the results on several representative tasks. We are aware that there have been several existing papers [446, 447] and websites [448–450] that present the suggestions and guidelines to design good prompts. As a comparison, we mainly aim to discuss the key factors (ingre- dients and principles) that are useful for prompt creation, and provide experimental results and analysis on popular tasks as the reference to the beginners.\n\nKey Ingredients.\nref_doc_id: general_surveys\/2303.18223v16.pdf",
  "conversation_history": [],
  "metadata": {
    "question_type": "situational",
    "seed_document_id": 1650,
    "situational_context": "Jane, a curious undergraduate student, casually browses through a dense survey paper, trying to generate basic questions to better understand the various techniques and principles behind effective prompt creation for large language models.",
    "topic": "Prompt Engineering for Large Language Models"
  }
}
{
  "id": "96f7234e-8736-4a97-8ab2-42c5302e7fbe",
  "question": "Hallo, ich bereite gerade eine Zusammenfassung für eine Konferenz vor und suche nach Informationen über aktuelle Umfragen zu Halluzinationen in großen Sprachmodellen. Könnten Sie mir sagen, was das Thema der Umfrage von L. Huang und Kollegen in CoRR ist?",
  "reference_answer": "Eine Umfrage über Halluzinationen in großen Sprachmodellen: Prinzipien, Taxonomie, Herausforderungen und offene Fragen.",
  "reference_context": "Document 2553: Unnamed: 0: 2553\ntext: abs\/2306.13394, 2023.\n\n[843] Y. Zhang, Y. Li, L. Cui, D. Cai, L. Liu, T. Fu, X. Huang, E. Zhao, Y. Zhang, Y. Chen, L. Wang, A. T. Luu, W. Bi,\n\n135\n\nF. Shi, and S. Shi, “Siren’s song in the AI ocean: A survey on hallucination in large language models,” CoRR, vol. abs\/2309.01219, 2023.\n\n[844] A. Gunjal, J. Yin, and E. Bas, “Detecting and prevent- ing hallucinations in large vision language models,” CoRR, vol. abs\/2308.06394, 2023. J. Lu, J. Rao, K. Chen, X. Guo, Y. Zhang, B. Sun, C. Yang, and J. Yang, “Evaluation and mitigation of agnosia in multimodal large language models,” CoRR, vol. abs\/2309.04041, 2023.\n\n[845]\n\n[846] A. Rohrbach, L. A. Hendricks, K. Burns, T. Darrell, and K. Saenko, “Object hallucination in image cap- tioning,” in EMNLP. Association for Computational Linguistics, 2018, pp. 4035–4045.\n\n[847] Y. Li, Y. Du, K. Zhou, J. Wang, W. X. Zhao, and J.-R.\nref_doc_id: general_surveys\/2303.18223v16.pdf\n\nDocument 2462: Unnamed: 0: 2462\ntext: abs\/2305.06983, 2023.\n\n[663] L. Huang, W. Yu, W. Ma, W. Zhong, Z. Feng, H. Wang, Q. Chen, W. Peng, X. Feng, B. Qin, and T. Liu, “A survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions,” CoRR, vol. abs\/2311.05232, 2023. [664] Y. Li, Y. Du, K. Zhou, J. Wang, W. X. Zhao, and J. Wen, “Evaluating object hallucination in large vision-language models,” CoRR, vol. abs\/2305.10355, 2023.\n\n[665] S. Kadavath, T. Conerly, A. Askell, T. J. Henighan, D. Drain, E. Perez, N. Schiefer, Z. Dodds, N. Das- Sarma, E. Tran-Johnson, S. Johnston, S. El-Showk, A. Jones, N. Elhage, T. Hume, A. Chen, Y. Bai, S. Bowman, S. Fort, D. Ganguli, D. Hernandez, J. Ja- cobson, J. Kernion, S. Kravec, L. Lovitt, K. Ndousse, C. Olsson, S. Ringer, D. Amodei, T. B. Brown, J. Clark, N. Joseph, B. Mann, S. McCandlish, C. Olah, and J. Kaplan, “Language models (mostly) know what they know,” CoRR, vol.\nref_doc_id: general_surveys\/2303.18223v16.pdf",
  "conversation_history": [],
  "metadata": {
    "question_type": "situational",
    "seed_document_id": 2553,
    "situational_context": "A researcher in AI is compiling information for a conference by summarizing recent survey papers on hallucinations in large language models.",
    "topic": "Hallucination Detection in Language Models"
  }
}
{
  "id": "7bc3573e-bbb9-4267-a153-8b612edbb05d",
  "question": "Hallo, ich bereite gerade eine Präsentation über die Sicherheitslücken von großen Sprachmodellen vor. Könnten Sie mir sagen, welche Methode die sememe-basierte Wortersetzungsmethode mit einem Suchalgorithmus zur Generierung von adversarialen Beispielen kombiniert?",
  "reference_answer": "[264] kombiniert die sememe-basierte Wortersetzungsmethode und einen Suchalgorithmus basierend auf der Partikelschwarmoptimierung, um adversariale Beispiele zu generieren.",
  "reference_context": "Document 2881: Unnamed: 0: 2881\ntext: [264] combines the sememe-based word substitution method and search algorithm based on particle swarm optimization to generate adversarial samples.\n\nModel Defects Some unrelated human factors can also mislead the PFM to make wrong predictions. For example, [33] discovers that the performance of BERT is limited in the reasoning task due to utilizing false statistical information in the dataset, which dramatically affects the performance by destroying this property. [265] deﬁnes universal adversarial triggers. When triggers are connected to any input, it can induce the model to generate speciﬁc predictions.\n\nBackdoor Attacks There are still many methods to manipulate the predicted results of the pretraining model employing a backdoor attack. [266] demonstrates that it is possible to construct a weight poisoning attack in which pretrained weights are injected. After the ﬁne-tuning stage, the backdoor is exposed. At- tackers manipulate model predictions easily by injecting arbitrary keywords. [267] shows that PFMs in NLP can be manipulated by modifying the model corpus. The “meaning” of new words or existing words can be controlled by changing their weight parameters.\n\nDefense Against Attacks The human-in-the-loop method [31, 32] has been proposed and applied to gen- erate more natural, efﬁcient, and diversiﬁed adversarial samples. Some defense approaches have been pro- posed to defend against such attacks. [268] designs an auxiliary anomaly detection classiﬁer and uses a multi-task learning procedure to defend against adversarial samples. On the other hand, some defects in the PFM may be inherited by the custom models in transfer learning, such as the adversarial vulnerabilities and backdoors mentioned above.\nref_doc_id: general_surveys\/2302.09419v3.pdf\n\nDocument 2880: Unnamed: 0: 2880\ntext: [262] successfully attack the three target models of BERT, CNN, and RNN by generating natural adversarial samples, which indicates that the current language processing model still has a large room for improvement in terms of security. However, it is difﬁcult to achieve due to the distinct discreteness of languages in NLP. In particular, the generation of adversarial sam-\n\n38\n\nples in the text must take into account linguistic characteristics to ensure that the sample’s syntax and ﬂuency are not harmed while affecting the model’s output. For example, [263] uses adversarial samples to attack the ﬁne-tuning stage of the BERT model for text classiﬁcation and entailment successfully. [264] combines the sememe-based word substitution method and search algorithm based on particle swarm optimization to generate adversarial samples.\n\nModel Defects Some unrelated human factors can also mislead the PFM to make wrong predictions. For example, [33] discovers that the performance of BERT is limited in the reasoning task due to utilizing false statistical information in the dataset, which dramatically affects the performance by destroying this property. [265] deﬁnes universal adversarial triggers. When triggers are connected to any input, it can induce the model to generate speciﬁc predictions.\n\nBackdoor Attacks There are still many methods to manipulate the predicted results of the pretraining model employing a backdoor attack. [266] demonstrates that it is possible to construct a weight poisoning attack in which pretrained weights are injected. After the ﬁne-tuning stage, the backdoor is exposed. At- tackers manipulate model predictions easily by injecting arbitrary keywords.\nref_doc_id: general_surveys\/2302.09419v3.pdf",
  "conversation_history": [],
  "metadata": {
    "question_type": "situational",
    "seed_document_id": 2881,
    "situational_context": "A graduate student in computer science is preparing for a presentation on the security weaknesses of large language models.",
    "topic": "Machine Learning Model Security and Privacy"
  }
}
{
  "id": "e481985c-3c6d-44d5-b67b-de11cc1361cc",
  "question": "Hallo, ich bin gerade mitten in einer intensiven Forschungssitzung und frage mich, welches Papier sich mit der Verbesserung der NLG-Bewertung durch LLM-Paraphrasierung befasst?",
  "reference_answer": "T. Tang, H. Lu, Y. E. Jiang, H. Huang, D. Zhang, W. X. Zhao, und F. Wei, „Not all metrics are guilty: Improving NLG evaluation with LLM paraphrasing,“ CoRR, vol. abs\/2305.15067, 2023.",
  "reference_context": "Document 2449: Unnamed: 0: 2449\ntext: abs\/2302.04023, 2023.\n\n[641] Y. Liu, A. R. Fabbri, P. Liu, Y. Zhao, L. Nan, R. Han, S. Han, S. R. Joty, C. Wu, C. Xiong, and D. Radev, “Revisiting the gold standard: Grounding summa- rization evaluation with robust human evaluation,” CoRR, vol. abs\/2212.07981, 2022.\n\n[642] A. R. Fabbri, W. Kryscinski, B. McCann, C. Xiong, R. Socher, and D. R. Radev, “Summeval: Re- evaluating summarization evaluation,” Trans. Assoc. Comput. Linguistics, vol. 9, pp. 391–409, 2021. [643] T. Tang, H. Lu, Y. E. Jiang, H. Huang, D. Zhang, W. X. Zhao, and F. Wei, “Not all metrics are guilty: Improv- ing NLG evaluation with LLM paraphrasing,” CoRR, vol. abs\/2305.15067, 2023.\n\n[644] X. Wang, X. Tang, W. X. Zhao, J. Wang, and J. Wen, “Rethinking the evaluation for conversational rec- ommendation in the era of large language models,” CoRR, vol. abs\/2305.13112, 2023.\nref_doc_id: general_surveys\/2303.18223v16.pdf\n\nDocument 2450: Unnamed: 0: 2450\ntext: Assoc. Comput. Linguistics, vol. 9, pp. 391–409, 2021. [643] T. Tang, H. Lu, Y. E. Jiang, H. Huang, D. Zhang, W. X. Zhao, and F. Wei, “Not all metrics are guilty: Improv- ing NLG evaluation with LLM paraphrasing,” CoRR, vol. abs\/2305.15067, 2023.\n\n[644] X. Wang, X. Tang, W. X. Zhao, J. Wang, and J. Wen, “Rethinking the evaluation for conversational rec- ommendation in the era of large language models,” CoRR, vol. abs\/2305.13112, 2023.\n\n[645] M. Gao, J. Ruan, R. Sun, X. Yin, S. Yang, and X. Wan, “Human-like summarization evaluation with chat- gpt,” CoRR, vol. abs\/2304.02554, 2023.\n\n[646] Y. Ji, Y. Gong, Y. Peng, C. Ni, P. Sun, D. Pan, B. Ma, and X. Li, “Exploring chatgpt’s ability to rank con- tent: A preliminary study on consistency with hu- man preferences,” CoRR, vol. abs\/2303.07610, 2023.\nref_doc_id: general_surveys\/2303.18223v16.pdf",
  "conversation_history": [],
  "metadata": {
    "question_type": "situational",
    "seed_document_id": 2449,
    "situational_context": "Amid an intense research session, a computer science student wonders how insights from large language models influence the latest advancements in natural language generation evaluation.",
    "topic": "Others"
  }
}
{
  "id": "af4ede3d-1c5b-4c4a-b293-1d661974bd56",
  "question": "Hallo, ich bin ein neugieriger KI-Forscher und beschäftige mich gerade mit der Kodierungsmethodik von Wissensgraphen. Könnten Sie mir bitte erklären, wie Song et al. das Problem des Informationsverlusts in großen Graphen adressieren?",
  "reference_answer": "Song et al. kodieren Graphensemantik mit einem graph-state LSTM, das Informationsweitergabe zwischen Knoten während einer Reihe von Zustandsübergängen ermöglicht.",
  "reference_context": "Document 83: Unnamed: 0: 83\ntext: It could handle cycles in KGs for capturing global information. Meanwhile, its linearized graph nature could still result in considerable structural information loss, especially for large graphs. To address this issue, Song et al. [177] encode graph semantics with a graph-state LSTM which enables information propagation between nodes during a series of state transitions. It proves to be capable of modeling non-local interactions between nodes while also efficient due to high parallelization. Zhao et al. [175] propose DUALENC, a dual encoding model, to bridge the structural discrepancy between input graph and output text. Specifically, it utilizes a GCN-based graph encoder to extract structural information, while a neural planner is also adopted to create a sequential content plan of a graph for generating linear output text. Alternatively, Koncel-Kedziorski et al. [178] encode graph structure for text generation with a transformer-based architecture extended from the graph attention network (GAT) [179]. The idea is to compute the node representations of KG by traversing its local neighborhood with self-attention mechanism. In contrast, Ribeiro et al. [180] focus on utilizing local and global node encoding strategies jointly to capture complementary information from graph contexts. Adapted from transformer, HetGT [181] aims at modeling different relationships\n\nJ. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.\n\n111:19\n\n111:20\n\nYihan Cao, Siyu Li, Yixin Liu, Zhiling Yan, Yutong Dai, Philip S. Yu, and Lichao Sun\n\nin the graph independently to avoid information loss by simply mixing them.\nref_doc_id: general_surveys\/2303.04226v1.pdf",
  "conversation_history": [],
  "metadata": {
    "question_type": "situational",
    "seed_document_id": 83,
    "situational_context": "A curious AI researcher delves into the intricacies of knowledge graph encoding methods, contemplating how these advanced techniques might improve language model interpretations.",
    "topic": "Generative Artificial Intelligence"
  }
}
{
  "id": "b1889618-6bbf-47d7-bf56-e581223f178f",
  "question": "Hallo zusammen, ich bereite gerade eine Präsentation über den Einfluss von Instruction Tuning Techniken auf verschiedene Sprachmodelle vor. Kann mir jemand sagen, was eine wichtige Ressource zur Erstellung von Aufgabenbeschreibungen für verschiedene Datensätze ist?",
  "reference_answer": "Eine wichtige Ressource zur Erstellung von Aufgabenbeschreibungen für verschiedene Datensätze ist die Crowd-Sourcing-Plattform PromptSource.",
  "reference_context": "Document 1535: Unnamed: 0: 1535\ntext: As important public resources, existing studies have released a large number of labeled data format- ted in natural language (see the list of available resources in Table 3) as introduced in Section 3.3.1. Next, we introduce four major methods for constructing formatted instances (see an illustration in Figure 11) and then discuss several key factors for instance construction.\n\nFormatting NLP Task Datasets. Before instruction tuning was proposed, several early studies [181, 332, 333] collected the instances from a diverse range of traditional NLP tasks (e.g., text summarization, text classification, and translation) to create supervised multi-task training datasets. As a major source of instruction tuning instances, it is convenient to for- mat these multi-task training datasets with natural language task descriptions. Specifically, recent work [28, 66, 67, 88] augments the labeled datasets with human-written task de- scriptions, which instructs LLMs to understand the tasks by explaining the task goal. For example, in Figure 11(a), a task description “Please answer this question” is added for each example in the question-answering task. After instruction tuning, LLMs can generalize well to other unseen tasks by following their task descriptions [28, 67, 69]. In particular, it has been shown that instructions are the crucial factor in task generalization ability for LLMs [67]: by fine-tuning the model on labeled datasets with the task descriptions re- moved, it results in a dramatic drop in model performance. To better generate labeled instances for instruction tuning, a crowd-sourcing platform, PromptSource [180] has been proposed to effectively create, share, and verify the task descriptions for different datasets.\nref_doc_id: general_surveys\/2303.18223v16.pdf\n\nDocument 1534: Unnamed: 0: 1534\ntext: After instruction tuning, LLMs can demonstrate superior abilities to generalize to unseen tasks [28, 67, 69], even in a multilingual setting [94].\n\nA recent survey [331] presents a systematic overview of the research on instruction tuning. In comparison to that, we mainly focus on the effect of instruction tuning on LLMs and provide detailed guidelines or strategies for instance collection and tuning. In addition, we also discuss the use of instruction tuning for satisfying the real needs of users, which has been widely applied in existing LLMs, e.g., InstructGPT [66] and GPT-4 [46].\n\n5.1.1 Formatted Instance Construction\n\nGenerally, an instruction-formatted instance consists of a task description (called an instruction), an optional input, the corresponding output, and a small number of demon- strations (optional). As important public resources, existing studies have released a large number of labeled data format- ted in natural language (see the list of available resources in Table 3) as introduced in Section 3.3.1. Next, we introduce four major methods for constructing formatted instances (see an illustration in Figure 11) and then discuss several key factors for instance construction.\n\nFormatting NLP Task Datasets. Before instruction tuning was proposed, several early studies [181, 332, 333] collected the instances from a diverse range of traditional NLP tasks (e.g., text summarization, text classification, and translation) to create supervised multi-task training datasets. As a major source of instruction tuning instances, it is convenient to for- mat these multi-task training datasets with natural language task descriptions.\nref_doc_id: general_surveys\/2303.18223v16.pdf",
  "conversation_history": [],
  "metadata": {
    "question_type": "situational",
    "seed_document_id": 1535,
    "situational_context": "A graduate student preparing a class presentation on the impact of instruction tuning techniques across various language models seeks to craft engaging questions for their peers to prompt discussion.",
    "topic": "Others"
  }
}
{
  "id": "bf7a4ef1-be7f-419a-982c-fba640aae39f",
  "question": "Hallo, ich habe gerade einen Überblick über große Sprachmodelle gelesen und frage mich, wofür die Graphklassifikation (GC) häufig in sozialen und molekularen Daten verwendet wird?",
  "reference_answer": "Die Graphklassifikation (GC) wird häufig in sozialen, molekularen und Protein-Grafikdaten verwendet, um die Eigenschaft der gegebenen Gemeinschaft, chemischen Verbindung und des Proteins vorherzusagen.",
  "reference_context": "Document 3025: Unnamed: 0: 3025\ntext: 224, 200, 543] [201, 202, 198, 209, 210, 211, 541, 544] [203, 188, 189, 545, 546, 542, 194, 224, 543, 200] [201, 202, 198, 209, 210, 211, 544] [545] [545] [542, 211] [212] [542, 224, 543, 202, 211] [542, 211] [215] [215] [212]\n\n67\n\nGraph Classiﬁcation Graph Classiﬁcation (GC) is commonly used in social, molecular, and protein graph data, which aims to predict the property of the given community, chemical compound, and protein. The statistic results as shown in Table 9.\n\nTable 9: The statistics of the datasets for GC. Homogeneous:Hom, Heterogeneous:Het.\nref_doc_id: general_surveys\/2302.09419v3.pdf\n\nDocument 3026: Unnamed: 0: 3026\ntext: 202, 198, 209, 210, 211, 541, 544] [203, 188, 189, 545, 546, 542, 194, 224, 543, 200] [201, 202, 198, 209, 210, 211, 544] [545] [545] [542, 211] [212] [542, 224, 543, 202, 211] [542, 211] [215] [215] [212]\n\n67\n\nGraph Classiﬁcation Graph Classiﬁcation (GC) is commonly used in social, molecular, and protein graph data, which aims to predict the property of the given community, chemical compound, and protein. The statistic results as shown in Table 9.\n\nTable 9: The statistics of the datasets for GC. Homogeneous:Hom, Heterogeneous:Het.\n\nName ZINC15 ChEMBL PPI-pre MUTAG PTC BBBP Tox21 ToxCast SIDER ClinTox MUV HIV BACE PPI-88K IMDB-M IMDB-B FreeSolv ESOL Lipophilicity QM7 QM8 COLLAB RDT-B RDT-M NCI1 NCI109 PROTEINS D&D Mutagenicity METR-LA\n\nUsage Pretraining Pretraining Pretraining Downstream Downstream Downstream Downstream Downstream Downstream Downstream Downstream Downstream Downstream Downstream Downstream Downstream Downstream Downstream Downstream Downstream Downstream Downstream Downstream Downstream Downstream Downstream Downstream Downstream Downstream Downstream\n\nSource Molecule Molecule Protein Molecule Molecule Molecule Molecule Molecule Molecule Molecule Molecule Molecule Molecule Protein Movie Movie Molecule Molecule Molecule Molecule Molecule Co-author Co-author Co-author Molecule Molecule Molecule Molecule Molecule Trafﬁc\n\nType Hom Hom Het Hom Hom Hom Hom Hom Hom Hom Hom Hom Hom Het Hom Hom Hom Hom Hom Hom Hom Hom Hom Hom Hom Hom Hom Hom Hom Hom\n\nGraphs 2M 456K 395K 188 344 2,\nref_doc_id: general_surveys\/2302.09419v3.pdf\n\nDocument 3024: Unnamed: 0: 3024\ntext: 429\n\n4,732\n\n44K\n\n100K 1M 172K - 819K 240M 73K 443K 2B\n\nClass\n\n7\n\n6\n\n3\n\n5 5 6 - 121 9 122 52 -\n\nFeatures\n\n1,433\n\n3,703\n\n500\n\n- 8,189 - 50 12,047 - - -\n\nRelated Paper [203, 188, 189, 545, 546, 214, 194, 224, 543, 542] [200, 201, 202, 198, 209, 210, 211, 544] [203, 188, 189, 546, 542, 194, 224, 200, 543] [201, 202, 198, 209, 210, 211, 541, 544] [203, 188, 189, 545, 546, 542, 194, 224, 543, 200] [201, 202, 198, 209, 210, 211, 544] [545] [545] [542, 211] [212] [542, 224, 543, 202, 211] [542, 211] [215] [215] [212]\n\n67\n\nGraph Classiﬁcation Graph Classiﬁcation (GC) is commonly used in social, molecular, and protein graph data, which aims to predict the property of the given community, chemical compound, and protein. The statistic results as shown in Table 9.\n\nTable 9: The statistics of the datasets for GC.\nref_doc_id: general_surveys\/2302.09419v3.pdf",
  "conversation_history": [],
  "metadata": {
    "question_type": "situational",
    "seed_document_id": 3025,
    "situational_context": "Curious about a survey paper on large language models, a researcher jots down a basic question about the methods used for graph classification statistics in social and molecular data.",
    "topic": "Graph Datasets and Classification"
  }
}
{
  "id": "d69fdf9c-3523-449f-879c-0d9462288d98",
  "question": "Hallo, ich bin Datenanalyst in einem Tech-Unternehmen und untersuche, wie sich große Sprachmodelle an unterschiedliche Datenverteilungen anpassen können. Welche Methode hat speziell die Generalisierungsfähigkeit von LLMs verbessert?",
  "reference_answer": "Die Methode des Reinforcement Learning from Human Feedback (RLHF) hat die Generalisierungsfähigkeit von LLMs verbessert.",
  "reference_context": "Document 4433: Unnamed: 0: 4433\ntext: In a brief summary: LLMs are more versatile w.r.t. the data availability, while fine-tuned models can be considered\n\nwith abundant annotated data.\n\n3.3 Test data\/user data\n\nWhen deploying LLMs for downstream tasks, we often face challenges stemming from distributional differences between\n\nthe test\/user data and that of the training data. These disparities may encompass domain shifts [132], out-of-distribution\n\nvariations [31], or even adversarial examples [82]. Such challenges significantly hinder fine-tuned modes’ effectiveness\n\nin real-world applications. They fit into a specific distribution and have a poor ability to generalize to OOD data.\n\nHowever, LLMs perform quite well facing such scenarios because they do not have an explicit fitting process. Moreover,\n\nrecent advancements have further enhanced the ability of language models in this regard. The Reinforcement Learning\n\nfrom Human Feedback (RLHF) method has notably enhanced LLMs’ generalization capabilities [77]. For example,\n\nInstructGPT demonstrates proficiency in following various instructions for a wide range of tasks and occasionally\n\ncomplying with instructions in different languages, even though such instructions are scarce. Similarly, ChatGPT\n\nexhibits consistent advantages on most adversarial and out-of-distribution (OOD) classification and translation tasks\n\n[109]. Its superiority in understanding dialogue-related texts led to an impressive performance on the DDXPlus dataset\n\n[101], a medical diagnosis dataset designed for OOD evaluation.\n\n4 PRACTICAL GUIDE FOR NLP TASKS\n\nIn this section, we discuss in detail the use cases and no use cases for LLMs in various downstream NLP tasks and the\n\ncorresponding model abilities. And in Figure 2, we summarize all discussions into a decision flow.\nref_doc_id: general_surveys\/2304.13712v2.pdf",
  "conversation_history": [],
  "metadata": {
    "question_type": "situational",
    "seed_document_id": 4433,
    "situational_context": "A data analyst in a tech company is skimming through survey papers on large language models to better understand the adaptability of models like ChatGPT for varying data distributions in customer service applications.",
    "topic": "Large Language Models Applications"
  }
}
{
  "id": "c222dae4-443e-4b40-b30b-6f8576c15d14",
  "question": "Hallo, ich bereite gerade eine Seminarpräsentation über fortgeschrittene Transformer-Architekturen vor. Könnten Sie mir bitte erklären, wie der Reward r im Kontext eines gegebenen Eingangs x und Ausgangs y ausgedrückt wird?",
  "reference_answer": "r = Reward(x,y), wobei Reward(·) das Belohnungsmodell ist.",
  "reference_context": "Document 904: Unnamed: 0: 904\ntext: Given an input x and an output y, the reward can be expressed as\n\nr = Reward(x,y)\n\nwhere Reward(·) is the reward model. r can be interpreted as a measure of how well the output y aligns with the desired behavior given the input x. As discussed in the previous subsection, both x\n\n3The training loss for the value network (or critic network) in A2C is generally formulated as the mean squared error between the computed return rt +γV (st+1) and the predicted state value V (st). Suppose that the value network is parameterized by ω. The loss function is given by\n\nLv(ω) =\n\n1 M\n\nX(cid:0)rt + γVω(st+1) − Vω(st)(cid:1)2\n\nwhere M is the number of training samples, for example, for a sequence of T tokens, we can set M = T.\n\n179\n\n(4.29)\n\n(4.30)\n\n(4.31)\n\n(4.33)\n\n(4.32)\n\n180\n\nAlignment\n\nReward (Scalar)\n\nWr\n\nLinear Map\n\nRepresentation\n\nhx0\n\nhx1\n\nhx2\n\n···\n\nhxm\n\nhy1\n\nhy2\n\n···\n\nhlast\n\nat Each Position\n\nTransformer Decoder (LLM)\n\nx0\n\nx1\n\nx2\n\n···\n\nxm\n\ny1\n\ny2\n\nyn ··· (Last Token ⟨EOS⟩)\n\nFig. 4.8: Architecture of the reward model based on Transformer. The main component of this model is still an LLM. We use the Transformer decoder as the sequence representation model. We extract the representation of the last position of the decoder as the representation of the entire sequence [x,y].\nref_doc_id: general_surveys\/2501.09223v2.pdf",
  "conversation_history": [],
  "metadata": {
    "question_type": "situational",
    "seed_document_id": 904,
    "situational_context": "A graduate student is preparing for a seminar presentation on advanced transformer architectures and their applications in language models.",
    "topic": "Reward Modeling and Ranking in LLMs"
  }
}
{
  "id": "45233aab-0fb7-49e5-a668-380d25776592",
  "question": "Hallo, ich interessiere mich sehr für die Herausforderungen bei der Aktualisierung von großen Sprachmodellen. Könnten Sie mir erklären, welche speziellen Schwierigkeiten beim Aktualisieren von isoliertem Modellverhalten oder Faktenwissen auftreten?",
  "reference_answer": "Das Aktualisieren von isoliertem Modellverhalten oder Faktenwissen kann teuer und ungezielt sein, was unbeabsichtigte Nebeneffekte verursachen könnte.",
  "reference_context": "Document 3988: Unnamed: 0: 3988\ntext: However, re-training the model with updated pre-training data is expensive, and trying to “unlearn” old facts and learn new ones during fine-tuning is non-trivial.\n\nExisting model editing techniques are lim- ited in their effectiveness of updating isolated knowledge [642, 205]. For example, Hoelscher- Obermaier et al. [205] find that model edits can result in unintended associations. This low speci- ficity limits their applicability to real-world use cases, where only a single faulty or outdated bit of information should be updated in a model, and related pieces of information must reflect this up- date in information equally, without unrelated ones being changed.\n\nIsolated Model Updates without Side- Effects [205]\n\nUpdating isolated model behavior or factual knowledge can be expensive and untargeted, which might cause unintended side-effects.\n\nTwo popular approaches for addressing this is- sue are Model editing [513, 642], which aims at “bug-fixing” models efficiently and leveraging non-parametric knowledge sources in retrieval- augmented language modeling (which we omit here and detail in Sec. 2.8). Current model editing techniques change the model’s behavior by mod- ifying the model parameters or using an external post-edit model.\n\nModifying Model Parameters techniques can be further split into locate-then-edit methods [102, 360, 361] which first locate the “buggy” part of the model parameters and then apply an update to them to alter their behavior, and meta-learning methods [111, 372] which use an external model to predict the weight update.\nref_doc_id: general_surveys\/2307.10169v1.pdf",
  "conversation_history": [],
  "metadata": {
    "question_type": "situational",
    "seed_document_id": 3988,
    "situational_context": "A curious tech enthusiast wonders about the challenges and innovative strategies involved in updating large language models while avoiding unintended associations or errors.",
    "topic": "Large Language Models in Behavioral and Cognitive Research"
  }
}
{
  "id": "da1ad1a1-b024-462a-b565-51fb777c3203",
  "question": "Hallo, auf der Tech-Konferenz habe ich gehört, dass große Sprachmodelle wie GPT-3.5 bei quantitativen Aufgaben in regulatorischen Prüfungen Schwierigkeiten haben. Können Sie mir sagen, wie GPT-3.5 in solchen Szenarien abschneidet?",
  "reference_answer": "Das beste Modell (neuestes GPT-3.5) hat Schwierigkeiten mit quantitativer Argumentation und erzielt Ergebnisse, die ähnlich wie zufälliges Raten bei Multiple-Choice-Fragen sind.",
  "reference_context": "Document 4086: Unnamed: 0: 4086\ntext: [49] evaluate GPT-3.5 and previous GPT ver- sions on actual and synthetic questions from the Uniform CPA Examination Regulation section and AICPA Blueprints for legal, financial, accounting, technology, and ethical tasks. Using only zero-shot prompting, the best performing model (latest GPT- 3.5) struggles with quantitative reasoning, achiev-\n\nUser Prompt\n\nGeneral Prompt\n\nPre-processing\n\nModule 1\n\nLLM\n\nOutput\n\nGeneral Prompt\n\nLLM\n\nResidual\n\nOutput\n\nPre-processing\n\nGeneral Prompt\n\nRe-run\n\nEg., Generate a plot outline for a new novel as paragraph headings\n\nEg., Using the outline, generate a draft for the xth paragraph heading\n\nEg., Check the spelling and consistency of this paragraph given the outline and plot summary\n\nIterate\n\nModule 3\n\nModule 2\n\nOutput\n\nLLM\n\nFigure 14: Modular Prompting. Illustration of using a series of separate prompts [368, 637, 368, 579, 584] and processing steps to enable an LLM to perform tasks that would either not fit in a single context window or could not easily be specified in a single prompting step.\n\ning results similar to random guessing on multiple- choice questions. However, on qualitative sections, GPT-3.5 achieved 50-70% accuracy, significantly ahead of random guessing and approaching human- level scores.\n\nNumerical Reasoning [436, 49]\n\nLLMs have generally seen worse perfor- mance on quantitative tasks, potentially con- straining their applications in knowledge work areas such as financial services or ac- counting.\n\nWu et al.\nref_doc_id: general_surveys\/2307.10169v1.pdf",
  "conversation_history": [],
  "metadata": {
    "question_type": "situational",
    "seed_document_id": 4086,
    "situational_context": "A researcher at a tech conference raises a question about the challenges large language models face with quantitative reasoning in regulatory exams.",
    "topic": "Large Language Models in Behavioral and Cognitive Research"
  }
}
{
  "id": "dc5e4741-4cde-4bd2-bade-95138c7c9ff4",
  "question": "Hallo, ich arbeite an meiner Abschlussarbeit über fortgeschrittene Fähigkeiten von Sprachmodellen. Könnten Sie mir sagen, welche drei grundlegenden Fähigkeiten bei der Evaluierung von LLMs betrachtet werden?",
  "reference_answer": "Die drei grundlegenden Fähigkeiten sind Sprachgenerierung, Wissensnutzung und komplexes Denken.",
  "reference_context": "Document 1740: Unnamed: 0: 1740\ntext: 55\n\n7 CAPACITY AND EVALUATION\n\nTo examine the effectiveness and superiority of LLMs, a surge of tasks and benchmarks have been proposed for conducting empirical ability evaluation and analysis. In this section, we first introduce three types of basic ability evalu- ation of LLMs for language generation and understanding, then present several advanced ability evaluations with more complicated settings or goals, and finally discuss existing benchmarks, evaluation approaches, and empirical analysis.\n\n7.1 Basic Ability\n\nIn this part, we mainly focus on three basic types of ability evaluation for LLMs, i.e., language generation, knowledge utilization, and complex reasoning. It is noted that we do not intend to have complete coverage of all the related tasks, but instead only focus on the most widely discussed or studied tasks for LLMs. Next, we introduce these tasks in detail.\n\n7.1.1 Language Generation\n\nAccording to the task definition, existing tasks about lan- guage generation can be roughly categorized into language modeling, conditional text generation, and code synthesis tasks. Note that code synthesis is not a typical NLP task, we include it for discussion because it can be directly solved by a number of LLMs (trained on code data) in a similar generation approach as natural language text.\n\nLanguage Modeling. As the most fundamental ability of LLMs, language modeling aims to predict the next token based on the previous tokens [1], which mainly focuses on the capacity of basic language understanding and gen- eration. For evaluating such an ability, typical language modeling datasets that existing work uses include Penn Treebank [540], WikiText-103 [541], and the Pile [166], where the metric of perplexity is commonly used for evaluating the model performance under the zero-shot setting.\nref_doc_id: general_surveys\/2303.18223v16.pdf",
  "conversation_history": [],
  "metadata": {
    "question_type": "situational",
    "seed_document_id": 1740,
    "situational_context": "A graduate student preparing a thesis on advanced language model capabilities is compiling a list of common evaluation benchmarks and tasks for LLMs to ensure they've covered major assessment areas.",
    "topic": "LLM Evaluation and Benchmarking"
  }
}
{
  "id": "00af1da7-db68-4a5c-8b5e-672003a0b7c4",
  "question": "Hallo, ich bin ein neugieriger Postgraduiertenstudent und lese gerade ein Übersichtsartikel über große Sprachmodelle. Können Sie mir erklären, wie die Forscher die Berechnungskosten senken, wenn sie ein Dokument in mehrere Segmente unterteilen, um die Risiken von KI zu verstehen?",
  "reference_answer": "Die Berechnungskosten können gesenkt werden, indem das Dokument in relativ kurze Segmente unterteilt wird und die gleiche Aufgabe auf jedem Segment ausgeführt wird. Diese Segmente können parallel verarbeitet werden, um die Berechnungskosten weiter zu reduzieren.",
  "reference_context": "Document 692: Unnamed: 0: 692\ntext: For example, consider a problem of determining whether a document discusses the risks of AI. We can instruct the LLM with the following prompt.\n\nYou are provided with a text. Please determine whether it discusses the risks of AI.\n\n{∗document∗}\n\nIf the document is long, the computation will be expensive. Alternatively, we can divide the document into relatively short segments and perform the same task on each segment. These segments can be processed in parallel to further reduce the computational cost. Next, we determine\n\n119\n\n120\n\nPrompting\n\nthe relevancy of each segment to the topic of AI risks. The final output is then generated using another prompt.\n\nYour task is to determine whether a text discusses the risks of AI. This text has been divided into segments, and you have obtained the relevancy of each segment to the topic of AI risks. Based on this, please provide your final result.\n\nSegment 1: {∗relevancy-to-the-topic1∗}\n\nSegment 2: {∗relevancy-to-the-topic2∗}\n\nSegment 3: {∗relevancy-to-the-topic3∗}\n\n...\n\nNow let us return to a more general discussion of problem decomposition in prompting. While problem decomposition can be applied to various NLP problems, it has been more extensively discussed and tested in reasoning tasks recently. For complex reasoning tasks, we often need a multi-step reasoning path to reach a correct conclusion. We can use LLMs to achieve this in three different ways. First, LLMs can directly reach the conclusion. In other words, they can predict without explicit reasoning processes, and there is a hidden and uninterpretable reasoning mechanism.\nref_doc_id: general_surveys\/2501.09223v2.pdf",
  "conversation_history": [],
  "metadata": {
    "question_type": "situational",
    "seed_document_id": 692,
    "situational_context": "A curious postgraduate student perusing a general survey paper about large language models wonders about the researchers' approach to dissecting and understanding AI risks discussed in multiple document segments.",
    "topic": "Prompt Engineering for Large Language Models"
  }
}
{
  "id": "d20cb250-af68-4256-8e2f-244dd8629f39",
  "question": "Hallo, ich bereite gerade eine Präsentation über Datensets zur Bewertung der Robustheit von Modellen für maschinelles Leseverständnis vor. Könnten Sie mir erklären, was Dureaderrobust ist?",
  "reference_answer": "Dureaderrobust ist ein chinesisches Datenset zur Bewertung der Robustheit von Modellen für maschinelles Leseverständnis.",
  "reference_context": "Document 3776: Unnamed: 0: 3776\ntext: 518–533. 26\n\n[361] D. Khashabi, S. Chaturvedi, M. Roth, S. Upadhyay, and D. Roth, “Looking beyond the surface: A challenge set for reading comprehen- sion over multiple sentences,” in Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), 2018, pp. 252–262. 26\n\n[362] T. Kwiatkowski, J. Palomaki, O. Redfield, M. Collins, A. Parikh, C. Alberti, D. Epstein, I. Polosukhin, J. Devlin, K. Lee et al., “Natural questions: a benchmark for question answering research,” Transactions of the Association for Computational Linguistics, vol. 7, pp. 453–466, 2019. 26\n\n[363] C. C. Shao, T. Liu, Y. Lai, Y. Tseng, and S. Tsai, “Drcd: A chinese machine reading comprehension dataset,” arXiv preprint arXiv:1806.00920, 2018. 26\n\n[364] W. He, K. Liu, J. Liu, Y. Lyu, S. Zhao, X. Xiao, Y. Liu, Y. Wang, H. Wu, Q. She et al., “Dureader: a chinese machine reading comprehension dataset from real-world applications,” arXiv preprint arXiv:1711.05073, 2017.\nref_doc_id: general_surveys\/2307.06435v8.pdf\n\nDocument 3778: Unnamed: 0: 3778\ntext: 26\n\n[365] H. Tang, J. Liu, H. Li, Y. Hong, H. Wu, and H. Wang, “Dureaderrobust: A chinese dataset towards evaluating the robustness of machine reading comprehension models,” arXiv preprint arXiv:2004.11142, 2020. 26\n\n[366] J. Welbl, N. F. Liu, and M. Gardner, “Crowdsourcing multiple choice science questions,” arXiv preprint arXiv:1707.06209, 2017. 26 [367] C. Xiong, Z. Dai, J. Callan, Z. Liu, and R. Power, “End-to-end neural ad-hoc ranking with kernel pooling,” in Proceedings of the 40th International ACM SIGIR conference on research and development in information retrieval, 2017, pp. 55–64. 26\n\n[368] A. Peñas, E. Hovy, P. Forner, Á. Rodrigo, R. Sutcliffe, and R. Morante, “Qa4mre 2011-2013: Overview of question answering for machine reading evaluation,” in Information Access Evaluation. Multilinguality, Multimodality, and Visualization: 4th International Conference of the CLEF Initiative, CLEF 2013, Valencia, Spain, September 23-26, 2013. Proceedings 4. Springer, 2013, pp. 303–320. 26\n\n[369] S. Lim, M. Kim, and J. Lee, “Korquad1. 0: Korean qa dataset for machine reading comprehension,” arXiv preprint arXiv:1909.07005, 2019.\nref_doc_id: general_surveys\/2307.06435v8.pdf\n\nDocument 3775: Unnamed: 0: 3775\ntext: 26, 28\n\n[359] Y. Cui, T. Liu, Z. Yang, Z. Chen, W. Ma, W. Che, S. Wang, and G. Hu, “A sentence cloze dataset for chinese machine reading comprehension,” arXiv preprint arXiv:2004.03116, 2020. 26\n\n[360] Y. Li, T. Liu, D. Li, Q. Li, J. Shi, and Y. Wang, “Character-based bilstm-crf incorporating pos and dictionaries for chinese opinion target extraction,” in Asian Conference on Machine Learning. PMLR, 2018, pp. 518–533. 26\n\n[361] D. Khashabi, S. Chaturvedi, M. Roth, S. Upadhyay, and D. Roth, “Looking beyond the surface: A challenge set for reading comprehen- sion over multiple sentences,” in Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), 2018, pp. 252–262. 26\n\n[362] T. Kwiatkowski, J. Palomaki, O. Redfield, M. Collins, A. Parikh, C. Alberti, D. Epstein, I. Polosukhin, J. Devlin, K. Lee et al., “Natural questions: a benchmark for question answering research,” Transactions of the Association for Computational Linguistics, vol. 7, pp. 453–466, 2019.\nref_doc_id: general_surveys\/2307.06435v8.pdf",
  "conversation_history": [],
  "metadata": {
    "question_type": "situational",
    "seed_document_id": 3776,
    "situational_context": "A graduate student is preparing a presentation on datasets used to evaluate the robustness of machine reading comprehension models in various languages.",
    "topic": "Question Answering Datasets"
  }
}
{
  "id": "b015dcb0-ac7f-4d8d-9dbe-61df40963e7f",
  "question": "Hallo, ich bereite gerade ein Seminar über die Implementierung von strukturierten Sprachverarbeitungs-Frameworks in großen Sprachmodellen vor. Könnten Sie mir erklären, was 'Rails' in der fortgeschrittenen Prompt-Entwicklung für LLMs sind?",
  "reference_answer": "Rails in der fortgeschrittenen Prompt-Entwicklung beziehen sich auf eine Methode, um die Ausgabe von Large Language Models (LLMs) durch vordefinierte Regeln oder Vorlagen zu steuern und zu kontrollieren. Diese Methode soll sicherstellen, dass die Antworten des Modells bestimmten Standards oder Kriterien entsprechen, um die Relevanz, Sicherheit und Genauigkeit der Ausgabe zu verbessern.",
  "reference_context": "Document 4679: Unnamed: 0: 4679\ntext: The LLM is prompted to consider responses from multiple expert perspectives, which are then synthesized to form a comprehensive and well-rounded answer. This tech- nique not only enhances the depth of the response but also incorporates a range of viewpoints, reflecting a more holistic understanding of the subject matter.\n\n6) Chains: Chains refer to the method of linking multiple components in a sequence to handle complex tasks with Large Language Models (LLMs). This approach involves creating a series of interconnected steps or processes, each contributing to the final outcome. The concept of Chains is based on the idea of constructing a workflow where different stages or components are sequentially arranged. Each component in a Chain performs a specific function, and the output of one serves as the input for the next. This end-to-end arrangement allows for more complex and nuanced processing, as each stage can be tailored to handle a specific aspect of the task. Chains can vary in complexity and structure, depending on the requirements. In “PromptChainer: Chaining Large Lan- guage Model Prompts through Visual Programming” [162], the authors not only describe the main challenges in designing chains, but also describe a visual tool to support those tasks.\n\n7) Rails: Rails in advanced prompt engineering refer to a method of guiding and controlling the output of Large\n\nLanguage Models (LLMs) through predefined rules or tem- plates. This approach is designed to ensure that the model’s responses adhere to certain standards or criteria, enhancing the relevance, safety, and accuracy of the output. The concept of Rails involves setting up a framework or a set of guidelines that the LLM must follow while generating responses. These guidelines are typically defined using a modeling language or templates known as Canonical Forms, which standardize the way natural language sentences are structured and delivered.\nref_doc_id: general_surveys\/2402.06196v3.pdf\n\nDocument 4680: Unnamed: 0: 4680\ntext: Chains can vary in complexity and structure, depending on the requirements. In “PromptChainer: Chaining Large Lan- guage Model Prompts through Visual Programming” [162], the authors not only describe the main challenges in designing chains, but also describe a visual tool to support those tasks.\n\n7) Rails: Rails in advanced prompt engineering refer to a method of guiding and controlling the output of Large\n\nLanguage Models (LLMs) through predefined rules or tem- plates. This approach is designed to ensure that the model’s responses adhere to certain standards or criteria, enhancing the relevance, safety, and accuracy of the output. The concept of Rails involves setting up a framework or a set of guidelines that the LLM must follow while generating responses. These guidelines are typically defined using a modeling language or templates known as Canonical Forms, which standardize the way natural language sentences are structured and delivered.\n\nRails can be designed for various purposes, depending on\n\nthe specific needs of the application:\n\n\n\nTopical Rails: Ensure that particular topic or domain.\n\nthe LLM sticks to a\n\n\n\nFact-Checking Rails: Aimed at minimizing the gen- eration of false or misleading information.\nref_doc_id: general_surveys\/2402.06196v3.pdf",
  "conversation_history": [],
  "metadata": {
    "question_type": "situational",
    "seed_document_id": 4679,
    "situational_context": "A graduate student in the field of artificial intelligence is preparing to present a seminar on the implementation of structured language processing frameworks in large language models.",
    "topic": "Prompt Engineering for Large Language Models"
  }
}
{
  "id": "ecfb8181-a376-4424-8a9b-d1be4ef453d2",
  "question": "Hallo, ich bin ein Informatikstudent und interessiere mich für Innovationen im Deep Learning. Könnten Sie mir bitte sagen, was das Hauptthema des Papiers 'Tesseract' von B. Wang, Q. Xu, Z. Bian und Y. You ist?",
  "reference_answer": "Das Hauptthema des Papiers 'Tesseract' ist die effiziente Parallelisierung der Tensor-Parallelität.",
  "reference_context": "Document 2283: Unnamed: 0: 2283\ntext: abs\/1710.03740, 2017.\n\n[324] Q. Xu, S. Li, C. Gong, and Y. You, “An efficient 2d method for training super-large deep learning models,” CoRR, vol. abs\/2104.05343, 2021.\n\n[325] B. Wang, Q. Xu, Z. Bian, and Y. You, “Tesseract: Parallelize the tensor parallelism efficiently,” in Pro- ceedings of the 51st International Conference on Parallel Processing, ICPP 2022, Bordeaux, France, 29 August 2022 - 1 September 2022. ACM, 2022.\n\n[326] Z. Bian, Q. Xu, B. Wang, and Y. You, “Maximizing parallelism in distributed training for huge neural networks,” CoRR, vol. abs\/2105.14450, 2021. [327] S. Li, F. Xue, C. Baranwal, Y. Li, and Y. You, “Se- quence parallelism: Long sequence training from system perspective,” arXiv e-prints, pp. arXiv–2105, 2021.\n\n[328] L. Zheng, Z. Li, H. Zhang, Y. Zhuang, Z. Chen, Y. Huang, Y. Wang, Y. Xu, D. Zhuo, E. P. Xing et al., “Alpa: Automating inter-and {Intra-Operator} parallelism for distributed deep learning,” in OSDI, 2022, pp. 559–578.\nref_doc_id: general_surveys\/2303.18223v16.pdf",
  "conversation_history": [],
  "metadata": {
    "question_type": "situational",
    "seed_document_id": 2283,
    "situational_context": "Curious about innovations in deep learning, a computer science student skims through recent survey papers to grasp the various parallel processing methods utilized in training super-large neural networks.",
    "topic": "Large Language Models and Training Systems"
  }
}
{
  "id": "aeda618c-c74a-4577-bf05-0f7998b06f3c",
  "question": "Hallo, ich bin ein Forscher und interessiere mich dafür, wie sich verschiedene Bewertungseinstellungen auf die Leistung großer Sprachmodelle auswirken. Könnten Sie mir erklären, welche Vorteile die menschliche Bewertung bei der Beurteilung der Fähigkeiten von LLMs bietet?",
  "reference_answer": "Ein Vorteil der menschlichen Bewertung ist ihre Fähigkeit, die tatsächlichen Fähigkeiten von LLMs direkt widerzuspiegeln. Sie bietet eine direktere Messung der Leistung von LLMs in realen Szenarien und ermöglicht eine tiefere Einsicht in die Stärken und Schwächen von LLMs in verschiedenen Aufgaben und Kontexten.",
  "reference_context": "Document 1815: Unnamed: 0: 1815\ntext: The tasks involved in these benchmarks often contain sufficient test samples to measure the core abilities (e.g., reasoning). The whole evaluation procedure can be (almost) automatic, and it is convenient to carry out test experiments for various base LLMs, especially useful for monitoring the performance of model checkpoints during pre-training. However, LLMs are often sensitive to the eval- uation settings, including the question prompts, zero-shot or few-shot tests, and the answer parsing methods. Thus, one should take possible influencing factors into consideration when conducting the evaluation experiments. The evalua- tion results should be noted with the adopted evaluation settings. Another issue is the data contamination [56, 740], i.e., the test data itself or relevant content has been contained in the pre-training corpora. This phenomenon has become increasingly severe since more and more open data has been collected for developing LLMs.\n\nHuman-based approach. Human evaluation offers several advantages when assessing the capabilities of LLMs to solve real-world tasks. One of the key benefits is its ability to directly reflect the actual abilities of LLMs. Based on feed- back and experiences from real users, human evaluation provides a more direct measure of LLMs’ performance in real-world scenarios. Further, it can conduct more flexible and diverse evaluation tasks based on human evaluators. For instance, users can submit various queries and test the abilities of LLMs according to their own task cognition. It allows for a deep understanding of the strengths and weak- nesses of LLMs across different types of tasks and contexts. However, human evaluation also has inherent limitations that could potentially affect its accuracy and consistency.\nref_doc_id: general_surveys\/2303.18223v16.pdf\n\nDocument 1816: Unnamed: 0: 1816\ntext: This phenomenon has become increasingly severe since more and more open data has been collected for developing LLMs.\n\nHuman-based approach. Human evaluation offers several advantages when assessing the capabilities of LLMs to solve real-world tasks. One of the key benefits is its ability to directly reflect the actual abilities of LLMs. Based on feed- back and experiences from real users, human evaluation provides a more direct measure of LLMs’ performance in real-world scenarios. Further, it can conduct more flexible and diverse evaluation tasks based on human evaluators. For instance, users can submit various queries and test the abilities of LLMs according to their own task cognition. It allows for a deep understanding of the strengths and weak- nesses of LLMs across different types of tasks and contexts. However, human evaluation also has inherent limitations that could potentially affect its accuracy and consistency. Factors such as personalized tastes and varying education levels among evaluators can introduce biases or even incon- sistencies in the evaluation process. In some cases, users’ judgments are likely to be subjective, which may not reflect the true capabilities of the LLMs. Moreover, conducting robust and reliable human evaluations often requires a large number of evaluators, which can be very expensive and time-consuming. In addition, human evaluation is often not reproducible, making it infeasible to extend existing evaluation results or track the progress of LLMs.\n\nModel-based approach. As a surrogate for human-based approaches, model-based approaches serve to diminish the reliance on human involvement, and enable more efficient and scalable evaluation. In addition, LLMs can provide meaningful explanations for the assigned rating scores, thereby enhancing the interpretability of evaluations.\nref_doc_id: general_surveys\/2303.18223v16.pdf",
  "conversation_history": [],
  "metadata": {
    "question_type": "situational",
    "seed_document_id": 1815,
    "situational_context": "A researcher wants to understand the impact of different evaluation settings on the performance of large language models.",
    "topic": "LLM Evaluation and Benchmarking"
  }
}
{
  "id": "9820a875-2e03-40a3-8fe9-ef6945a9b1a1",
  "question": "Hallo, ich bin ein KI-Student und habe die ganze Woche über Forschungsarbeiten gelesen. Können Sie mir erklären, wie die Paraphrasierung in der Prompt-Generierung von Haviv et al. [43] durchgeführt wird? Ich denke darüber nach, wie ich diese Technik für meine Abschlussarbeit über natürliche Sprachverarbeitung nutzen kann.",
  "reference_answer": "Haviv et al. [43] führen die Paraphrasierung durch, nachdem die Eingabe x in die Prompt-Vorlage eingefügt wurde, sodass für jede einzelne Eingabe eine andere Paraphrase generiert werden kann.",
  "reference_context": "Document 3245: Unnamed: 0: 3245\ntext: Frequent middle words or dependency paths can serve as a template as in “[X] middle words [Z]”\n\ne D2: Prompt Paraphrasing. Paraphrasing-based approaches take in an existing seed prompt (e.g., manually constructed or mined), paraphrase it into a set of other candidate prompts, and then selects the one that achieves the highest training accuracy on the target task. This paraphrasing can be done in a number of ways, including using round-trip trans- lation of the prompt into another language then back [52], using replacement of phrases from a thesaurus [147], or using a neural prompt rewriter specifically optimized to improve accuracy of systems using the prompt [43]. Notably, Haviv et al. [43] perform paraphrasing after the input x is input into the prompt template, allowing a different paraphrase to be generated for each individual input.\n\ne D3: Gradient-based Search. Wallace et al. [138] applied a gradient-based search over actual tokens to find short sequences that can trigger the underlying pre-trained LM to generate the desired target prediction. This search is done in an iterative fashion, stepping through tokens in the prompt. Built upon this method, Shin et al. [125] automatically search for template tokens using downstream application training samples and demonstrates strong performance in prompting scenarios.\n\nD4: Prompt Generation. Other works treat the generation of prompts as a text genera-\n\ntion task and use standard natural language generation models to perform this task. For\n\nexample, Gao et al. [32] introduce the seq2seq pre-trained LM T5 into the template search process.\nref_doc_id: general_surveys\/3560815.pdf\n\nDocument 3246: Unnamed: 0: 3246\ntext: Notably, Haviv et al. [43] perform paraphrasing after the input x is input into the prompt template, allowing a different paraphrase to be generated for each individual input.\n\ne D3: Gradient-based Search. Wallace et al. [138] applied a gradient-based search over actual tokens to find short sequences that can trigger the underlying pre-trained LM to generate the desired target prediction. This search is done in an iterative fashion, stepping through tokens in the prompt. Built upon this method, Shin et al. [125] automatically search for template tokens using downstream application training samples and demonstrates strong performance in prompting scenarios.\n\nD4: Prompt Generation. Other works treat the generation of prompts as a text genera-\n\ntion task and use standard natural language generation models to perform this task. For\n\nexample, Gao et al. [32] introduce the seq2seq pre-trained LM T5 into the template search process. Since T5 has been pre-trained on a task of filling in missing spans, they use T5 to\n\ngenerate template tokens by (1) specifying the position to insert template tokens within a\n\ntemplate® and (2) providing training samples for T5 to decode template tokens. Guo et al.\n\n[36] use reinforcement learning [132] to generate prompts to control the text generation\n\nprocess. Ben-David et al. [5] propose a domain adaptation algorithm that trains T5 to gener-\n\nate unique domain relevant features (DRFs) (a set of keywords that characterize domain\n\ninformation) for each input. Then those DRFs can be concatenated with the input to form a\n\ntemplate and be further used by downstream tasks.\n\nD5: Prompt Scoring. Davison et al.\nref_doc_id: general_surveys\/3560815.pdf",
  "conversation_history": [],
  "metadata": {
    "question_type": "situational",
    "seed_document_id": 3245,
    "situational_context": "With an overcast mind from a week buried in research papers, a graduate student in AI contemplated how prompt generation techniques could streamline their thesis on natural language processing.",
    "topic": "Others"
  }
}
{
  "id": "6059414e-1bfd-4e46-a78a-0716a180478f",
  "question": "Hallo, ich bereite gerade eine Präsentation über die Transparenz von Open-Source-Large-Language-Models für eine akademische Konferenz vor. Könnten Sie mir sagen, was das Ziel von LLM360 laut dem Artikel von Z. Liu und anderen ist?",
  "reference_answer": "Das Ziel von LLM360 ist es, vollständig transparente Open-Source-LLMs zu entwickeln.",
  "reference_context": "Document 5442: Unnamed: 0: 5442\ntext: [703] C. T. Wolf, “Democratizing ai? experience and accessibility in the age of artificial intelligence,” XRDS: Crossroads, The ACM Magazine for Students, vol. 26, no. 4, pp. 12–15, 2020.\n\n[704] Z. Liu, A. Qiao, W. Neiswanger, H. Wang, B. Tan, T. Tao, J. Li, Y. Wang, S. Sun, O. Pangarkar, et al., “Llm360: Towards fully transparent open-source llms,” arXiv preprint arXiv:2312.06550, 2023. [705] S.-h. Huang and C.-y. Chen, “Combining lora to gpt-neo to reduce\n\nlarge language model hallucination,” 2024.\n\n[706] T. Le Scao, A. Fan, C. Akiki, E. Pavlick, S. Ili´c, D. Hesslow, R. Castagné, A. S. Luccioni, F. Yvon, M. Gallé, et al., “Bloom: A 176b-parameter open-access multilingual language model,” 2023.\nref_doc_id: general_surveys\/682263.pdf",
  "conversation_history": [],
  "metadata": {
    "question_type": "situational",
    "seed_document_id": 5442,
    "situational_context": "A postgraduate student is preparing a presentation on the transparency of open-source large language models for an upcoming academic conference.",
    "topic": "Others"
  }
}
{
  "id": "2f0bdd1c-8047-4909-bbb2-12159ceea285",
  "question": "Hallo, ich bereite eine Präsentation über die Entwicklung von großen Sprachmodellen vor und möchte wissen, wer die Autoren des Papiers 'Exploring the limits of transfer learning with a unified text-to-text transformer' sind?",
  "reference_answer": "Die Autoren werden im gegebenen Kontext nicht genannt.",
  "reference_context": "Document 1240: Unnamed: 0: 1240\ntext: Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of Machine Learning Research, 21(140):1–67, 2020.\n\n[Ramachandran et al., 2017] Prajit Ramachandran, Barret Zoph, and Quoc V Le. Searching for activation\n\nfunctions. arXiv preprint arXiv:1710.05941, 2017.\n\n[Rolnick et al., 2019] David Rolnick, Arun Ahuja, Jonathan Schwarz, Timothy Lillicrap, and Gregory Wayne. Experience replay for continual learning. Advances in Neural Information Processing Systems, 32, 2019.\n\n[Rosenfeld et al., 2020] Jonathan S Rosenfeld, Amir Rosenfeld, Yonatan Belinkov, and Nir Shavit. A con- structive prediction of the generalization error across scales. In Proceedings of International Conference on Learning Representations, 2020.\n\n[Ruan et al., 2024] Junhao Ruan, Long Meng, Weiqiao Shan, Tong Xiao, and Jingbo Zhu. A survey of llm\n\nsurveys. https:\/\/github.com\/NiuTrans\/ABigSurveyOfLLMs, 2024.\n\n[Rubin et al., 2022] Ohad Rubin, Jonathan Herzig, and Jonathan Berant. Learning to retrieve prompts for in-context learning. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 2655–2671, 2022.\n\n[Russell, 2019] Stuart Russell. Human Compatible: Artificial Intelligence and the Problem of Controls.\n\nViking, 2019.\nref_doc_id: general_surveys\/2501.09223v2.pdf",
  "conversation_history": [],
  "metadata": {
    "question_type": "situational",
    "seed_document_id": 1240,
    "situational_context": "A computer science student is preparing a presentation on the evolution and impact of large language models, specifically focusing on key papers and advancements over recent years.",
    "topic": "Large Language Models Research"
  }
}
