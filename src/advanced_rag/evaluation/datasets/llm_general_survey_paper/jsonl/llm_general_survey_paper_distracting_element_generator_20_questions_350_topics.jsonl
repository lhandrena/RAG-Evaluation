{
  "id": "4025393e-68cc-408b-84a8-265a9ba67fa1",
  "question": "Unter der Bedingung, dass das Papier von Richard Shin und anderen aus dem Jahr 2021 sich mit den Herausforderungen von LLMs in Regressionstasks beschäftigt, wie lautet der Titel dieses Papiers?",
  "reference_answer": "Constrained language models yield few-shot semantic parsers.",
  "reference_context": "Document 3388: Unnamed: 0: 3388\ntext: 2016. Improving neural machine translation models with mono- lingual data. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Association for Computational Linguistics, 86-96. https:\/\/doi.org\/10.18653\/v1\/P16-1009\n\nRichard Shin, Christopher H. Lin, Sam Thomson, Charles Chen, Subhro Roy, Emmanouil Antonios Platanios, Adam Pauls, Dan Klein, Jason Eisner, and Benjamin Van Durme. 2021. Constrained language models yield few-shot se- mantic parsers. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP’21), Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih (Eds.). Association for Computational Linguistics, 7699-7715. https:\/\/doi.org\/10.18653\/v1\/2021.emnlp- main.608\n\nTaylor Shin, Yasaman Razeghi, Robert L. Logan IV, Eric Wallace, and Sameer Singh. 2020. AutoPrompt: Eliciting knowledge from language models with automatically generated prompts. In Proceedings of the Empirical Methods in Natural Language Processing (EMNLP’20).\n\nJake Snell, Kevin Swersky, and Richard Zemel. 2017. Prototypical networks for few-shot learning. In Advances in Neural Information Processing Systems, Vol. 30.\n\nJake Snell, Kevin Swersky, and Richard S. Zemel. 2017. Prototypical networks for few-shot learning.\nref_doc_id: general_surveys\/3560815.pdf",
  "conversation_history": [],
  "metadata": {
    "question_type": "distracting element",
    "seed_document_id": 3388,
    "distracting_context": "Unnamed: 0: 4466\ntext: They can also be used for quality\n\nevaluation in NLP tasks and have bonuses like interpretability.\n\n4.5.1 No use case. LLMs generally struggle with some tasks due to differences in objectives and training data.\n\nAlthough LLMs have achieved remarkable success in various natural language processing tasks, their performance\n\nin regression tasks has been less impressive. For example, ChatGPT’s performance on the GLUE STS-B dataset, which is\n\na regression task evaluating sentence similarity, is inferior to a fine-tuned RoBERTa performance [130]. The Regression\n\ntasks typically involve predicting a continuous value rather than a discrete label, posing unique challenges for LLMs. One\n\nprimary reason for their subpar performance is the inherent difference between the language modeling objective and the\n\nregression task objective. LLMs are designed to predict the next word in a sequence or generate coherent text, with their\n\npre-training focused on capturing linguistic patterns and relationships. Consequently, their internal representations\n\nmay not be well-suited for modeling continuous numerical outputs. Besides, LLMs have predominantly been trained\n\non text data, focusing on capturing the intricacies of natural language processing. As a result, their performance on\n\nmultimodal data, which involves handling multiple data types such as text, images, audio, video, actions, and robotics,\n\nremains largely unexplored. And fine-tuned multimodal models, like BEiT[110] and PaLI [19], still dominate many\n\ntasks such as visual question answering (VQA) and image captioning. Nonetheless, the recently introduced GPT-4 [76]\n\nhas taken the step in multimodal fusion, but there is still a lack of detailed evaluation of its capabilities.\n\n4.5.2 Use case. LLMs are particularly suitable for certain tasks.\nref_doc_id: general_surveys\/2304.13712v2.pdf",
    "topic": "Natural Language Processing Research"
  }
}
{
  "id": "20529c63-eda9-4ce7-ad57-9d0bc76f77b3",
  "question": "Welche Modelle werden als Beispiele für vortrainierte Sprachmodelle in der modernen Ära genannt, insbesondere unter der Bedingung, dass sie effektiv in der Lage sind, den Raum der Argumentation zu erkunden und unversprechende Pfade frühzeitig zu beschneiden?",
  "reference_answer": "BERT [Devlin et al., 2019] und GPT [Brown et al., 2020]",
  "reference_context": "Document 296: Unnamed: 0: 296\ntext: These pre-trained models serve as foundation models that can be easily adapted to different tasks via fine-tuning or prompting. As a result, the paradigm of NLP has been enormously changed. In many cases, large-scale supervised learning for specific tasks is no longer required, and instead, we only need to adapt pre-trained foundation models.\n\nWhile pre-training has gained popularity in recent NLP research, this concept dates back decades to the early days of deep learning. For example, early attempts to pre-train deep learning systems include unsupervised learning for RNNs, deep feedforward networks, autoencoders, and others [Schmidhuber, 2015]. In the modern era of deep learning, we experienced a resurgence of pre-training, caused in part by the large-scale unsupervised learning of various word embedding models [Mikolov et al., 2013b; Pennington et al., 2014]. During the same period, pre-training also attracted significant interest in computer vision, where the backbone models were trained on relatively large labeled datasets such as ImageNet, and then applied to different downstream tasks [He et al., 2019; Zoph et al., 2020]. Large-scale research on pre-training in NLP began with the development of language models using self-supervised learning. This family of models covers several well-known examples like BERT [Devlin et al., 2019] and GPT [Brown et al., 2020], all with a similar idea that general language understanding and generation can be achieved by train- ing the models to predict masked words in a huge amount of text. Despite the simple nature of this approach, the resulting models show remarkable capability in modeling linguistic structure, though they are not explicitly trained to achieve this.\nref_doc_id: general_surveys\/2501.09223v2.pdf",
  "conversation_history": [],
  "metadata": {
    "question_type": "distracting element",
    "seed_document_id": 296,
    "distracting_context": "Unnamed: 0: 1124\ntext: By evaluating or guiding the generation at each intermediate step, the LLM can explore the reasoning space more effectively, potentially pruning unpromising paths early or allocating more resources to explore more plausible ones.\n\n241\n\n242\n\nInference\n\nStep-level search with verifiers can also be modeled as a tree search problem. In this paradigm, each node (or state) corresponds to a partial reasoning path, a≤i = (a1,...,ai), representing the sequence of i reasoning steps taken so far (i.e., a path from the root node to the current node). The objective of the search process is to explore the underlying state space, starting from an initial empty path, to find a complete path that constitutes a correct solution. Note that we use a≤i here to represent a partial reasoning path instead of y≤i. While this makes notation a bit inconsistent with that used for representing complete solutions (y) or full paths in solution-level search, it serves to highlight the focus on individual actions or steps.\n\nThe core components of step-level search with verifiers are:\n\nNode Representation. A node is a partial reasoning path a≤i = (a1,...,ai). The root node is an empty path, and terminal nodes are complete reasoning paths.\n\nNode Expansion. Given a current partial path a≤i, the LLM is used to generate one or i+1,...,a(M) i+1 }. Each candidate step, when appended i+1).\n\nVerification. The verifier V (·) evaluates the quality of a newly generated step in the context of the current partial path a≤i = (a1,...,ai) and the original problem x. As with solution- level verification, step-level verifiers might output a numerical score, a categorical label, and textual feedback.\nref_doc_id: general_surveys\/2501.09223v2.pdf",
    "topic": "Pretrained Language Models"
  }
}
{
  "id": "8be15335-6736-4847-a7c8-3957071e9744",
  "question": "Unter der Bedingung, dass 'Free Dolly' als weltweit erstes wirklich offenes, instruktionsabgestimmtes LLM eingeführt wurde, welche Herausforderungen und Anwendungen von Large Language Models werden im gegebenen Kontext diskutiert?",
  "reference_answer": "'Free Dolly' ist die Einführung des weltweit ersten wirklich offenen, instruktionsabgestimmten LLM.",
  "reference_context": "Document 2212: Unnamed: 0: 2212\ntext: comparison corpus, evaluation, and detec- tion,” arXiv preprint arXiv:2301.07597, 2023.\n\n[185] M. Conover, M. Hayes, A. Mathur, J. Xie, J. Wan, S. Shah, A. Ghodsi, P. Wendell, M. Zaharia, and R. Xin. (2023) Free dolly: Introducing the world’s first truly open instruction-tuned llm.\n\n[186] A. K¨opf, Y. Kilcher, D. von R¨utte, S. Anagnostidis, Z.- R. Tam, K. Stevens, A. Barhoum, N. M. Duc, O. Stan- ley, R. Nagyfi et al., “Openassistant conversations– democratizing large language model alignment,” arXiv preprint arXiv:2304.07327, 2023.\n\n[187] R. Taori, I. Gulrajani, T. Zhang, Y. Dubois, X. Li, C. Guestrin, P. Liang, and T. B. Hashimoto, “Stanford alpaca: An instruction-following llama model,” https:\/\/github.com\/tatsu-lab\/stanford alpaca, 2023. J. Cheung, “Guanaco - generative universal assistant for natural-language adaptive context-aware om- nilingual outputs,” https:\/\/guanaco-model.github. io\/, 2023.\nref_doc_id: general_surveys\/2303.18223v16.pdf",
  "conversation_history": [],
  "metadata": {
    "question_type": "distracting element",
    "seed_document_id": 2212,
    "distracting_context": "Unnamed: 0: 3825\ntext: 3 2 0 2\n\nl u J\n\n9 1\n\n] L C . s c [\n\n1 v 9 6 1 0 1 . 7 0 3 2 : v i X r a\n\nChallenges and Applications of Large Language Models\n\nJean Kaddourα, †, ∗, Joshua Harrisβ, ∗, Maximilian Mozesα, Herbie Bradleyγ, δ, ϵ, Roberta Raileanuζ, and Robert McHardyη, ∗\n\nαUniversity College London βUK Health Security Agency γEleutherAI δUniversity of Cambridge\n\nϵStability AI\n\nζMeta AI Research ηInstaDeep\n\nAbstract\n\nLarge Language Models (LLMs) went from non-existent to ubiquitous in the machine learn- ing discourse within a few years. Due to the fast pace of the field, it is difficult to identify the remaining challenges and already fruitful application areas. In this paper, we aim to es- tablish a systematic set of open problems and application successes so that ML researchers can comprehend the field’s current state more quickly and become productive.\n\nContents\n\n1\n\nIntroduction\n\n1\n\nDesignUnfathomable Datasets, Tokenizer-Reliance,Fine-Tuning OverheadScience Evaluations Based on Static Human-Written Ground Truth,Lacking Experimental Designs,Lack of ReproducibilityBehaviorPrompt Brittleness, Misaligned Behavior,Outdated KnowledgeDetecting Generated Texts, Brittle EvaluationsHigh Pre-Training CostsHigh Inference Latency, Limited Context Length, HallucinationsTasks Not SolvableBy Scale\n\n2 Challenges\n\n. . . 2.1 Unfathomable Datasets . . . .\nref_doc_id: general_surveys\/2307.10169v1.pdf",
    "topic": "Large Language Models"
  }
}
{
  "id": "fb57813e-eb68-4176-b84c-9e8c40820dd2",
  "question": "Unter der Bedingung, dass der Artikel 'Character-llm: A trainable agent for role-playing' im Kontext von Veranstaltungen erwähnt wird, welche Konferenz wird dabei genannt?",
  "reference_answer": "EMNLP 2023 in Singapur.",
  "reference_context": "Document 2611: Unnamed: 0: 2611\ntext: abs\/2305.14325, 2023.\n\n[972] Y. Shao, L. Li, J. Dai, and X. Qiu, “Character-llm: A trainable agent for role-playing,” in Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore, Decem- ber 6-10, 2023, H. Bouamor, J. Pino, and K. Bali, Eds. Association for Computational Linguistics, 2023, pp. 13153–13187.\n\n[973] W. Hua, X. Yang, Z. Li, W. Cheng, and Y. Zhang, “Trustagent: Towards safe and trustworthy llm- based agents through agent constitution,” CoRR, vol. abs\/2402.01586, 2024.\n\n[974] L. Huang, W. Yu, W. Ma, W. Zhong, Z. Feng, H. Wang, Q. Chen, W. Peng, X. Feng, B. Qin, and T. Liu, “A survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions,” CoRR, vol. abs\/2311.05232, 2023. I. Loshchilov and F. Hutter, “Decoupled weight de- cay regularization,” in ICLR (Poster). OpenRe- view.net, 2019.\nref_doc_id: general_surveys\/2303.18223v16.pdf",
  "conversation_history": [],
  "metadata": {
    "question_type": "distracting element",
    "seed_document_id": 2611,
    "distracting_context": "Unnamed: 0: 2559\ntext: abs\/2306.09265, 2023.\n\n[856] Z. Li, Y. Wang, M. Du, Q. Liu, B. Wu, J. Zhang, C. Zhou, Z. Fan, J. Fu, J. Chen, X. Huang, and Z. Wei, “Reform-eval: Evaluating large vision lan- guage models via unified re-formulation of task- oriented benchmarks,” CoRR, vol. abs\/2310.02569, 2023.\n\n[857] B. Li, R. Wang, G. Wang, Y. Ge, Y. Ge, and Y. Shan, “Seed-bench: Benchmarking multimodal\n\nllms with generative comprehension,” CoRR, vol. abs\/2307.16125, 2023.\n\n[858] W. Yu, Z. Yang, L. Li, J. Wang, K. Lin, Z. Liu, X. Wang, and L. Wang, “Mm-vet: Evaluating large multi- modal models for integrated capabilities,” CoRR, vol. abs\/2308.02490, 2023. J. Wang, L. Meng, Z. Weng, B. He, Z. Wu, and Y. Jiang, “To see is to believe: Prompting GPT- 4V for better visual instruction tuning,” CoRR, vol. abs\/2311.07574, 2023.\nref_doc_id: general_surveys\/2303.18223v16.pdf",
    "topic": "Large Language Model Evaluation Benchmarks"
  }
}
{
  "id": "d1772ff8-5729-4215-9d01-483e716bc81e",
  "question": "Welche Normalisierungspositionen werden in LLMs verwendet, insbesondere im Kontext der CoT-Prompting-Ansätze, die eine schrittweise Generierung erfordern?",
  "reference_answer": "Es gibt drei allgemeine Optionen für die Normalisierungsposition: post-LN, pre-LN und sandwich-LN.",
  "reference_context": "Document 1483: Unnamed: 0: 1483\ntext: DeepNorm. DeepNorm is proposed by Microsoft [277] to stabilize the training of deep Transformers. With Deep- Norm as residual connections, Transformers can be scaled up to 1,000 layers [277], which has shown the advantages of stability and good performance. It has been adopted by GLM-130B [93].\n\nNormalization Position. In addition to the normalization method, normalization position also plays a crucial role in the LLMs. There are generally three choices for the normal- ization position, i.e., post-LN, pre-LN, and sandwich-LN.\n\nPost-LN. Post-LN is used in the vanilla Trans- former [22], which is placed between residual blocks. How- ever, existing work has found that the training of Trans- formers with post-LN tends to be instable due to the large gradients near the output layer [286]. Thus, post-LN is rarely employed in existing LLMs except combined with other strategies (e.g., combining post-LN with pre-LN in GLM- 130B [93]).\n\nPre-LN. Different from post-LN, pre-LN [287] is applied before each sub-layer, and an additional LN is placed before the final prediction. Compared with post-LN, the Trans- formers with pre-LN are more stable in training. However, it performs worse than the variants with post-LN [288]. Despite the decreasing performance, most LLMs still adopt pre-LN due to the training stability. However, one excep- tion is that pre-LN has been found unstable in GLM when training models more than 100B parameters [93].\n\nSandwich-LN.\nref_doc_id: general_surveys\/2303.18223v16.pdf\n\nDocument 1482: Unnamed: 0: 1482\ntext: LayerNorm. In the early research, BatchNorm [284] is a commonly used normalization method. However, it is difficult to deal with sequence data of variable lengths and small-batch data. Thus, LayerNorm [275] is introduced to conduct layerwise normalization. Specifically, the mean and variance over all activations per layer are calculated to re- center and re-scale the activations.\n\nRMSNorm. To improve the training speed of Lay- erNorm (LN), RMSNorm [276] is proposed by re-scaling the activations with only the root mean square (RMS) of the summed activations, instead of the mean and variance. Related research has demonstrated its superiority in training speed and performance on Transformer [285]. Representa- tive models that adopt RMSNorm include Gopher [64] and Chinchilla [34].\n\nDeepNorm. DeepNorm is proposed by Microsoft [277] to stabilize the training of deep Transformers. With Deep- Norm as residual connections, Transformers can be scaled up to 1,000 layers [277], which has shown the advantages of stability and good performance. It has been adopted by GLM-130B [93].\n\nNormalization Position. In addition to the normalization method, normalization position also plays a crucial role in the LLMs. There are generally three choices for the normal- ization position, i.e., post-LN, pre-LN, and sandwich-LN.\n\nPost-LN. Post-LN is used in the vanilla Trans- former [22], which is placed between residual blocks. How- ever, existing work has found that the training of Trans- formers with post-LN tends to be instable due to the large gradients near the output layer [286].\nref_doc_id: general_surveys\/2303.18223v16.pdf",
  "conversation_history": [],
  "metadata": {
    "question_type": "distracting element",
    "seed_document_id": 1483,
    "distracting_context": "Unnamed: 0: 1713\ntext: Figure 14 presents an illustration of CoT. In the following part, we will first elaborate on the basic CoT prompting approach and its improved strategies, then discuss when and why CoT prompting works.\n\n6.3.1 Basic CoT Prompting Approach\n\nCoT prompting is first proposed as an extension of ICL [33], which augments each demonstration ⟨input, output⟩ as ⟨input, CoT, output⟩. A CoT is a series of intermediate reasoning steps for connecting the input and output. With these augmented demonstrations, LLMs can follow them to generate CoTs and the answer for a new input. However, unlike ⟨input, output⟩ pairs in ICL, CoTs are difficult to obtain and usually require human annotation. Fortunately, it has been found that LLMs can be triggered to generate CoTs through simple instructions like “Let’s think step by step.” [507], making CoT prompting easy to use. There are also alternative magic prompts that can elicit the ability of CoT reasoning and further improve the performance of LLMs, such as “Take a deep breath and work on this problem step-by-step.” [470].\n\nAs illustrated in Figure 15, the generation process of CoT follows a chain structure in the basic CoT prompt- ing approach, where LLMs generate CoTs step by step. Typically, CoT takes the format of natural language text. However, textual CoTs may not work well on complex tasks that require rigorous logic for reasoning. Considering this, some work uses code [508, 509] due to its structured and precise nature. Furthermore, the authors in [510] propose to dynamically select text or code as the format of CoTs to combine their advantages.\nref_doc_id: general_surveys\/2303.18223v16.pdf",
    "topic": "Large Language Model Architectures"
  }
}
{
  "id": "696ba022-59c7-437c-b4e4-d18f7d829704",
  "question": "Unter der Bedingung, dass der Artikel 'Diffusion models beat gans on image synthesis' in einem der Jahre veröffentlicht wurde, die in den zitierten Arbeiten im Kontext erwähnt werden, in welchem Jahr wurde er dann veröffentlicht?",
  "reference_answer": "2021",
  "reference_context": "Document 5368: Unnamed: 0: 5368\ntext: 4, pp. IV–317, IEEE, 2007.\n\n[526] J. Kukaˇcka, V. Golkov, and D. Cremers, “Regularization for deep learning: A taxonomy,” arXiv preprint arXiv:1710.10686, 2017. [527] A. Sedghi, L. J. O’Donnell, T. Kapur, E. Learned-Miller, P. Mousavi, and W. M. Wells III, “Image registration: Maximum likelihood, min- imum entropy and deep learning,” Medical image analysis, vol. 69, p. 101939, 2021.\n\n[528] X. Pan, A. Tewari, T. Leimkühler, L. Liu, A. Meka, and C. Theobalt, “Drag your gan: Interactive point-based manipulation on the generative image manifold,” in ACM SIGGRAPH 2023 Conference Proceedings, pp. 1–11, 2023.\n\n[529] P. Dhariwal and A. Nichol, “Diffusion models beat gans on image synthesis,” Advances in neural information processing systems, vol. 34, pp. 8780–8794, 2021.\nref_doc_id: general_surveys\/682263.pdf",
  "conversation_history": [],
  "metadata": {
    "question_type": "distracting element",
    "seed_document_id": 5368,
    "distracting_context": "Unnamed: 0: 5423\ntext: [655] Y. LeCun, “A path towards autonomous machine intelligence version\n\n0.9. 2, 2022-06-27,” Open Review, vol. 62, 2022.\n\n[656] M. Hardy, I. Sucholutsky, B. Thompson, and T. Griffiths, “Large language models meet cognitive science: Llms as tools, models, and participants,” in Proceedings of the annual meeting of the cognitive science society, 2023.\n\n[657] A. Wan, E. Wallace, S. Shen, and D. Klein, “Poisoning language models during instruction tuning,” arXiv preprint arXiv:2305.00944, 2023.\n\n[658] H. Li, D. Guo, W. Fan, M. Xu, and Y. Song, “Multi-step jailbreaking\n\nprivacy attacks on chatgpt,” arXiv preprint arXiv:2304.05197, 2023.\n\n[659] F. Perez and I. Ribeiro, “Ignore previous prompt: Attack techniques for language models,” arXiv preprint arXiv:2211.09527, 2022. [660] Y. Liu, G. Deng, Y. Li, K. Wang, T. Zhang, Y. Liu, H. Wang, Y. Zheng, and Y. Liu, “Prompt injection attack against llm-integrated applications,” arXiv preprint arXiv:2306.05499, 2023.\n\n[661] C. Zhang, C. Zhang, T. Kang, D. Kim, S.-H.\nref_doc_id: general_surveys\/682263.pdf",
    "topic": "Deep Generative Models"
  }
}
{
  "id": "ffb35b73-914f-4437-b454-77c56d2ec697",
  "question": "Welche Konferenzbeiträge aus dem Jahr 2024 berücksichtigen die Herausforderungen der Halluzinationen und des Wissensstandes bei großen Sprachmodellen und bieten Lösungsansätze dafür?",
  "reference_answer": "[Ge et al., 2024] Yuan Ge et al. und [Gou et al., 2024] Zhibin Gou et al.",
  "reference_context": "Document 1184: Unnamed: 0: 1184\ntext: [Ge et al., 2024] Yuan Ge, Yilun Liu, Chi Hu, Weibin Meng, Shimin Tao, Xiaofeng Zhao, Hongxia Ma, Li Zhang, Boxing Chen, Hao Yang, Bei Li, Tong Xiao, and Jingbo Zhu. Clustering and rank- ing: Diversity-preserved instruction selection through expert-aligned quality estimation. arXiv preprint arXiv:2402.18191, 2024.\n\n[Gemma Team, 2024] Google DeepMind Gemma Team. Gemma: Open Models Based on Gemini Re-\n\nsearch and Technology, 2024.\n\n[Goodhart, 1984] Charles AE Goodhart. Problems of monetary management: the UK experience. Springer,\n\n1984.\n\n[Gordon et al., 2021] Mitchell A Gordon, Kevin Duh, and Jared Kaplan. Data and parameter scaling laws for neural machine translation. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 5915–5922, 2021.\n\n[Gou et al., 2024] Zhibin Gou, Zhihong Shao, Yeyun Gong, Yujiu Yang, Nan Duan, Weizhu Chen, et al. Critic: Large language models can self-correct with tool-interactive critiquing. In The Twelfth Interna- tional Conference on Learning Representations, 2024.\n\n[Gu and Dao, 2023] Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state\n\nspaces. arXiv preprint arXiv:2312.00752, 2023.\nref_doc_id: general_surveys\/2501.09223v2.pdf",
  "conversation_history": [],
  "metadata": {
    "question_type": "distracting element",
    "seed_document_id": 1184,
    "distracting_context": "Unnamed: 0: 1769\ntext: to “unconsciously” utilize the knowledge in task solving, which still lack an ability to accurately control the use of internal or external knowledge. Hallucinations would mislead LLMs to generate undesired outputs and mostly degrade the performance, leading to potential risks when\n\nHallucination\n\nLLMs are prone to generate untruthful informa- tion that either conflicts with the existing source or cannot be verified by the available source. Even the most powerful LLMs such as ChatGPT face great challenges in migrating the hallucina- tions of the generated texts. This issue can be partially alleviated by special approaches such as alignment tuning and tool utilization.\n\nKnowledge recency. As another major challenge, LLMs would encounter difficulties when solving tasks that require the latest knowledge beyond the training data. To tackle this issue, a straightforward approach is to regularly update LLMs with new data. However, it is very costly to fine-tune LLMs, and also likely to cause the catastrophic forgetting issue when incrementally training LLMs. Therefore, it is necessary to develop efficient and effective approaches that can integrate new knowledge into existing LLMs, making them up-to-date. Existing studies have explored how to utilize the external knowledge source (e.g., search engine)\n\n59\n\nto complement LLMs, which can be either jointly optimized with LLMs [655] or used as a plug-and-play module [661]. For instance, ChatGPT utilizes a retrieval plugin to access up-to-date information sources [667]. By incorporating the extracted relevant information into the context [668–670], LLMs can acquire new factual knowledge and perform better on relevant tasks. However, such an approach seems to be still at a superficial level.\nref_doc_id: general_surveys\/2303.18223v16.pdf",
    "topic": "Large Language Models and Transformers"
  }
}
{
  "id": "f746e86a-092b-4d11-acd6-b4076c8fe15d",
  "question": "Unter der Voraussetzung, dass der Artikel von A. Sudmann auch die medienpolitische Dimension der Barrierefreiheit in der künstlichen Intelligenz behandelt, in welchem Jahr wurde er veröffentlicht?",
  "reference_answer": "2018",
  "reference_context": "Document 5181: Unnamed: 0: 5181\ntext: [81] A. Sudmann, “On the media-political dimension of artificial intelli- gence: Deep learning as a black box and openai,” Digital Culture & Society, vol. 4, no. 1, pp. 181–200, 2018.\n\n[82] A. Koubaa, “Gpt-4 vs. gpt-3.5: A concise showdown,” 2023. [83] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray, et al., “Training language models to follow instructions with human feedback,” Advances in Neural Information Processing Systems, vol. 35, pp. 27730–27744, 2022.\n\n[84] S. Huang, L. Dong, W. Wang, Y. Hao, S. Singhal, S. Ma, T. Lv, L. Cui, O. K. Mohammed, Q. Liu, et al., “Language is not all you need: Aligning perception with language models,” arXiv preprint arXiv:2302.14045, 2023.\n\n[85] W. X. Zhao, K. Zhou, J. Li, T. Tang, X. Wang, Y. Hou, Y. Min, B. Zhang, J. Zhang, Z. Dong, et al., “A survey of large language models,” arXiv preprint arXiv:2303.18223, 2023.\nref_doc_id: general_surveys\/682263.pdf",
  "conversation_history": [],
  "metadata": {
    "question_type": "distracting element",
    "seed_document_id": 5181,
    "distracting_context": "Unnamed: 0: 3321\ntext: Prompt Augmentation. Existing prompt augmentation methods are limited by the input\n\nlength, i.e., feeding too many demonstrations to input is infeasible. Therefore, how to select\n\ninformative demonstrations, and order them in an appropriate is an interesting but challeng- ing problem [62].\n\nPrompt Sharing. All the above considerations refer to the application of prompt in a single task, domain or language. We may also consider prompt sharing, where prompt learning is\n\napplied to multiple tasks, domains, or languages. Some key issues that may arise include how to design individual prompts for different tasks, and how to modulate their interaction with each other. So far this field has not been explored. Figure 3 illustrates a simple multiple prompt learning strategy for multiple tasks, where prompt templates are partially shared.\n\nACM Computing Surveys, Vol. 55, No. 9, Article 195. Publication date: January 2023.\n\nA Systematic Survey of Prompting Methods in Natural Language Processing 195:25\n\nMovie Review (X1) | Really awesome movie! Template | [Domain_name]: This is [MASK]. Product Review (X2) | It’s very easy to use!\n\nFig. 3. Multi-prompt learning for multi-task, multi-domain, or multi-lingual learning. We use different colors\n\nto differentiate different components as follows. “~” for input text, ““>” for template, “—” for prompt.\n\nPrompt 1 | Movie: [X1] This is [MASK]. > Prompt 2 | Product: [X2] This is [MASK]. >\n\n9.6 Theoretical and Empirical Analysis of Prompting\n\nDespite their success in many scenarios, theoretical analysis and guarantees for prompt-based learning are scarce. Wei et al.\nref_doc_id: general_surveys\/3560815.pdf",
    "topic": "Large Language Models"
  }
}
{
  "id": "c0aea538-91cc-435c-9279-57565262e80f",
  "question": "Unter der Voraussetzung, dass das Papier auf eine umfassende Analyse abzielt, welches Werk untersucht die anhaltende anti-muslimische Voreingenommenheit in großen Sprachmodellen?",
  "reference_answer": "A. Abid, M. Farooqi, and J. Zou, “Persistent anti- muslim bias in large language models,” in AIES ’21: AAAI\/ACM Conference on AI, Ethics, and Society, Virtual Event, USA, May 19-21, 2021.",
  "reference_context": "Document 2585: Unnamed: 0: 2585\ntext: abs\/2212.01326, 2022.\n\n[911] D. Trautmann, A. Petrova, and F. Schilder, “Legal prompt engineering for multilingual legal judgement prediction,” CoRR, vol. abs\/2212.02199, 2022. [912] A. Tamkin, M. Brundage, J. Clark, and D. Ganguli, “Understanding the capabilities, limitations, and so- cietal impact of large language models,” CoRR, vol. abs\/2102.02503, 2021.\n\n[913] Z. Sun, “A short survey of viewing large language models in legal aspect,” CoRR, vol. abs\/2303.09136, 2023.\n\n[914] A. Abid, M. Farooqi, and J. Zou, “Persistent anti- muslim bias in large language models,” in AIES ’21: AAAI\/ACM Conference on AI, Ethics, and Society, Virtual Event, USA, May 19-21, 2021, M. Fourcade, B. Kuipers, S. Lazar, and D. K. Mulligan, Eds. ACM, 2021, pp. 298–306.\n\n[915] A. Shah and S. Chava, “Zero is not hero yet: Bench- marking zero-shot performance of llms for financial tasks,” CoRR, vol. abs\/2305.16633, 2023.\n\n[916] D. Araci, “Finbert: Financial sentiment analysis with pre-trained language models,” CoRR, vol. abs\/1908.10063, 2019.\nref_doc_id: general_surveys\/2303.18223v16.pdf",
  "conversation_history": [],
  "metadata": {
    "question_type": "distracting element",
    "seed_document_id": 2585,
    "distracting_context": "Unnamed: 0: 4436\ntext: Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond\n\nY\n\nY\n\nN\n\nY\n\nN\n\nY\n\nN\n\nY\n\nN\n\nN\n\nY\n\nY\n\nY\n\nY\n\nFine-tunedModels\n\nFine-tunedModels\n\nLLMs\n\nN\n\nLLMs\n\nLLMs\n\nN\n\nLLMs\n\nLLMs\n\nLLMs\n\nN\n\nCommon NLU\/ NLG tasks\n\nN\n\nMultiple Tasks\n\nN\n\nFine-tunedModelsNLPApplication\n\nJust a few labeled data (Zero\/Few-shot)\n\nTasks with little relation to language modelling(e.g. regression)\n\nMimicking Human(e.g. chatbot)\n\nDifficult Tasks Requiring scaling(e.g.Reasoning, Emergent abilities )\n\nKnowledge-intensive tasks\n\nContexts contain enough knowledge\n\nRequired knowledge inconsistent with the real-word.\n\nLLMs\n\nY\n\nCreative and Complex text\/code generation\n\nOut-of-distribution （O.O.D) data\n\nFig. 2. The decision flow for choosing LLMs or fine-tuned models 2for user’s NLP applications. The decision flow helps users assess whether their downstream NLP applications at hand meet specific conditions and, based on that evaluation, determine whether LLMs or fine-tuned models are the most suitable choice for their applications. During the decision process in the figure, means meeting the condition, and of the last condition means there’s no means not meeting the condition. The yellow circle for model working well on this kind of application.\n\nN\n\nY\n\nRemark 2 Fine-tuned models generally are a better choice than LLMs in traditional NLU tasks, but LLMs can provide help\n\nwhile requiring strong generalization ability.\n\n4.1.1 No use case.\nref_doc_id: general_surveys\/2304.13712v2.pdf",
    "topic": "Others"
  }
}
{
  "id": "2e009711-13b7-4cb9-a9de-362c6595239b",
  "question": "Unter der Voraussetzung, dass das Papier auch auf die Herausforderungen der Barrierefreiheit eingeht, welche drei populären LLM-Familien werden in dem Papier besprochen?",
  "reference_answer": "GPT, LLaMA, PaLM",
  "reference_context": "Document 4550: Unnamed: 0: 4550\ntext: to build LLM-powered AI systems for their tasks. This paper gives a timely survey of the recent advances on LLMs. We hope this survey will prove a valuable and accessible resource for students, researchers and developers.\n\nLLMs are large-scale, pre-trained, statistical language mod- els based on neural networks. The recent success of LLMs is an accumulation of decades of research and development of language models, which can be categorized into four waves that have different starting points and velocity: statistical lan- guage models, neural language models, pre-trained language models and LLMs.\n\nStatistical language models (SLMs) view text as a sequence of words, and estimate the probability of text as the product of their word probabilities. The dominating form of SLMs are Markov chain models known as the n-gram models, which compute the probability of a word conditioned on its immediate proceeding n − 1 words. Since word probabilities are estimated using word and n-gram counts collected from text corpora, the model needs to deal with data sparsity (i.e., assigning zero probabilities to unseen words or n-grams) by using smoothing, where some probability mass of the model is reserved for unseen n-grams [12]. N-gram models are widely used in many NLP systems. However, these models are incomplete in that they cannot fully capture the diversity and variability of natural language due to data sparsity.\n\nThe recent advances on transformer-based large language models (LMs), pretrained on Web-scale text corpora, signifi- cantly extended the capabilities of language models (LLMs).\nref_doc_id: general_surveys\/2402.06196v3.pdf\n\nDocument 4548: Unnamed: 0: 4548\ntext: LLMs’ ability of general-purpose language understanding and generation is acquired by training billions of model’s parameters on massive amounts of text data, as predicted by scaling laws [1], [2]. The research area of LLMs, while very recent, is evolving rapidly in many different ways. In this paper, we review some of the most prominent LLMs, including three popular LLM families (GPT, LLaMA, PaLM), and discuss their characteristics, contributions and limitations. We also give an overview of techniques developed to build, and augment LLMs. We then survey popular datasets prepared for LLM training, fine-tuning, and evaluation, review widely used LLM evaluation metrics, and compare the performance of several popular LLMs on a set of representative benchmarks. Finally, we conclude the paper by discussing open challenges and future research directions.\n\nI.\n\nINTRODUCTION\n\nLanguage modeling is a long-standing research topic, dat- ing back to the 1950s with Shannon’s application of informa- tion theory to human language, where he measured how well simple n-gram language models predict or compress natural language text [3]. Since then, statistical language modeling became fundamental to many natural language understanding and generation tasks, ranging from speech recognition, ma- chine translation, to information retrieval [4], [5], [6].\n\nto build LLM-powered AI systems for their tasks. This paper gives a timely survey of the recent advances on LLMs. We hope this survey will prove a valuable and accessible resource for students, researchers and developers.\n\nLLMs are large-scale, pre-trained, statistical language mod- els based on neural networks.\nref_doc_id: general_surveys\/2402.06196v3.pdf",
  "conversation_history": [],
  "metadata": {
    "question_type": "distracting element",
    "seed_document_id": 4550,
    "distracting_context": "Unnamed: 0: 2554\ntext: abs\/2308.06394, 2023. J. Lu, J. Rao, K. Chen, X. Guo, Y. Zhang, B. Sun, C. Yang, and J. Yang, “Evaluation and mitigation of agnosia in multimodal large language models,” CoRR, vol. abs\/2309.04041, 2023.\n\n[845]\n\n[846] A. Rohrbach, L. A. Hendricks, K. Burns, T. Darrell, and K. Saenko, “Object hallucination in image cap- tioning,” in EMNLP. Association for Computational Linguistics, 2018, pp. 4035–4045.\n\n[847] Y. Li, Y. Du, K. Zhou, J. Wang, W. X. Zhao, and J.-R. Wen, “Evaluating object hallucination in large vision-language models,” in The 2023 Conference on Empirical Methods in Natural Language Processing, 2023. [Online]. Available: https:\/\/openreview.net\/ forum?id=xozJw0kZXF\n\n[848] D. A. Hudson and C. D. Manning, “GQA: A new dataset for real-world visual reasoning and compo- sitional question answering,” in CVPR. Computer Vision Foundation \/ IEEE, 2019, pp. 6700–6709.\nref_doc_id: general_surveys\/2303.18223v16.pdf",
    "topic": "Others"
  }
}
{
  "id": "e23e806c-159d-47ed-9956-1ad48adc7c00",
  "question": "Welcher Artikel behandelt das Thema 'Rationale-augmented ensembles in language models', vorausgesetzt, dass die Modelle speziell für fortgeschrittene Fähigkeiten wie die Unterstützung externer Plugins optimiert wurden?",
  "reference_answer": "Der Artikel von X. Wang, J. Wei, D. Schuurmans, Q. V. Le, E. H. Chi und D. Zhou mit dem Titel 'Rationale-augmented ensembles in language models' wird im Jahr 2022 in CoRR veröffentlicht.",
  "reference_context": "Document 2337: Unnamed: 0: 2337\ntext: [431] X. Wang, J. Wei, D. Schuurmans, Q. V. Le, E. H. Chi, and D. Zhou, “Rationale-augmented ensembles in language models,” CoRR, 2022.\n\n[432] D. Zhou, N. Sch¨arli, L. Hou, J. Wei, N. Scales, X. Wang, D. Schuurmans, O. Bousquet, Q. Le, and E. H. Chi, “Least-to-most prompting enables com- plex reasoning in large language models,” CoRR, vol. abs\/2205.10625, 2022.\n\n[433] T. Khot, H. Trivedi, M. Finlayson, Y. Fu, Sabhar- and A. prompting: A modular tasks,” CoRR, [Online]. Available:\n\nK. Richardson, wal, approach for vol. https:\/\/doi.org\/10.48550\/arXiv.2210.02406\n\nP. Clark,\n\n“Decomposed\n\nsolving complex 2022.\n\nabs\/2210.02406,\n\n[434] L. Wang, W. Xu, Y. Lan, Z. Hu, Y. Lan, R. K. Lee, and E. Lim, “Plan-and-solve prompting: Improving zero-shot chain-of-thought reasoning by large language models,” CoRR, vol. abs\/2305.04091, 2023. [Online].\nref_doc_id: general_surveys\/2303.18223v16.pdf\n\nDocument 2336: Unnamed: 0: 2336\ntext: abs\/2205.09712, 2022.\n\n[429] X. Wang, J. Wei, D. Schuurmans, Q. V. Le, E. H. Chi, and D. Zhou, “Self-consistency improves chain of thought reasoning in language models,” CoRR, vol. abs\/2203.11171, 2022.\n\n[430] Y. Li, Z. Lin, S. Zhang, Q. Fu, B. Chen, J. Lou, and W. Chen, “On the advance of making language models better reasoners,” CoRR, vol. abs\/2206.02336, 2022.\n\n[431] X. Wang, J. Wei, D. Schuurmans, Q. V. Le, E. H. Chi, and D. Zhou, “Rationale-augmented ensembles in language models,” CoRR, 2022.\n\n[432] D. Zhou, N. Sch¨arli, L. Hou, J. Wei, N. Scales, X. Wang, D. Schuurmans, O. Bousquet, Q. Le, and E. H. Chi, “Least-to-most prompting enables com- plex reasoning in large language models,” CoRR, vol. abs\/2205.10625, 2022.\n\n[433] T. Khot, H. Trivedi, M. Finlayson, Y. Fu, Sabhar- and A. prompting: A modular tasks,” CoRR, [Online]. Available:\n\nK. Richardson, wal, approach for vol.\nref_doc_id: general_surveys\/2303.18223v16.pdf",
  "conversation_history": [],
  "metadata": {
    "question_type": "distracting element",
    "seed_document_id": 2337,
    "distracting_context": "Unnamed: 0: 1853\ntext: 49.96 (Davinci002) on GSM8k, and 79.88 (ChatGPT) v.s. 51.22 (Claude) on HumanEval.\n\nClaude 2, ChatGPT and Davinci003 perform better on inter- action with environment and tool manipulation tasks. On the two evaluation tasks, Claude 2, ChatGPT and Davinci003, per- form better than other models by a large margin, e.g., 36.40 (Claude 2) v.s. 26.00 (Davinci002) on HotpotQA, 44.53 (Chat- GPT) v.s. 7.74 (Claude) on Gorilla-TF, and 72.58 (Davinci003) v.s. 22.04 (Claude) on Gorilla-TH. A possible reason is that these three models have been specially optimized towards these advanced abilities, e.g., supporting the use of external plugins.\n\nAll the comparison models perform not well on very diffi- cult reasoning tasks. On MATH and HotpotQA, all models (including ChatGPT) perform not well. The two tasks are very difficult to solve, requiring accurate understanding of complex mathematical knowledge and performing multi- hop reasoning across documents, respectively. Further, these models also have a relatively weak performance on machine translation task (WMT). A possible reason is that WMT also contains many evaluation examples in minor languages, which might not be well covered in the pre-training data of these LLMs.\n\nAnalysis of Open-Source Models.\nref_doc_id: general_surveys\/2303.18223v16.pdf",
    "topic": "Large Language Models for Information Retrieval"
  }
}
{
  "id": "9dc368ca-7c42-44a8-92dd-430bf4199cfc",
  "question": "Unter der Bedingung, dass der Artikel 'Large language models are better reasoners with self-verification' sich mit komplexem Problemlösen durch fortgeschrittene Methoden wie least-to-most prompting beschäftigt, welche Autoren haben zu diesem Artikel beigetragen?",
  "reference_answer": "Y. Weng, M. Zhu, F. Xia, B. Li, S. He, K. Liu, und J. Zhao",
  "reference_context": "Document 2378: Unnamed: 0: 2378\ntext: [515] Y. Weng, M. Zhu, F. Xia, B. Li, S. He, K. Liu, and J. Zhao, “Large language models are better reasoners with self-verification,” CoRR, abs\/2212.09561, 2023.\n\n[516] W. Jiang, H. Shi, L. Yu, Z. Liu, Y. Zhang, Z. Li, and J. T. Kwok, “Forward-backward reasoning in large language models for mathematical verifica- tion,” 2023. J. Long, “Large language model guided tree-of- thought,” CoRR, vol. abs\/2305.08291, 2023.\n\n[517]\n\n[518] S. Mo and M. Xin, “Tree of uncertain thoughts reasoning for large language models,” CoRR, vol. abs\/2309.07694, 2023.\n\n[519] M. Besta, N. Blach, A. Kubicek, R. Gerstenberger, L. Gianinazzi, J. Gajda, T. Lehmann, M. Podstawski, H. Niewiadomski, P. Nyczyk, and T. Hoefler, “Graph of thoughts: Solving elaborate problems with large language models,” CoRR, vol. abs\/2308.09687, 2023. [520] B. Lei, P. Lin, C. Liao, and C. Ding, “Boosting log- ical reasoning in large language models through a new framework: The graph of thought,” CoRR, vol. abs\/2308.08614, 2023.\nref_doc_id: general_surveys\/2303.18223v16.pdf\n\nDocument 2377: Unnamed: 0: 2377\ntext: abs\/2304.13007, 2023.\n\n[513] Z. Ling, Y. Fang, X. Li, Z. Huang, M. Lee, R. Memi- sevic, and H. Su, “Deductive verification of chain-of- thought reasoning,” CoRR, vol. abs\/2306.03872, 2023. [514] T. Xue, Z. Wang, Z. Wang, C. Han, P. Yu, and H. Ji, inconsis- “RCOT: detecting and rectifying factual tency in reasoning by reversing chain-of-thought,” CoRR, vol. abs\/2305.11499, 2023.\n\n[515] Y. Weng, M. Zhu, F. Xia, B. Li, S. He, K. Liu, and J. Zhao, “Large language models are better reasoners with self-verification,” CoRR, abs\/2212.09561, 2023.\n\n[516] W. Jiang, H. Shi, L. Yu, Z. Liu, Y. Zhang, Z. Li, and J. T. Kwok, “Forward-backward reasoning in large language models for mathematical verifica- tion,” 2023. J. Long, “Large language model guided tree-of- thought,” CoRR, vol. abs\/2305.08291, 2023.\n\n[517]\n\n[518] S. Mo and M. Xin, “Tree of uncertain thoughts reasoning for large language models,” CoRR, vol. abs\/2309.07694, 2023.\nref_doc_id: general_surveys\/2303.18223v16.pdf",
  "conversation_history": [],
  "metadata": {
    "question_type": "distracting element",
    "seed_document_id": 2378,
    "distracting_context": "Unnamed: 0: 696\ntext: However, many real-world problems require complex reasoning. One key characteristic of these problems is that the reasoning steps may not be fixed. The reasoning path can vary for different problems, and each step of reasoning may depend on the outcomes of prior steps. In\n\n3.2 Advanced Prompting Methods\n\nsuch cases, it is undesirable to use fixed sub-problem generation in advance. Instead, sub-problems should be generated dynamically based on the input problem, and, if possible, generated on the fly during the reasoning process. This makes problem decomposition more challenging compared with designing divide-and-conquer algorithms. Ideally, we would like to jointly design both the systems for sub-problem generation and sub-problem solving. But a more practical and widely used approach is to adopt separate models for these tasks. A straightforward way to achieve this is to adapt an LLM for these tasks by either prompting or tuning the model.\n\nHere we consider a method based on the above idea, called least-to-most prompting [Zhou et al., 2023b]. The motivation for this method arises from the challenges of solving difficult rea- soning problems — those that cannot be addressed by simply generalizing from a few examples. For these problems, a more effective problem-solving strategy is to follow a progressive sequence of sub-problems that systematically lead to the conclusion. More specifically, in the least-to-most prompting method, sub-problem generation is performed by prompting an LLM with instructions and\/or demonstrations. For example, below is a 2-shot prompt for sub-problem generation in least-to-most prompting.\n\nTASK Your task is to decompose a problem into several sub-problems. You will\n\nbe given a few examples to illustrate how to achieve this.\nref_doc_id: general_surveys\/2501.09223v2.pdf",
    "topic": "Others"
  }
}
{
  "id": "25558d57-cea9-4b75-bf89-203b295b965e",
  "question": "Unter der Bedingung, dass Meta alle seine entwickelten LLMs als Open-Source bereitstellt, welche spezifischen Beiträge leistet das Unternehmen zur Förderung der Forschung und Entwicklung von Open-Source-LLMs?",
  "reference_answer": "Meta trägt erheblich zu Open-Source-LLMs bei und fördert die Forschung an LLMs. Meta zeichnet sich als eines der großzügigsten kommerziellen Unternehmen aus, da alle von Meta entwickelten LLMs Open-Source sind.",
  "reference_context": "Document 4421: Unnamed: 0: 4421\ntext: 1. The evolutionary tree of modern LLMs traces the development of language models in recent years and highlights some of the most well-known models. Models on the same branch have closer relationships. Transformer-based models are shown in non-grey colors: decoder-only models in the blue branch, encoder-only models in the pink branch, and encoder-decoder models in the green branch. The vertical position of the models on the timeline represents their release dates. Open-source models are represented by solid squares, while closed-source models are represented by hollow ones. The stacked bar plot in the bottom right corner shows the number of models from various companies and institutions.\n\nb) OpenAI consistently maintains its leadership position in LLM, both currently and potentially in the future. Other\n\ncompanies and institutions are struggling to catch up with OpenAI in developing models comparable to GPT-3\n\nand the current GPT-4. This leadership position may be attributed to OpenAI’s steadfast commitment to its\n\ntechnical path, even when it was not widely acknowledged initially.\n\nc) Meta contributes significantly to open-source LLMs and promotes research of LLMs. When considering contri-\n\nbutions to the open-source community, particularly those related to LLMs, Meta stands out as one of the most\n\ngenerous commercial companies, as all the LLMs developed by Meta are open-sourced.\n\nd) LLMs exhibit a tendency towards closed-sourcing. In the early stages of LLM development (before 2020), the\n\nmajority of models were open-sourced.\nref_doc_id: general_surveys\/2304.13712v2.pdf\n\nDocument 4422: Unnamed: 0: 4422\ntext: b) OpenAI consistently maintains its leadership position in LLM, both currently and potentially in the future. Other\n\ncompanies and institutions are struggling to catch up with OpenAI in developing models comparable to GPT-3\n\nand the current GPT-4. This leadership position may be attributed to OpenAI’s steadfast commitment to its\n\ntechnical path, even when it was not widely acknowledged initially.\n\nc) Meta contributes significantly to open-source LLMs and promotes research of LLMs. When considering contri-\n\nbutions to the open-source community, particularly those related to LLMs, Meta stands out as one of the most\n\ngenerous commercial companies, as all the LLMs developed by Meta are open-sourced.\n\nd) LLMs exhibit a tendency towards closed-sourcing. In the early stages of LLM development (before 2020), the\n\nmajority of models were open-sourced. However, with the introduction of GPT-3, companies have increasingly\n\n3\n\n4\n\nJingfeng Yang, Hongye Jin, Ruixiang Tang, Xiaotian Han, Qizhang Feng, Haoming Jiang, Bing Yin, and Xia Hu\n\nTable 1. Summary of Large Language Models.\nref_doc_id: general_surveys\/2304.13712v2.pdf\n\nDocument 4423: Unnamed: 0: 4423\ntext: c) Meta contributes significantly to open-source LLMs and promotes research of LLMs. When considering contri-\n\nbutions to the open-source community, particularly those related to LLMs, Meta stands out as one of the most\n\ngenerous commercial companies, as all the LLMs developed by Meta are open-sourced.\n\nd) LLMs exhibit a tendency towards closed-sourcing. In the early stages of LLM development (before 2020), the\n\nmajority of models were open-sourced. However, with the introduction of GPT-3, companies have increasingly\n\n3\n\n4\n\nJingfeng Yang, Hongye Jin, Ruixiang Tang, Xiaotian Han, Qizhang Feng, Haoming Jiang, Bing Yin, and Xia Hu\n\nTable 1. Summary of Large Language Models.\n\nCharacteristic\n\nLLMs\n\nEncoder-Decoder or Encoder-only (BERT-style)\n\nTraining: Masked Language Models\n\nModel type: Discriminative\n\nPretrain task:\n\nPredict masked words\n\nELMo [80], BERT [28], RoBERTa [65], DistilBERT [90], BioBERT [57], XLM [54], Xlnet [119], ALBERT [55], ELECTRA [24], T5 [84], GLM [123], XLM-E [20], ST-MoE [133], AlexaTM [95]\n\nDecoder-only (GPT-style)\n\nTraining Autoregressive Language Models\n\nModel type: Generative\n\nPretrain task:\n\nPredict next word\n\nGPT-3 [16], OPT [126].\nref_doc_id: general_surveys\/2304.13712v2.pdf",
  "conversation_history": [],
  "metadata": {
    "question_type": "distracting element",
    "seed_document_id": 4421,
    "distracting_context": "Unnamed: 0: 3732\ntext: 19\n\n[251] Z. Liu, B. Oguz, C. Zhao, E. Chang, P. Stock, Y. Mehdad, Y. Shi, R. Kr- ishnamoorthi, and V. Chandra, “Llm-qat: Data-free quantization aware training for large language models,” arXiv preprint arXiv:2305.17888, 2023. 19\n\n[252] Y. Guo, A. Yao, H. Zhao, and Y. Chen, “Network sketching: Exploiting binary structure in deep cnns,” in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2017, pp. 5955–5963. 19\n\n[253] J. Kim, J. H. Lee, S. Kim, J. Park, K. M. Yoo, S. J. Kwon, and D. Lee, “Memory-efficient fine-tuning of compressed large language models via sub-4-bit integer quantization,” arXiv preprint arXiv:2305.14152, 2023. 19\n\n[254] M. Sun, Z. Liu, A. Bair, and J. Z. Kolter, “A simple and effec- tive pruning approach for large language models,” arXiv preprint arXiv:2306.11695, 2023. 19\n\n[255] Z. Wang, J. Wohlwend, and T. Lei, “Structured pruning of large language models,” arXiv preprint arXiv:1910.04732, 2019. 19 [256] L. Yin, Y. Wu, Z. Zhang, C.-Y.\nref_doc_id: general_surveys\/2307.06435v8.pdf",
    "topic": "Large Language Models"
  }
}
{
  "id": "7f5896fc-fbc0-4856-babb-8a8e6a9a91d0",
  "question": "Unter der Bedingung, dass der Artikel von S. Wu, H. Fei, L. Qu, W. Ji und T.-S. Chua im Jahr 2023 veröffentlicht wurde, was ist das zentrale Thema dieser Veröffentlichung?",
  "reference_answer": "Next-gpt: Any-to-any multimodal llm",
  "reference_context": "Document 4841: Unnamed: 0: 4841\ntext: [Online]. Available: https:\/\/github.com\/togethercomputer\/stripedhyena\n\n[212] D. Y. Fu, S. Arora, J. Grogan, I. Johnson, S. Eyuboglu, A. W. Thomas, B. Spector, M. Poli, A. Rudra, and C. R´e, “Monarch mixer: A simple sub-quadratic gemm-based architecture,” 2023.\n\n[213] G. J. McLachlan, S. X. Lee, and S. I. Rathnayake, “Finite mixture models,” Annual review of statistics and its application, vol. 6, pp. 355–378, 2019.\n\n[214] H. Liu, C. Li, Q. Wu, and Y. J. Lee, “Visual instruction tuning,” arXiv\n\npreprint arXiv:2304.08485, 2023.\n\n[215] S. Liu, H. Cheng, H. Liu, H. Zhang, F. Li, T. Ren, X. Zou, J. Yang, H. Su, J. Zhu, L. Zhang, J. Gao, and C. Li, “Llava-plus: Learning to use tools for creating multimodal agents,” arXiv preprint arXiv:2311.05437, 2023.\n\n[216] S. Wu, H. Fei, L. Qu, W. Ji, and T.-S. Chua, “Next-gpt: Any-to-any\n\nmultimodal llm,” arXiv preprint arXiv:2309.05519, 2023.\nref_doc_id: general_surveys\/2402.06196v3.pdf",
  "conversation_history": [],
  "metadata": {
    "question_type": "distracting element",
    "seed_document_id": 4841,
    "distracting_context": "Unnamed: 0: 1663\ntext: Note that these tips are suggested in a general manner, it does not indicate that they are the best prompts for the corresponding tasks. This part will be continuously updated with more guidelines or tips. We welcome readers to contribute to this collection of prompt tips. We present the detailed procedure to contribute to the prompt tips, at the link: https:\/\/github. com\/RUCAIBox\/LLMSurvey\/tree\/main\/Prompts.\n\nEmpirical Analysis. We further conduct empirical studies to present the impact of prompts on task performance. To conduct the experiments, we select a variety of tasks that span language generation, knowledge utilization, complex reasoning, structure data generation, and information re- trieval. For each task, we manually write a prompt that follows general guidelines introduced above. Note that the tested prompts may not be the optimal for these tasks, since they mainly aim to help readers understand how to write an effective prompt for solving different tasks. Also, we add a simplified prompt as the comparison for most tasks. Following the experimental settings in Section 7.4, we examine the 3-shot performance of ChatGPT on complex reasoning tasks (Colored Objects and GSM8k), and zero- shot performance on other tasks. We report the experimental results in Table 17, where we also include the supervised performance in existing papers as reference.\n\nCarefully designed prompts can boost the zero-shot or few- shot performance of ChatGPT. By comparing the results of using different prompts on the same task, we can see that using the carefully designed prompts can achieve better per- formance than the simpler ones.\nref_doc_id: general_surveys\/2303.18223v16.pdf",
    "topic": "Others"
  }
}
{
  "id": "74a1427e-23ca-47fe-948e-07384453d686",
  "question": "Welche spezifischen Bereiche werden in der Umfrage zu Anwendungen von LLMs behandelt, unter der Bedingung, dass die Umfrage auch auf die Herausforderungen bei der Implementierung in verschiedenen Sektoren eingeht?",
  "reference_answer": "Die Umfrage behandelt Anwendungen von LLMs in verschiedenen Bereichen, darunter Medizin, Bildung, Finanzen, Ingenieurwesen, Medien, Unterhaltung, Politik und Recht.",
  "reference_context": "Document 4862: Unnamed: 0: 4862\ntext: We then discuss the wide range of tasks where they are used and also discuss applications of LLMs in diﬀerent domains, including medicine, education, ﬁnance, engineering, media, entertainment, politics, and law. We also discuss how LLMs are shaping the future of AI and their increasing role in scientiﬁc discovery, and how they can be used to solve real-world problems. Next, we explore the challenges associated with deploying LLMs in\n\nreal-world scenarios, including ethical considerations, model biases, interpretability, and computational resource requirements. This survey also highlights techniques for enhancing the robustness and controllability of LLMs and addressing bias, fairness, and quality issues in Generative AI. Finally, we conclude by highlighting the future of LLM research and the challenges that need to be addressed in order to make this technology more reliable and useful. This survey is intended to provide researchers, practitioners, and enthusiasts with a comprehensive understanding of LLMs, their evolution, applications, and challenges. By consolidating the state-of-the-art knowledge in the ﬁeld, this article is anticipated to serve as a valuable resource for learning\n\nthe current state-of-the-art as well as further advancements in the development and utilization of LLMs for a wide range of real-world applications. The GitHub repo for this project is available at https:\/\/github.com\/anas-zafar\/LLM-Survey\n\n1\n\nA Survey on Large Language Models: Applications, Challenges, Limitations, and Practical Usage\n\nMuhammad Usman Hadi1,*, Qasem Al-Tashi2,12*, Rizwan Qureshi2,*, Abbas Shah3,*,\nref_doc_id: general_surveys\/682263.pdf\n\nDocument 4863: Unnamed: 0: 4863\ntext: Finally, we conclude by highlighting the future of LLM research and the challenges that need to be addressed in order to make this technology more reliable and useful. This survey is intended to provide researchers, practitioners, and enthusiasts with a comprehensive understanding of LLMs, their evolution, applications, and challenges. By consolidating the state-of-the-art knowledge in the ﬁeld, this article is anticipated to serve as a valuable resource for learning\n\nthe current state-of-the-art as well as further advancements in the development and utilization of LLMs for a wide range of real-world applications. The GitHub repo for this project is available at https:\/\/github.com\/anas-zafar\/LLM-Survey\n\n1\n\nA Survey on Large Language Models: Applications, Challenges, Limitations, and Practical Usage\n\nMuhammad Usman Hadi1,*, Qasem Al-Tashi2,12*, Rizwan Qureshi2,*, Abbas Shah3,*, Amgad Muneer2, Muhammad Irfan4, Anas Zafar5, Muhammad Bilal Shaikh6, Naveed Akhtar7, Mohammed Ali Al-Garadi8, Syed Zohaib Hassan9, Maged Shoman9, Jia Wu2, and Seyedali Mirjalili10,11, Mubarak Shah12\n\n1School of Engineering, Ulster University, Belfast, BT15 1AP, United Kingdom (m.hadi@ulster.ac.uk) 2Department of Imaging Physics, The University of Texas MD Anderson Cancer Center, Houston, TX 77030, USA (qaal@mdanderson.org; fnu.rizwan@ucf.edu;\nref_doc_id: general_surveys\/682263.pdf",
  "conversation_history": [],
  "metadata": {
    "question_type": "distracting element",
    "seed_document_id": 4862,
    "distracting_context": "Unnamed: 0: 4979\ntext: s e l p m a x e\n\ne f a s\n\nn o\n\ng n i n u t\n\nk a e r b l i a j\n\no t\n\ne l b i t p e c s u s\n\ns s e l\n\nt i\n\ng n i k a m\n\n, y t e f a s\n\ns t i\n\ns e c n a h n e\n\n. s k c a t t a\n\na t e\n\nM\n\ny t i l a u q\n\nh g i h\n\ns s e l\n\nh t i\n\nw d e m r o f r e p\n\ne b\n\nn a c A M I L\n\nf o\n\ng n i n u t - e n i\n\nF\n\ne l b a l i a v a\n\nt o N\n\nA M I L\n\n4 2 0 2\n\na t a d\n\n13\n\nFig. 5: Graph representing the summary of LLMs released during 2019 and 2024 from different developers, with yearly total. Please note that the LLMs shown here are summarized until July 2024.\n\nC. Training of LLMs\n\nseveral key steps [248]. The process typically begins with the collection and preprocessing of a large amount of text data from diverse sources [249], such as books, articles, websites, and other textual corpora. The curated dataset [250] serves as the foun- dation for training the LLMs.\nref_doc_id: general_surveys\/682263.pdf",
    "topic": "Large Language Models"
  }
}
{
  "id": "331912c6-7d37-4089-9ec4-fc4923ec2d2c",
  "question": "Unter der Bedingung, dass 15% der Tokens in einem BERT-ähnlichen Maskierungsprozess zufällig ausgewählt werden, was ist das Ziel der effizienten Feinabstimmung bei großen Sprachmodellen (LLMs)?",
  "reference_answer": "Mit dem Aufkommen von LLMs hat die effiziente Feinabstimmung zunehmende Forschungsaufmerksamkeit erlangt, um einen leichteren Anpassungsansatz für nachgelagerte Aufgaben zu entwickeln.",
  "reference_context": "Document 1644: Unnamed: 0: 1644\ntext: Further, one can only keep a single large model copy, while maintaining a number of task-specific low-rank decomposition matrices for adapting to different downstream tasks. Further, several studies have also discussed how to set the rank in a more principled approach, e.g., importance score based allocation [414] and search-free optimal rank selection [415].\n\n32. Here, prompt tuning denotes a category of related efficient tuning methods exemplified by the work [405, 410, 411], instead of a spe- cific method as used in [405]. Indeed, the prefix based tuning meth- ods [404, 409] can be also considered as prompting methods, which are called deep prompting tuning in [409]. In this survey, prompt tuning specially refer to the methods that only include the prompt tokens at the input layer, in the context of LLMs. We assign p-tuning v2 [409] to the category of prefix tuning, because it incorporates layerwise prompts in langauge models.\n\n43\n\nBesides the above methods, there is extensive research on efficient tuning of Transformer language models. How- ever, a more comprehensive discussion of efficient tuning is beyond the scope of this article, which can be found in the related papers on this topic [408, 416].\n\n5.3.2 Parameter-Efficient Fine-Tuning on LLMs\n\nWith the rising of LLMs, efficient tuning has attracted increasing research attention for developing a more lightweight adaptation approach in downstream tasks.\n\nIn particular, LoRA [149] has been widely applied to open-source LLMs (e.g., LLaMA and BLOOM) for parameter-efficient fine-tuning.\nref_doc_id: general_surveys\/2303.18223v16.pdf\n\nDocument 1645: Unnamed: 0: 1645\ntext: In this survey, prompt tuning specially refer to the methods that only include the prompt tokens at the input layer, in the context of LLMs. We assign p-tuning v2 [409] to the category of prefix tuning, because it incorporates layerwise prompts in langauge models.\n\n43\n\nBesides the above methods, there is extensive research on efficient tuning of Transformer language models. How- ever, a more comprehensive discussion of efficient tuning is beyond the scope of this article, which can be found in the related papers on this topic [408, 416].\n\n5.3.2 Parameter-Efficient Fine-Tuning on LLMs\n\nWith the rising of LLMs, efficient tuning has attracted increasing research attention for developing a more lightweight adaptation approach in downstream tasks.\n\nIn particular, LoRA [149] has been widely applied to open-source LLMs (e.g., LLaMA and BLOOM) for parameter-efficient fine-tuning. Among these research at- tempts, LLaMA and its variants have gained much atten- tion for parameter-efficient tuning. For example, Alpaca- LoRA [148] has been trained using LoRA as a lightweight tuned version of Alpaca [146] (a fine-tuned 7B LLaMA model with 52K human demonstrations of instruction fol- lowing). There are extensive explorations of Alpaca-LoRA ranging in different languages or model sizes, which can be found in the collection page33. A recent study LLaMA- Adapter [417] inserts learnable prompt vectors into each Transformer layer, in which zero-initialized attention has been proposed to improve the training by mitigating the influence of under-fitted prompt vectors.\nref_doc_id: general_surveys\/2303.18223v16.pdf",
  "conversation_history": [],
  "metadata": {
    "question_type": "distracting element",
    "seed_document_id": 1644,
    "distracting_context": "Unnamed: 0: 373\ntext: [SEP] Apples grow on trees . [SEP]\n\nLabel:\n\nNotNext\n\n23\n\n(1.18)\n\n24\n\nPre-training\n\nInput:\n\n[CLS] It is raining . [SEP] I need an umbrella . [SEP]\n\nSelect tokens with a probability of 15%\n\nToken Selection:\n\n[CLS] It is raining . [SEP] I need an umbrella . [SEP]\n\nMask selected tokens with a probability of 80%\n\nToken Masking:\n\n[CLS] It is [MASK] . [SEP] I need [MASK] umbrella . [SEP]\n\nAlter selected tokens with a probability of 10%\n\nToken: Replacement\n\n[CLS] It is [MASK] . [SEP] I need [MASK] hat . [SEP]\n\nKeep selected tokens unchanged with a probability of 10%\n\nUnchanged:\n\n[CLS] It is [MASK] . [SEP] I need [MASK] hat . [SEP]\n\nTrain the Transformer encoder with the modified sequence\n\ntraining\n\nI\n\nan umbrella\n\nh0\n\nh1\n\nh2\n\nh3\n\nh4\n\nh5\n\nh6\n\nh7\n\nh8\n\nh9\n\nh10\n\nh11\n\nTransformer Encoder\n\ne0\n\ne1\n\ne2\n\ne3\n\ne4\n\ne5\n\ne6\n\ne7\n\ne8\n\ne9\n\ne10\n\ne11\n\n[CLS]\n\nIt\n\nis\n\n[MASK]\n\n.\n\n[SEP]\n\nI\n\nneed [MASK] hat\n\n.\n\n[SEP]\n\nFig. 1.5: A running example of BERT-style masked language modeling. First, 15% of the tokens are randomly selected.\nref_doc_id: general_surveys\/2501.09223v2.pdf",
    "topic": "Parameter-Efficient Fine-Tuning of Large Language Models"
  }
}
{
  "id": "a10415ac-08b3-4b52-868d-9ce618d63053",
  "question": "Welche Veröffentlichung behandelt die Nutzung von ChatGPT im juristischen Bildungsbereich unter der Bedingung, dass die Veröffentlichung auch auf die Feinabstimmung von Modellen eingeht?",
  "reference_answer": "J. H. Choi, K. E. Hickman, A. Monahan, und D. B. Schwarcz, „ChatGPT Goes to Law School,“ Journal of Legal Education, Januar 2023. Forthcoming.",
  "reference_context": "Document 5345: Unnamed: 0: 5345\ntext: Available at SSRN.\n\n[468] M. Ajevski, K. Barker, A. Gilbert, L. Hardie, and F. Ryan, “Chatgpt and the future of legal education and practice,” The Law Teacher, vol. 0, no. 0, pp. 1–13, 2023.\n\n[469] “The legal ai you’ve been waiting for.” https:\/\/casetext.com\/cocounsel\/.\n\n31 July 2023].\n\n[470] J. Cui, Z. Li, Y. Yan, B. Chen, and L. Yuan, “Chatlaw: Open-source legal large language model with integrated external knowledge bases,” arXiv preprint arXiv:2306.16092, 2023.\n\n[471] J. J. Nay, “Law Informs Code: A Legal Informatics Approach to\n\nAligning Artificial Intelligence with Humans,” arXiv preprint, 2022.\n\n[472] D. Trautmann, A. Petrova, and F. Schilder, “Legal Prompt Engineering\n\nfor Multilingual Legal Judgement Prediction,” arXiv preprint, 2022.\n\n[473] J. H. Choi, K. E. Hickman, A. Monahan, and D. B. Schwarcz, “ChatGPT Goes to Law School,” Journal of Legal Education, January 2023. Forthcoming.\n\n[474] F. Yu, L. Quartey, and F. Schilder, “Legal prompting: Teaching a\n\nlanguage model to think like a lawyer,” 2022.\nref_doc_id: general_surveys\/682263.pdf\n\nDocument 5346: Unnamed: 0: 5346\ntext: [471] J. J. Nay, “Law Informs Code: A Legal Informatics Approach to\n\nAligning Artificial Intelligence with Humans,” arXiv preprint, 2022.\n\n[472] D. Trautmann, A. Petrova, and F. Schilder, “Legal Prompt Engineering\n\nfor Multilingual Legal Judgement Prediction,” arXiv preprint, 2022.\n\n[473] J. H. Choi, K. E. Hickman, A. Monahan, and D. B. Schwarcz, “ChatGPT Goes to Law School,” Journal of Legal Education, January 2023. Forthcoming.\n\n[474] F. Yu, L. Quartey, and F. Schilder, “Legal prompting: Teaching a\n\nlanguage model to think like a lawyer,” 2022.\n\n[475] J. H. Choi, K. E. Hickman, A. Monahan, and D. B. Schwarcz, “Supra\n\nNote 7,” 2023. Reference to a previously cited work.\n\n[476] The Guardian, “Two US Lawyers Fined for Submitting Fake Court\n\nCitations by ChatGPT,” June 2023.\n\n[477] ABC News, “US Lawyer Uses ChatGPT to Research Case with\n\nEmbarrassing Result,” June 2023.\n\n[478] Business Standard, “US Judge Orders Lawyers Not to Use ChatGPT-\n\ndrafted Content in Court,” May 2023.\n\n[479] P. Rivas and L. Zhao, “Marketing with ChatGPT: Navigating the Ethical Terrain of GPT-Based Chatbot Technology,” AI, vol.\nref_doc_id: general_surveys\/682263.pdf",
  "conversation_history": [],
  "metadata": {
    "question_type": "distracting element",
    "seed_document_id": 5345,
    "distracting_context": "Unnamed: 0: 3460\ntext: Xuan Yuan 2.0 combined the pre-training and fine-tuning stages to avoid catastrophic forgetting.\n\n10\n\nPREPRINT\n\nModels\n\nT5\n\nGPT-3\n\nmT5\n\nPanGu-α\n\nCodex\n\nERNIE 3.0\n\nJurassic-1\n\nYuan 1.0\n\nGopher\n\nERNIE 3.0 Titan\n\nOPT\n\nBLOOM\n\nGalactica\n\nGLaM\n\nLaMDA\n\nAlphaCode\n\nTABLE I: Noteworthy findings and insights from pre-trained Large Language Model.\n\nFindings & Insights\n\nEncoder and decoder with shared parameters perform equivalently when parameters are not shared • Fine-tuning model layers (adapter layers) work better than the conventional way of training on only classification layers\n\nFew-shot performance of LLMs is better than the zero-shot, suggesting that LLMs are meta-learners\n\nLarge multi-lingual models perform equivalently to single language models on downstream tasks. However, smaller multi- lingual models perform worse\n\nLLMs are good at a few shot capabilities\n\nPrompt fine-tuning requires updating very few parameters while achieving performance comparable to full model fine-tuning • Prompt fine-tuning takes more time to converge as compared to full model fine-tuning • Inserting prompt tokens in-between sentences can allow the model to understand relations between sentences and long sequences\n\nPrompt fine-tuning requires updating very few parameters while achieving performance comparable to full model fine-tuning • Prompt fine-tuning takes more time to converge as compared to full model fine-tuning • Inserting prompt tokens in-between sentences can allow the model to understand relations between sentences and long sequences\n\nIn an analysis, CPM-2 finds that prompts work as a provider (additional context) and aggregator (aggregate information with the input text) for the model\n\nThis LLM focuses on code evaluations and introduces a novel way of selecting the best code samples.\nref_doc_id: general_surveys\/2307.06435v8.pdf",
    "topic": "ChatGPT and Large Language Models Applications"
  }
}
{
  "id": "22333a27-b93b-47a8-a2af-f3b2a5f2af81",
  "question": "Unter welchen Bedingungen können große Sprachmodelle (LLMs) bei bestimmten Aufgaben nicht überlegen sein, insbesondere wenn die Aufgaben spezielles Wissen erfordern, das nicht durch die reale Welt abgedeckt wird?",
  "reference_answer": "In einigen Aufgaben ist das erforderliche Wissen nicht das von LLMs über die reale Welt gelernte Wissen. In solchen Fällen können LLMs nicht gut arbeiten, da das Wissen innerhalb der LLMs über die reale Welt für die Aufgabe nutzlos ist oder das erforderliche Wissen sogar im Widerspruch zur realen Welt steht.",
  "reference_context": "Document 4453: Unnamed: 0: 4453\ntext: 4.3.2 No use case. There are some other tasks requiring knowledge different from that learned by LLMs. The required knowledge is not that learned by LLMs about the real world. In such tasks, LLMs are not notably superior.\n\nSome tasks only require the model to capture the self-contained knowledge in the contexts. The knowledge in the\n\ncontexts from the input is enough for the model to make predictions. For these tasks, small fine-tuned models can work\n\npretty well. One such task is machine reading comprehension (MRC). An MRC task provides several paragraphs and\n\nrequires the model to predict the answer to questions based on these paragraphs. We’ve discussed MRC in the previous\n\nsection because it’s also a traditional NLU task.\n\nAnother scenario is that the knowledge within LLMs about real world is useless to the task, or even the required\n\nknowledge is counterfactual to the real world. As a result, the LLMs cannot work well on such tasks. In some cases,\n\ninconsistent knowledge may even make the LLMs worse than random guessing. For example, in Big-Bench, the Mnist\n\nascii task requires the model to tell the digit represented by an ASCII art. The capability required by this task is nothing about real-world knowledge. Also, in the Inverse Scaling Phenomenon competition [70], the task redefine math redefines a common symbol and requires the model to choose between the original meaning and the meaning derived from the\n\nredefinition. What it requires contrasts to the LLMs’ knowledge, thus LLMs even perform worse than random guessing.\n\nAs an alternative to real-world knowledge in LLMs, access to extra knowledge is allowed, and models can thus get\n\nenough knowledge for a task via retrieval augmentation.\nref_doc_id: general_surveys\/2304.13712v2.pdf",
  "conversation_history": [],
  "metadata": {
    "question_type": "distracting element",
    "seed_document_id": 4453,
    "distracting_context": "Unnamed: 0: 1048\ntext: If the speculated tokens are correct, they are accepted, and the process continues with the next set of tokens. If they are incorrect, the incorrect speculations are discarded, and the verification model is used to generate the correct tokens.\n\nTo be more specific, let us see the speculative decoding method presented in Leviathan et al. [2023]’s work. In this method, the draft model is a small language model, denoted by Prq(yi|x,y<i), while the verification model is a normal LLM, denoted by Prp(yi|x,y<i). The goal is that, given a prefix, we use the draft model to autoregressively predict up to τ tokens. The verification model is then employed to generate the last token at the point where errors begin to occur in the speculative predictions. Figure 5.7 illustrates one step in this decoding process.\n\nThe speculative decoding algorithm can be summarized as follows.\n\nGiven the prefix [x,y≤i], we use the draft model to predict the next τ consecutive tokens, denoted by {ˆyi+1,..., ˆyi+τ}. This is a token-by-token generation process, given by\n\nˆyi+t = argmax\n\nPrq(yi+t|x,y≤i, ˆyi+1...ˆyi+t−1)\n\n(5.34)\n\nyi+t\n\nWe evaluate {ˆyi+1,..., ˆyi+τ} using the verification model, that is, we compute {Prp(ˆyi+1|x,y≤i) ,...,Prp(ˆyi+τ|x,y≤i, ˆyi+1...ˆyi+τ−1)}.\nref_doc_id: general_surveys\/2501.09223v2.pdf",
    "topic": "Large Language Models and Fine-Tuning"
  }
}
{
  "id": "3b7ef55a-f35d-436d-a5c8-558fd3764053",
  "question": "Unter der Voraussetzung, dass das Papier von B. Guo und anderen im Jahr 2023 in einem renommierten Journal veröffentlicht wurde, wie lautet der vollständige Titel dieses Papiers?",
  "reference_answer": "How close is chatgpt to human experts? comparison corpus, evaluation, and detection.",
  "reference_context": "Document 2211: Unnamed: 0: 2211\ntext: abs\/2204.05862, 2022. [Online]. Available: https:\/\/doi.org\/10.48550\/ arXiv.2204.05862\n\n[184] B. Guo, X. Zhang, Z. Wang, M. Jiang, J. Nie, Y. Ding, J. Yue, and Y. Wu, “How close is chatgpt to human experts? comparison corpus, evaluation, and detec- tion,” arXiv preprint arXiv:2301.07597, 2023.\n\n[185] M. Conover, M. Hayes, A. Mathur, J. Xie, J. Wan, S. Shah, A. Ghodsi, P. Wendell, M. Zaharia, and R. Xin. (2023) Free dolly: Introducing the world’s first truly open instruction-tuned llm.\n\n[186] A. K¨opf, Y. Kilcher, D. von R¨utte, S. Anagnostidis, Z.- R. Tam, K. Stevens, A. Barhoum, N. M. Duc, O. Stan- ley, R. Nagyfi et al., “Openassistant conversations– democratizing large language model alignment,” arXiv preprint arXiv:2304.07327, 2023.\nref_doc_id: general_surveys\/2303.18223v16.pdf",
  "conversation_history": [],
  "metadata": {
    "question_type": "distracting element",
    "seed_document_id": 2211,
    "distracting_context": "Unnamed: 0: 2229\ntext: abs\/2206.14858, 2022.\n\n[219] T. Saier, J. Krause, and M. F¨arber, “unarxive 2022: All arxiv publications pre-processed for nlp, includ- ing structured full-text and citation network,” arXiv preprint arXiv:2303.14957, 2023.\n\n[220] H. A. Simon, “Experiments with a heuristic com- piler,” J. ACM, vol. 10, no. 4, pp. 493–506, 1963. [221] Z. Manna and R. J. Waldinger, “Toward automatic program synthesis,” Commun. ACM, vol. 14, no. 3, pp. 151–165, 1971.\n\n[222] Z. Feng, D. Guo, D. Tang, N. Duan, X. Feng, M. Gong, L. Shou, B. Qin, T. Liu, D. Jiang, and M. Zhou, “Codebert: A pre-trained model for programming and natural languages,” in Findings of EMNLP, 2020. J. Austin, A. Odena, M. I. Nye, M. Bosma, J. Cai, H. Michalewski, D. Dohan, E. M. Terry, Q. V. Le, and C. Sutton, “Program syn- thesis with large language models,” CoRR, vol. abs\/2108.07732, 2021.\n\n[223]\n\nJiang, C.\nref_doc_id: general_surveys\/2303.18223v16.pdf",
    "topic": "Large Language Models Applications and Evaluation"
  }
}
{
  "id": "1d0129a4-c81c-486a-b4ea-ff2c2cf29226",
  "question": "Wie beeinflusst die Instruktionsausrichtung bei großen Sprachmodellen (LLMs) deren Fähigkeit, auf Benutzeranweisungen zu reagieren, insbesondere im Kontext der in der Literatur erwähnten Modelle wie BERT oder RoBERTa?",
  "reference_answer": "Das Ziel der Instruktionsausrichtung (oder Instruktions-Feinabstimmung) ist es, das LLM so zu tunen, dass es genau auf Benutzeranweisungen und -absichten reagiert.",
  "reference_context": "Document 821: Unnamed: 0: 821\ntext: 4.2 Instruction Alignment\n\n4.2 Instruction Alignment\n\nOne feature of LLMs is that they can follow the prompts provided by users to perform various tasks. In many applications, a prompt consists of a simple instruction and user input, and we want the LLM to follow this instruction to perform the task correctly. This ability of LLMs is also called the instruction-following ability. For example, below is a prompt where we want the LLM to extract key points and provide a concise summary for a lengthy article.\n\nInstruction Summarize this text in three sentences.\n\nInput Daylight Savings Time (DST) - the process of moving clocks forward by one hour in the summer - was started in Germany in 1916 ...\n\nOutput\n\nThis task requires the LLM to understand the instruction “Summarize this text in three sentences” and perform the summarization accordingly. However, LLMs are typically trained for next-token prediction rather than for generating outputs that follow instructions. Applying a pre-trained LLM to the above example would likely result in the model continuing to write the input article instead of summarizing the main points. The goal of instruction alignment (or instruction fine-tuning) is to tune the LLM to accurately respond to user instructions and intentions. The rest of this section will discuss some issues related to instruction alignment, including fine-tuning LLMs to follow instructions, generating or collecting instruction data, and generalizing instruction alignment.\n\n4.2.1 Supervised Fine-tuning\n\nOne straightforward approach to adapting LLMs to follow instructions is to fine-tune these mod- els using annotated input-output pairs [Ouyang et al., 2022; Wei et al., 2022a].\nref_doc_id: general_surveys\/2501.09223v2.pdf",
  "conversation_history": [],
  "metadata": {
    "question_type": "distracting element",
    "seed_document_id": 821,
    "distracting_context": "Unnamed: 0: 147\ntext: [41] X. Qiu, T. Sun, Y. Xu, Y. Shao, N. Dai, and X. Huang, “Pre-trained models for natural language processing: A survey,”\n\nScience China Technological Sciences, vol. 63, no. 10, pp. 1872–1897, 2020.\n\n[42] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “Bert: Pre-training of deep bidirectional transformers for language\n\nunderstanding,” arXiv preprint arXiv:1810.04805, 2018.\n\n[43] Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis, L. Zettlemoyer, and V. Stoyanov, “Roberta: A\n\nrobustly optimized bert pretraining approach,” arXiv preprint arXiv:1907.11692, 2019.\n\n[44] Z. Yang, Z. Dai, Y. Yang, J. Carbonell, R. R. Salakhutdinov, and Q. V. Le, “Xlnet: Generalized autoregressive pretraining\n\nfor language understanding,” Advances in neural information processing systems, vol. 32, 2019.\nref_doc_id: general_surveys\/2303.04226v1.pdf",
    "topic": "Instruction Fine-Tuning for LLMs"
  }
}
