{
  "id": "dd1da311-6363-4cac-a789-02b35e3ea838",
  "question": "Unter welchen Bedingungen und mit welchen spezifischen Anpassungen zeigt Abbildung 3.5 die Feinabstimmung von Präfixen für die Übersetzungsaufgabe?",
  "reference_answer": "Abbildung 3.5 zeigt eine Illustration des Prefix-Fine-Tunings für eine Übersetzungsaufgabe, bei der nur die Präfixvektoren pl 1 durch das Empfangen der Fehlgradienten vom Ausgang (d.h. der chinesischen Übersetzung) aktualisiert werden.",
  "reference_context": "Document 785: Unnamed: 0: 785\ntext: Hl+1 = Layer(Hl)[−m − 1 :] 0 hl+1 1\n\n= hl+1\n\n... hl+1 m\n\n(3.21)\n\nwhere [−m−1 :] denotes the slicing operation that extracts the last m+1 elements of a sequence. Given Hl+1\n\n, the input of the next layer can be expressed in the same form of Eq. (3.20):\n\nHl+1 = pl+1 = pl+1\n\n0 pl+1 1 0 pl+1 1\n\nn Hl+1 n hl+1\n\n... pl+1 ... pl+1\n\n0 hl+1 1\n\n... hl+1 m\n\n(3.22)\n\nHere each pi ∈ Rd can be seen as a learnable parameter. During training, pl as usual, and the parameters of the original Transformer model are kept fixed.\n\n0pl\n\n1...pl\n\nn are trained\n\nFigure 3.5 shows an illustration of prefix fine-tuning for a translation task. Here, only the prefix vectors pl 1 are updated by receiving the error gradients from the output (i.e., the Chinese translation). By adjusting these vectors for the translation task, the model adapts accordingly. This makes pl 1 serve as prompts which activate the LLM to perform the task without needing explicit input prompts like “Translate the following sentence from English to Chinese”. At test time, we prepend the optimized pl 1 to the layer, and the LLM will then translate the input sentence. Note that prefix fine-tuning introduces additional L × n × d parameters, where L is the number of layers, n is the number of prefixes, and d is the dimensionality of each prefix.\nref_doc_id: general_surveys\/2501.09223v2.pdf",
  "conversation_history": [],
  "metadata": {
    "question_type": "complex",
    "seed_document_id": 785,
    "topic": "Others"
  }
}
{
  "id": "9950006e-f2b7-4e2e-9c30-fe3aea332b50",
  "question": "Inwiefern beeinflusst die Vielfalt der Pretraining-Daten die Leistung von LLMs, insbesondere in Bezug auf die Fähigkeit, kontextbezogene und kohärente Antworten zu generieren, und welche Beispiele gibt es für Modelle, die von dieser Vielfalt profitieren?",
  "reference_answer": "Die Vielfalt der Pretraining-Daten spielt eine entscheidende Rolle bei der Gestaltung der Leistung des Modells, da sie das Modell mit einem reichen Verständnis von Wortwissen, Grammatik, Syntax und Semantik informiert und die Fähigkeit zur Kontexterkennung und zur Generierung kohärenter Antworten verbessert.",
  "reference_context": "Document 4430: Unnamed: 0: 4430\ntext: The importance of pretraining data lies in its capacity to\n\ninform the language model with a rich understanding of word knowledge, grammar, syntax, and semantics, as well\n\nas the ability to recognize context and generate coherent responses. The diversity of pretraining data also plays a\n\ncrucial role in shaping the model’s performance, and the selection of LLMs highly depends on the components of the\n\npretraining data. For example, PaLM [22] and BLOOM [92] excel in multilingual tasks and machine translation with an\n\nabundance of multilingual pretraining data. Moreover, PaLM’s performance in Question Answering tasks is enhanced\n\nby incorporating a considerable amount of social media conversations and Books corpus [22]. Likewise, code execution\n\nand code completion capabilities of GPT-3.5 (code-davinci-002) are amplified by the integration of code data in its\n\npretraining dataset. In brief, when selecting LLMs for downstream tasks, it is advisable to choose the model pre-trained\n\non a similar field of data.\n\n3.2 Finetuning data\n\nWhendeployingamodelfordownstreamtasks,itisessentialtoconsiderthreeprimaryscenariosbasedontheavailability\n\nof annotated data: zero, few, and abundant. In this section, we provide a succinct overview of the appropriate models to\n\nemploy for each scenario. Zero annotated data: In scenarios where annotated data is unavailable, utilizing LLMs in a zero-shot setting proves to be the most suitable approach. LLMs have been shown to outperform previous zero-shot methods [120]. Additionally,\n\nthe absence of a parameter update process ensures that catastrophic forgetting [49] is avoided since the language model\n\nparameters remain unaltered.\nref_doc_id: general_surveys\/2304.13712v2.pdf\n\nDocument 4429: Unnamed: 0: 4429\ntext: (2) LLMs are preferable to fine-tuned models when working with limited annotated data, and both can be\n\nreasonable choices when abundant annotated data is available, depending on specific task requirements.\n\n(3) It’s advisable to choose models pre-trained on fields of data that are similar to downstream tasks.\n\n3.1 Pretraining data\n\nPre-training data plays a pivotal role in the development of large language models. As the foundation of remarkable\n\ncapabilities [5, 47] of LLMs, the quality, quantitative, and diversity of pre-training data influence the performance\n\nof LLMs significantly [124]. The commonly used pretraining data consists of a myriad of text sources, including\n\nbooks, articles, and websites. The data is carefully curated to ensure a comprehensive representation of human\n\nknowledge, linguistic nuances, and cultural perspectives. The importance of pretraining data lies in its capacity to\n\ninform the language model with a rich understanding of word knowledge, grammar, syntax, and semantics, as well\n\nas the ability to recognize context and generate coherent responses. The diversity of pretraining data also plays a\n\ncrucial role in shaping the model’s performance, and the selection of LLMs highly depends on the components of the\n\npretraining data. For example, PaLM [22] and BLOOM [92] excel in multilingual tasks and machine translation with an\n\nabundance of multilingual pretraining data. Moreover, PaLM’s performance in Question Answering tasks is enhanced\n\nby incorporating a considerable amount of social media conversations and Books corpus [22]. Likewise, code execution\n\nand code completion capabilities of GPT-3.5 (code-davinci-002) are amplified by the integration of code data in its\n\npretraining dataset.\nref_doc_id: general_surveys\/2304.13712v2.pdf",
  "conversation_history": [],
  "metadata": {
    "question_type": "complex",
    "seed_document_id": 4430,
    "topic": "Others"
  }
}
{
  "id": "2a032233-dc70-4332-87ad-f01966fa11ee",
  "question": "Was ist die Bedeutung und mögliche Konsequenz, wenn der Wert von πθ(at|st) im Vergleich zu πθref(at|st) größer als 1 ist, unter der Bedingung, dass die aktuelle Politik die vorherige übertrifft?",
  "reference_answer": "Wenn πθ(at|st) (at|st) > 1 ist, wird die Aktion at von der aktuellen Politik im Vergleich zur Referenzpolitik mehr bevorzugt.",
  "reference_context": "Document 916: Unnamed: 0: 916\ntext: This can be written as\n\nU(τ;θ) =\n\nT X\n\nt=1\n\nπθ(at|st) πθref(at|st)\n\nA(st,at)\n\n(4.45)\n\nHere we replace the log-probability logπθ(at|st) with the ratio πθ(at|st) (at|st). θref denotes the pa- rameters of the previous policy (such as an initial model from which we start the training). So πθ(at|st) (at|st), also called the ratio function, can be interpreted as the log-probability ratio between πθref the current policy πθ and the previous policy πθref (call it the reference policy). By using the ratio function we reweight the observed rewards based on the likelihood of the actions under the current policy versus the reference policy. When πθ(at|st) (at|st) > 1, the action at is more favored by the current policy compared to the reference policy. By contrast, when πθ(at|st) (at|st) < 1, the action at is less favored by the current policy4.\n\nπθref\n\nπθref\n\nπθref\n\n4Consider a more general case where we wish to evaluate the policy using its expected reward (also see Eq. (4.18))\n\nJ(θ) = Eτ∼πθ\n\nh\n\nR(τ)\n\ni\n\n(4.46)\n\nHere τ ∼ πθ means that the sequence τ is generated by the policy πθ.\nref_doc_id: general_surveys\/2501.09223v2.pdf",
  "conversation_history": [],
  "metadata": {
    "question_type": "complex",
    "seed_document_id": 916,
    "topic": "Policy Gradient Reinforcement Learning"
  }
}
{
  "id": "3c8227f4-2fa8-469f-9fd6-eb373e72dade",
  "question": "Welche spezifische Herausforderung wird im Artikel von P. Clark et al. bezüglich des AI2 Reasoning Challenge beschrieben und welche Bedingungen müssen erfüllt sein, um diese zu bewältigen?",
  "reference_answer": "Das AI2 Reasoning Challenge stellt die Herausforderung dar, ob man das Frage-Antworten wirklich gelöst hat.",
  "reference_context": "Document 4833: Unnamed: 0: 4833\ntext: [193] P. Clark, I. Cowhey, O. Etzioni, T. Khot, A. Sabharwal, C. Schoenick, and O. Tafjord, “Think you have solved question answering? try arc, the AI2 reasoning challenge,” CoRR, vol. abs\/1803.05457, 2018. [Online]. Available: http:\/\/arxiv.org\/abs\/1803.05457\n\n[194] Y. Bisk, R. Zellers, R. L. Bras, J. Gao, and Y. Choi, “PIQA: reasoning about physical commonsense in natural language,” CoRR, vol. abs\/1911.11641, 2019. [Online]. Available: http:\/\/arxiv.org\/abs\/ 1911.11641\n\n[195] M. Sap, H. Rashkin, D. Chen, R. L. Bras, and Y. Choi, “Socialiqa: interactions,” CoRR, vol. Commonsense reasoning about abs\/1904.09728, 2019. [Online]. Available: http:\/\/arxiv.org\/abs\/1904. 09728\n\nsocial\n\n[196] T. Mihaylov, P. Clark, T. Khot, and A. Sabharwal, “Can a suit of armor conduct electricity? A new dataset for open book question answering,” CoRR, vol. abs\/1809.02789, 2018. [Online].\nref_doc_id: general_surveys\/2402.06196v3.pdf",
  "conversation_history": [],
  "metadata": {
    "question_type": "complex",
    "seed_document_id": 4833,
    "topic": "Question Answering Datasets"
  }
}
{
  "id": "8bbcaae7-9937-4d38-b7e6-6accc80ff232",
  "question": "Welche spezifischen Techniken wurden in den letzten Jahren entwickelt, um die Effektivität des Promptings bei großen Sprachmodellen zu verbessern, und welche dieser Techniken erfordern keine Anpassung der Modellarchitektur?",
  "reference_answer": "Techniken wie Few-Shot Learning, Zero-Shot Learning und CoT Reasoning wurden erforscht, um die Effektivität des Promptings zu verbessern.",
  "reference_context": "Document 811: Unnamed: 0: 811\ntext: Prompting approaches were first experimented with smaller models and later demonstrated impressive capabilities with large models like GPT-3, which could generate high-quality text in response to simple prompts across various tasks. As prompting technology evolved, prompt en- gineering emerged as a critical area of research. As discussed in this chapter, it broadly involves designing effective prompts to maximize model performance, encompassing both hand-crafted and automatically generated prompts. More recent research has explored how to enhance the ef- fectiveness of prompting through techniques like few-shot learning, zero-shot learning, and CoT reasoning, enabling LLMs to work effectively across a wide range of scenarios. A general dis- cussion of prompting can be very broad, and we cannot cover all details in this chapter. For more advanced techniques of prompting, the reader can refer to recent surveys. Topics include in-context learning [Li, 2023; Dong et al., 2022], CoT [Chu et al., 2023; Yu et al., 2023; Zhang et al., 2023a], efficient prompting [Chang et al., 2024], and general prompt engineering [Liu et al., 2023c; Chen et al., 2023a].\n\n153\n\n154\n\nPrompting\n\nNote that although we would ideally like to develop general prompting methods without ad- justing model architectures and parameters, the results of prompting generally depend heavily on the quality and size of the given LLMs. For stronger models, such as commercialized online LLMs, simple prompts may be sufficient to instruct these models to perform tasks correctly. In this case, prompt engineering is relatively easy, though we still need certain efforts to make LLMs work properly.\nref_doc_id: general_surveys\/2501.09223v2.pdf\n\nDocument 810: Unnamed: 0: 810\ntext: The widespread use of the modern concept of prompts began with the rise of large pre-trained models in the field of NLP. Initially, these models, such as BERT, were adapted to specific down- stream tasks mainly through fine-tuning. However, researchers soon discovered that by designing specific \"prompts\" — adding certain words or sentences to the input — the models could be triggered to respond to specific tasks without extensive fine-tuning. This motivated the NLP com- munity to develop and apply universal foundation models that can be prompted to address various tasks without changing the underlying architecture and the pre-training procedure.\n\nPrompting approaches were first experimented with smaller models and later demonstrated impressive capabilities with large models like GPT-3, which could generate high-quality text in response to simple prompts across various tasks. As prompting technology evolved, prompt en- gineering emerged as a critical area of research. As discussed in this chapter, it broadly involves designing effective prompts to maximize model performance, encompassing both hand-crafted and automatically generated prompts. More recent research has explored how to enhance the ef- fectiveness of prompting through techniques like few-shot learning, zero-shot learning, and CoT reasoning, enabling LLMs to work effectively across a wide range of scenarios. A general dis- cussion of prompting can be very broad, and we cannot cover all details in this chapter. For more advanced techniques of prompting, the reader can refer to recent surveys.\nref_doc_id: general_surveys\/2501.09223v2.pdf\n\nDocument 809: Unnamed: 0: 809\ntext: Solutions to these issues involve both general prompt designs and more advanced techniques, such as CoT and prompt learning, which have been explored extensively in recent research.\n\nIn NLP, prompting can be viewed as a technology that has evolved along with LLMs, and in a sense, it has opened the door to the practical application of these models in an impressive range of problem domains. In fact, if we expand the concept of prompts to some extent, it can be traced back to the early days of machine learning and NLP. For example, many NLP systems use hand-crafted features and templates to “prompt” specific tasks. Imagine developing a feature to indicate whether a text is formal or informal. We can feed this feature into a machine translation system to condition the translation on the type of the input text.\n\nThe widespread use of the modern concept of prompts began with the rise of large pre-trained models in the field of NLP. Initially, these models, such as BERT, were adapted to specific down- stream tasks mainly through fine-tuning. However, researchers soon discovered that by designing specific \"prompts\" — adding certain words or sentences to the input — the models could be triggered to respond to specific tasks without extensive fine-tuning. This motivated the NLP com- munity to develop and apply universal foundation models that can be prompted to address various tasks without changing the underlying architecture and the pre-training procedure.\n\nPrompting approaches were first experimented with smaller models and later demonstrated impressive capabilities with large models like GPT-3, which could generate high-quality text in response to simple prompts across various tasks. As prompting technology evolved, prompt en- gineering emerged as a critical area of research.\nref_doc_id: general_surveys\/2501.09223v2.pdf",
  "conversation_history": [],
  "metadata": {
    "question_type": "complex",
    "seed_document_id": 811,
    "topic": "Prompt Engineering for Language Models"
  }
}
{
  "id": "a73131d7-8ee6-49d5-9617-b282d2a9239c",
  "question": "Welche Rolle spielt das Long Convolution Modul in Hyena, insbesondere im Vergleich zu einem Attention-Modul, und welche Einschränkungen bringt es mit sich?",
  "reference_answer": "Im Long Convolution Modul werden Filter basierend auf relativen Positionen verwendet, um Informationen an verschiedenen Positionen in die mittleren Repräsentationen zu aggregieren, und Gating-Funktionen werden eingesetzt, um die Zwischenrepräsentationen weiter in die endgültige Ausgabe zu projizieren.",
  "reference_context": "Document 1477: Unnamed: 0: 1477\ntext: Hyena employs long convolution to replace the attention module. In the long convolution module, the filters based on relative positions are used to aggregate information at different positions into the middle represen- tations, and gating functions are employed to further project intermediate representations into the final output. However, due to the long convolution, Hyena can not infer like RNN and must explicitly access all previous states.\n\n23\n\nTABLE 7: Detailed formulations for the network configurations. Here, Sublayer denotes a FFN or a self-attention module in a Transformer layer, d denotes the size of hidden states, pi denotes position embedding at position i, Aij denotes the attention score between a query and a key, ri−j denotes a learnable scalar based on the offset between the query and the key, and RΘ,t denotes a rotary matrix with rotation degree t · Θ.\n\nConfiguration\n\nMethod\n\nEquation\n\nNormalization position\n\nPost Norm [22] Pre Norm [26] Sandwich Norm [274]\n\nNorm(x+Sublayer(x)) x + Sublayer(Norm(x)) x + Norm(Sublayer(Norm(x)))\n\nNormalization method\n\nLayerNorm [275]\n\nRMSNorm [276] DeepNorm [277]\n\n(cid:113) 1 d\n\nx−µ\n\n(cid:80)d\n\nσ · γ + β, µ = 1 RMS(x) · γ, RMS(x) = LayerNorm(α · x + Sublayer(x))\n\ni=1 xi,\nref_doc_id: general_surveys\/2303.18223v16.pdf",
  "conversation_history": [],
  "metadata": {
    "question_type": "complex",
    "seed_document_id": 1477,
    "topic": "Others"
  }
}
{
  "id": "f1b2234f-c0ac-4297-80d8-0661e8f3f04c",
  "question": "Welche Rolle spielt die Skalierung der Ausgabelänge in großen Sprachmodellen, insbesondere bei Aufgaben, die komplexes Denken erfordern, und wie wird dies in der Praxis umgesetzt?",
  "reference_answer": "Die Skalierung der Ausgabelänge bezieht sich auf die Erhöhung der Anzahl der während der Inferenz generierten Tokens. Dies ist besonders wichtig bei Aufgaben, die eine Langform-Generierung erfordern, wie z.B. das Schreiben von Geschichten.",
  "reference_context": "Document 1104: Unnamed: 0: 1104\ntext: Therefore, effective context scaling is not just about adding more information, but also about strategically selecting, structuring, and presenting the most pertinent information within the model’s processing capabilities.\n\nHere we omit the detailed discussion of these methods, as they have already been covered in previous chapters. See Chapters 2 and 3 for more details, including prompting, RAG, and long-sequence modeling methods.\n\n5.3.2 Search Scaling\n\nIn LLMs, decoding is a search process that aims to efficiently find the best output sequence given the input sequence. Search scaling (or decoding scaling) typically involves two aspects: scaling the output length and scaling the search space.\n\nScaling the output length refers to increasing the number of tokens generated during inference. This is especially important in tasks that require long-form generation, such as story writing. More recently, generating outputs with long thinking paths has shown strong performance in math prob- lem solving and code generation. For example, encouraging the model to generate long thinking paths before producing the final answers has been found to be very beneficial in performing com- plex reasoning. This idea has been widely used in developing recent LLMs for reasoning, such as OpenAI [2024]’s o1 and Deepseek [2025]’s R1. We will discuss more about output length scaling in Section 5.3.4.\n\nScaling the search space, on the other hand, refers to expanding the set of candidate output sequences considered during search, so that higher-quality outputs can be found. As discussed in Section 5.1.3, a simple example is that in beam search we increase the beam width to allow more candidate sequences to be explored in parallel at each decoding step.\nref_doc_id: general_surveys\/2501.09223v2.pdf\n\nDocument 1103: Unnamed: 0: 1103\ntext: These retrieved pieces of information are then added to the context provided to the LLM. This essentially expands the context to include timely or spe- cialized external knowledge. By doing so, the model grounds its responses in specific knowledge found in the external source. The LLM thus can generate responses that are not only relevant to the input but also factually accurate and up-to-date.\n\nHowever, as the context grows, these methods often suffer from the constraints of finite con- text window length. While model architectures and techniques (like efficient attention models) are continually evolving to support longer contexts, processing extremely long inputs still poses challenges. Increased computational cost is one factor. More critically, when the context window becomes very large, the model might struggle to attend effectively to the most relevant informa- tion (e.g., the “lost in the middle” phenomenon). Therefore, effective context scaling is not just about adding more information, but also about strategically selecting, structuring, and presenting the most pertinent information within the model’s processing capabilities.\n\nHere we omit the detailed discussion of these methods, as they have already been covered in previous chapters. See Chapters 2 and 3 for more details, including prompting, RAG, and long-sequence modeling methods.\n\n5.3.2 Search Scaling\n\nIn LLMs, decoding is a search process that aims to efficiently find the best output sequence given the input sequence. Search scaling (or decoding scaling) typically involves two aspects: scaling the output length and scaling the search space.\n\nScaling the output length refers to increasing the number of tokens generated during inference. This is especially important in tasks that require long-form generation, such as story writing. More recently, generating outputs with long thinking paths has shown strong performance in math prob- lem solving and code generation.\nref_doc_id: general_surveys\/2501.09223v2.pdf",
  "conversation_history": [],
  "metadata": {
    "question_type": "complex",
    "seed_document_id": 1104,
    "topic": "Others"
  }
}
{
  "id": "fde608f3-0755-4012-b521-818d28da5f9e",
  "question": "Unter welchen Bedingungen wird vorgeschlagen, die Instabilität bei der Generierung von CoT durch LLMs mit einer bestimmten Methode zu verbessern, und welche Methode wird bevorzugt?",
  "reference_answer": "Sampling-basierte Methoden schlagen vor, mehrere Begründungspfade zu sampeln, anstatt greedy decoding zu verwenden, und Self-Consistency generiert mehrere Begründungspfade und wählt die konsistenteste Antwort durch Mehrheitsabstimmung aus.",
  "reference_context": "Document 1716: Unnamed: 0: 1716\ntext: However, all these approaches rely on annotated CoT datasets, which limits their use in practice. To overcome this limitation, magic instructions such as “Let’s think step by step” can be used to automatically construct CoTs by prompting LLMs [427].\n\nEnhanced CoT Generation. Since LLMs are prone to producing incorrect reasoning steps and exhibiting insta- bility in the generation process, there are a number of studies [429, 511] to improve the generation of CoT. In this part, we will introduce two typical approaches to enhancing the generation of CoT: sampling- and verification-based methods.\n\nSampling-based methods. LLMs are known to suffer from instability during inference, which can lead to un- faithfulness in the generated reasoning steps. To address this issue, some work proposes to sample multiple rea- soning paths instead of using greedy decoding. As a rep- resentative solution, self-consistency [429] first generates several reasoning paths and then takes an ensemble over the corresponding answers, selecting the most consistent one through majority voting. However, such a method can still lead to wrong answers when most of the reasoning paths are misled. Considering this, the authors in [426] only vote on the k most complex reasoning paths based on their observation that reasoning paths with higher complexity (e.g., more reasoning steps) usually have better performance. Furthermore, MCR [512] proposes referring to the steps from other reasoning paths when generating the next step, and performs reasoning across multiple reasoning paths to generate the final answer.\n\nVerification-based methods.\nref_doc_id: general_surveys\/2303.18223v16.pdf",
  "conversation_history": [],
  "metadata": {
    "question_type": "complex",
    "seed_document_id": 1716,
    "topic": "Chain-of-Thought Prompting in LLMs"
  }
}
{
  "id": "320ab92c-c9b2-4602-856f-bffe02fa76ad",
  "question": "Welche spezifische Modellarchitektur, die auf BERT basiert, wird für die benannte Entitätenerkennung (NER) verwendet, und welche Besonderheiten weist sie im Vergleich zu anderen Klassifikationssystemen auf?",
  "reference_answer": "Die Architektur ist die gleiche wie bei BERT-basierten Klassifikationssystemen, mit nur der Änderung der Ausgabeschicht.",
  "reference_context": "Document 404: Unnamed: 0: 404\ntext: The architecture is the same as that of BERT-based classification systems, with only the change of the output layer.\n\nNumber (similarity, evaluation score, etc.)\n\nhcls\n\nh1\n\nh2\n\n...\n\nhm hm+1 hm+2 hm+3\n\n... hlen−1 hlen\n\nBERT\n\necls\n\ne1\n\ne2\n\n...\n\nem em+1 em+2 em+3\n\n...\n\nelen−1 elen\n\n[CLS]\n\nx1\n\nx2\n\n...\n\nxm [SEP]\n\ny1\n\ny2\n\n...\n\nyn\n\n[SEP]\n\nText 1\n\nText 2\n\nFor training or fine-tuning, we can minimize the regression loss of the model output as usual.\n\nSequence Labeling. Sequence labeling is a machine learning approach applicable to a wide range of NLP problems. This approach assigns a label to each token in an input sequence, and some linguistic annotations can then be derived from this sequence of labels. An ex- ample of sequence labeling in NLP is part-of-speech (POS) tagging. We label each word in a sentence with its corresponding POS tag. Another example is named entity recognition (NER) in which we label each word with an NER tag, and named entities are identified using these tags. See below for an illustration of the model architecture for NER.\n\n1.4 Applying BERT Models\n\nTag\n\nTag\n\n{B, I, O}{B, I, O}\n\nTag {B, I, O}\n\nhcls\n\nh1\n\n...\n\nh2 BERT\n\nhm hm+1\n\necls\n\ne1\n\ne2\n\n...\n\nem em+1\n\n[CLS]\n\nx1\n\nx2\n\n...\n\nxm [SEP]\n\nHere {B,I,O} is the tag set of NER.\nref_doc_id: general_surveys\/2501.09223v2.pdf",
  "conversation_history": [],
  "metadata": {
    "question_type": "complex",
    "seed_document_id": 404,
    "topic": "Transformer-based NLP Pre-training"
  }
}
{
  "id": "943061e0-6b51-4fa5-ab70-b1e653132566",
  "question": "Unter welchen Bedingungen wird die Generierung von Anweisungen und Feinabstimmungsproben durch große Sprachmodelle (LLMs) bevorzugt, insbesondere im Hinblick auf die Kosten und die Vielfalt der generierten Daten?",
  "reference_answer": "Die Generierung von Anweisungen und Feinabstimmungsproben durch LLMs wird bevorzugt, da die manuelle Entwicklung solcher Daten so teuer ist, dass sich die meisten Forschungsgruppen dies nicht leisten können.",
  "reference_context": "Document 851: Unnamed: 0: 851\ntext: Below is a prompt template.\n\nYou are provided with a set of input-output samples, each composed of an instruction, a user input, and an output. Please generate a new sample based on these.\n\nSample 1: {instruction1}\n\nInput: {user-input1}\n\nOutput: {output1}\n\nSample 2: {instruction2}\n\nInput: {user-input2}\n\nOutput: {output2}\n\nNew Sample: {new-instruction}\n\nThis newly-generated sample is examined by some heuristic rules (such as filtering out samples or instructions that are similar to those already in the pool). If it passes, the sample and instruction are added to the pool.\n\nThis generation process can be repeated many times to obtain a sufficient number of fine- tuning samples. Note that, above, we just show simple prompt templates for generating instruction and fine-tuning samples. Of course, we can develop better templates to generate more diverse and accurate instruction and fine-tuning samples. For example, for certain tasks like text classification,\n\n165\n\n166\n\nAlignment\n\nthe LLM may tend to produce biased predictions, for example, most generated samples belong to a single class. In such cases, we can adjust the order of generation of different fields. More specifically, we can specify the output (i.e., the class) with some prior, and prompt the LLM to generate user input given both the instruction and the output. This method resembles input inversion, where the LLM generates the input based on the specified output [Longpre et al., 2023].\n\nUsing LLM-generated instructions and fine-tuning samples has been a common method for developing LLMs, especially given that manually developing such data is so expensive that most research groups cannot afford it.\nref_doc_id: general_surveys\/2501.09223v2.pdf",
  "conversation_history": [],
  "metadata": {
    "question_type": "complex",
    "seed_document_id": 851,
    "topic": "LLM Reasoning and Problem Solving"
  }
}
{
  "id": "f6237d56-b4ad-4ea5-bf8c-140a129f75d1",
  "question": "Unter welcher Bedingung wird im Jahr 2023 von A. K. Lampinen und Kollegen das Thema der passiven Lernstrategien in Agenten und Sprachmodellen behandelt?",
  "reference_answer": "Passive learning of active causal strategies in agents and language models.",
  "reference_context": "Document 4254: Unnamed: 0: 4254\ntext: 2023. Causal reasoning and large language models: Opening a new frontier for causality.\n\n[287] P. Lab. 2023. Awesome-Prompt-Engineering. Original-\n\ndate: 2023-02-09T18:22:52Z.\n\n[288] A. K. Lampinen, S. C. Chan, I. Dasgupta, A. J. Nam and J. X. Wang. 2023. Passive learning of active causal strategies in agents and language models. arXiv preprint arXiv:2305.16183.\n\n[289] H. Laurençon, L. Saulnier, T. Wang, C. Akiki, A. V. del Moral, T. L. Scao, L. V. Werra, C. Mou et al. 2022. The big- science ROOTS corpus: A 1.6TB composite multilingual dataset. In Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track.\n\n[290] A. Lazaridou, E. Gribovskaya, W. Stokowiec and N. Grigorev. 2022. Internet-augmented language mod- els through few-shot prompting for open-domain question answering.\n\n[291] A. Lee, B. Miranda and S. Koyejo. 2023. Beyond Scale: the Diversity Coefficient as a Data Quality Metric Demon- strates LLMs are Pre-trained on Formally Diverse Data. ArXiv:2306.13840 [cs].\n\n[292] D. Lee, J. Lee, J.-W. Ha, J.-H.\nref_doc_id: general_surveys\/2307.10169v1.pdf",
  "conversation_history": [],
  "metadata": {
    "question_type": "complex",
    "seed_document_id": 4254,
    "topic": "Large Language Models Research"
  }
}
{
  "id": "0fc34c70-317a-4edf-81cf-c2c62c61eb92",
  "question": "Welche praktischen Anwendungen von großen Sprachmodellen (LLMs) werden im Artikel in den Bereichen Medizin, Bildung, Finanzen, Recht, Politik, Medien, Unterhaltung oder Ingenieurwesen unter Berücksichtigung von ethischen Bedenken und technischen Herausforderungen demonstriert?",
  "reference_answer": "Der Artikel demonstriert die Anwendungen von LLMs durch vier praktische Anwendungsfälle in den Bereichen Medizin, Bildung, Finanzen, Recht, Politik, Medien, Unterhaltung, Ingenieurwesen und anderen.",
  "reference_context": "Document 4890: Unnamed: 0: 4890\ntext: These aspects are generally not covered by the existing related articles. Our comprehensive examination also encompasses a discussion on the limitations associated with the LLMs, including considerations related to security, ethics, economy, and the environment. In addition, we present a set of guidelines to steer future research and development in the effective use of LLMs. We hope that this paper will contribute to a better understanding and utilization of LLMs.\n\nB. Contributions\n\nThe main contributions of this article are as follows:\n\n1) Providing a comprehensive overview of GenAI and LLMs, including their technical details, advancements, challenges, capabilities and limitations. We present state of the art analysis and comparisons of different LLMs. 2) Addressing ethical concerns about LLMs, including their computational requirements and potential for perpetuat- ing biases. We also discuss the limitations of LLMs; including, limited understanding of the physical world, infomration hallucination, fine- tokenization problems, tuning and risk of foundation models.\n\n3) Offering insights into the future potential of LLMs and their impact on society and demonstrating the applica- tions of LLM through four practical use cases in the fields of medicine, education, finance, law, politics, media, entertainment, engineering, and others.\n\n4) This article is uniquely presented in a manner to promote practical usage of LLMs, showcasing the actual LLM outputs to corroborate the discussions.\nref_doc_id: general_surveys\/682263.pdf",
  "conversation_history": [],
  "metadata": {
    "question_type": "complex",
    "seed_document_id": 4890,
    "topic": "Large Language Models (LLMs)"
  }
}
{
  "id": "3ba35b9b-bb33-4aac-a7fe-6f88e86a5cb1",
  "question": "Welche Personen oder Teams waren an der Entwicklung von mT5 beteiligt, und gibt es spezifische Rollen oder Beiträge, die sie geleistet haben?",
  "reference_answer": "L. Xue, N. Constant, A. Roberts, M. Kale, R. Al-Rfou, A. Siddhant, A. Barua und C. Raffel.",
  "reference_context": "Document 4387: Unnamed: 0: 4387\ntext: 2022. ByT5: Towards a token-free future with pre-trained byte-to-byte models. Transactions of the Association for Computational Linguis- tics, 10:291–306.\n\n[631] L. Xue, N. Constant, A. Roberts, M. Kale, R. Al-Rfou, A. Siddhant, A. Barua and C. Raffel. 2021. mT5: A mas- sively multilingual pre-trained text-to-text transformer. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 483–498, Online. Association for Computational Linguistics.\n\n[632] L. Yan, L. Sha, L. Zhao, Y. Li, R. Martinez-Maldonado, G. Chen, X. Li, Y. Jin et al. 2023. Practical and ethical chal- lenges of large language models in education: A systematic literature review.\n\n[633] G. Yang, E. Hu, I. Babuschkin, S. Sidor, X. Liu, D. Farhi, N. Ryder, J. Pachocki et al. 2021. Tuning large neural net- works via zero-shot hyperparameter transfer. Advances in Neural Information Processing Systems, 34:17084–17097.\n\n[634] J. Yang, H. Jin, R. Tang, X. Han, Q. Feng, H. Jiang, B. Yin and X. Hu. 2023. Harnessing the power of llms in practice: A survey on chatgpt and beyond.\nref_doc_id: general_surveys\/2307.10169v1.pdf",
  "conversation_history": [],
  "metadata": {
    "question_type": "complex",
    "seed_document_id": 4387,
    "topic": "Large Language Models"
  }
}
{
  "id": "192d4ddd-11a1-430e-be7c-294f63e23981",
  "question": "Welche Methoden gibt es, um LLMs zur Erstellung von Eingabeaufforderungen zu verwenden, und welche Rolle spielt dabei die Bereitstellung von Eingabe-Ausgabe-Paaren?",
  "reference_answer": "LLMs können zur Erstellung von Eingabeaufforderungen verwendet werden, indem ihnen eine Beschreibung der Aufgabe gegeben wird oder indem sie Beispiele für die Eingabe und Ausgabe der Aufgabe erhalten. Sie können dann die entsprechende Anweisung für die Aufgabe aus den bereitgestellten Eingaben und Ausgaben ableiten.",
  "reference_context": "Document 762: Unnamed: 0: 762\ntext: For example, we can directly instruct LLMs to produce prompts, providing them with a description of the task. You are given a task to complete using LLMs. Please write a prompt to guide the LLMs.\n\n{∗task-description∗}\n\nThis method is straightforward, but it still requires a human-provided description of the task. An alternative method is to use LLMs to generate prompts given examples of the input and output of the task. Here is a prompt template.\n\nYou are provided with several input-output pairs for a task. Please write an instruction for performing this task.\n\nInput: {∗input1∗} Output: {∗output1∗} Input: {∗input2∗} Output: {∗output2∗} ...\n\nAs such, LLMs can infer the corresponding instruction for the task from the provided inputs and outputs.\n\nEvaluation. Once we obtain the candidate pool C, we need to evaluate the prompts in C. One method is to feed each prompt into an LLM and assess the results on the downstream\n\n3.3 Learning to Prompt\n\ntask. For example, we can evaluate the output of the LLM given an input using a pre-defined metric, or alternatively, use the log-likelihood of the output as a measure of the quality of the prompt.\n\nPruning. If C contains a large number of prompts, it is reasonable to prune the unpromising prompts within it, thus reducing the computational burden in subsequent steps. This is a standard pruning problem. Given the evaluation score for each prompt, a simple method is to keep only a certain percentage of the prompts and discard the rest.\n\nExpansion. Expansion is a key operation in search algorithms used to explore different\n\nstates in the search space.\nref_doc_id: general_surveys\/2501.09223v2.pdf",
  "conversation_history": [],
  "metadata": {
    "question_type": "complex",
    "seed_document_id": 762,
    "topic": "Others"
  }
}
{
  "id": "a13ba0aa-3b0b-411a-9cd9-690f61d5a071",
  "question": "Welche spezifischen Aufgaben kann der Code Interpreter Plugin von OpenAI ausführen, und welche Bedingungen müssen erfüllt sein, damit er Python-Code basierend auf einer menschlichen Sprachbeschreibung generieren kann?",
  "reference_answer": "Der Code Interpreter ist ein integrierter Python-Code-Interpreter, der für logische Berechnungen sowie zum Schreiben von Code verwendet werden kann.",
  "reference_context": "Document 5095: Unnamed: 0: 5095\ntext: This plugin can be used to enable ChatGPT\n\n7https:\/\/openai.com\/blog\/ChatGPT-plugins 8https:\/\/github.com\/openai\/ChatGPT-retrieval-plugin\n\n31\n\nTABLE VIII: Publicly available AI \/LLM tools\n\nTools ChatGPT\n\nFunction Conversational AI Chatbot\n\nLink ChatGPT\n\nAvailability Both\n\nRoomGPT\n\nRedesign your room in eight different themes\n\nRoomGPT\n\nPublic\n\nHomeGPT\n\nRedesign your home and office\n\nHomeGPT\n\nSubscription based\n\nPDFGPT.IO\n\nTurns PDF into the knowledge base for a ChatGPT type interface\n\nPDFGPT\n\nSubscription based\n\nTexGPT\n\nHarnesses GPT-3’s power to help you write in Overleaf\n\nTexGPT\n\nPublic\n\nAcademicGPT\n\nAn AI tool to write and review scientific papers, critical analysis and explanation of complex concepts\n\nAcademicGPT\n\nPublic\n\nDiagramGPT\n\nAn AI tool for creating scientific diagrams and flow charts of different processes\n\nDiagramGPT\n\nPublic\n\nAutoGPT\n\nAuto-prompting without the user intervention\n\nAutoGPT\n\nPublic\n\nHuggingGPT [557] A framework to connect various AI models to solve AI\n\nHuggingGPT\n\nPublic\n\ntasks\n\nXrayGPT [558]\n\nAutomated analysis of chest radiographs\n\nXrayGPT\n\nPublic\n\nVideo-ChatGPT\n\nA vision language model for video understanding and conservation about videos\n\nVideo-ChatGPT\n\nPublic\n\nClimateGPT\n\nLarge language model for a conversation about the cli- mate in English and Arabic\n\nClimateGPT\n\nPublic\n\nCodeGPT\n\nAn AI assistant for coding\n\nCodeGPT\n\nPublic\n\nCode Llama\n\nOpen Foundation Models to generate and discuss code\n\nCode Llama\n\nPublic\n\nMiniGPT-4 [335] Multi-modal model for a number of tasks,\nref_doc_id: general_surveys\/682263.pdf\n\nDocument 5094: Unnamed: 0: 5094\ntext: The power that plugins provide in terms of flexibility to develop new applications has drawn a big attention towards plugin de- velopment. Apart from the above-mentioned early developers, two notable plugins already made available by OpenAI are the Code interpreter and the knowledge-based retrieval plugin.\n\nCode Interpreter: The Code interpreter is a built-in Python code interpreter which can be used for performing logical calculations as well as writing code. The inter- preter can use the language model’s understanding of a human language description of a problem and use that as input to develop Python code for the problem’s solution. • Knowledge-base retrieval: A knowledge-based retrieval plugin has also been open-sourced8 which can be used by developers. This plugin can be used to enable ChatGPT\n\n7https:\/\/openai.com\/blog\/ChatGPT-plugins 8https:\/\/github.com\/openai\/ChatGPT-retrieval-plugin\n\n31\n\nTABLE VIII: Publicly available AI \/LLM tools\n\nTools ChatGPT\n\nFunction Conversational AI Chatbot\n\nLink ChatGPT\n\nAvailability Both\n\nRoomGPT\n\nRedesign your room in eight different themes\n\nRoomGPT\n\nPublic\n\nHomeGPT\n\nRedesign your home and office\n\nHomeGPT\n\nSubscription based\n\nPDFGPT.IO\n\nTurns PDF into the knowledge base for a ChatGPT type interface\n\nPDFGPT\n\nSubscription based\n\nTexGPT\n\nHarnesses GPT-3’s power to help you write in Overleaf\n\nTexGPT\n\nPublic\n\nAcademicGPT\n\nAn AI tool to write and review scientific papers,\nref_doc_id: general_surveys\/682263.pdf",
  "conversation_history": [],
  "metadata": {
    "question_type": "complex",
    "seed_document_id": 5095,
    "topic": "Large Language Models and Chatbots"
  }
}
{
  "id": "d124845e-abda-46bd-8f38-98dde490f78b",
  "question": "Welche spezifische Technik könnte verwendet werden, um die ursprüngliche Abfrage zu optimieren und sicherzustellen, dass mehr relevante Dokumente abgerufen werden, insbesondere wenn die ursprüngliche Abfrage mehrdeutig ist?",
  "reference_answer": "Die Technik der Abfrageumschreibung (query rewriting) kann verwendet werden, um den Inhalt der Abfrage zu ändern, um wichtige Informationen hervorzuheben und potenzielle Unklarheiten zu beseitigen, was das Abrufen verwandter Dokumente erleichtert.",
  "reference_context": "Document 2023: Unnamed: 0: 2023\ntext: As a solution, the documents returned during the re- trieval stage can be reranked according to their relevance to the input [1036], filtering out low-quality or irrelevant doc- uments or placing less relevant documents in non-optimal positions within the prompt. Furthermore, both generation and reranking tasks [1027] can be jointly optimized to facil- iate better utilize of context documents. Additionally, LLMs\n\nRetrieval results refinement.\n\n89\n\ncan be directly used for document re-ranking by designing specific prompts or using context examples to accomplish this task [777]. In addition to document filtering or rerank- information extraction or automatic summarization ing, techniques can be employed to refine the retrieved content by extracting more concise and query-relevant content from the retrieved documents. Furthermore, existing research has proposed token-level compression strategies [1037], which select important tokens and remove unimportant parts from the candidate documents.\n\nIterative retrieval enhancement. In some complex appli- cation scenarios, a single retrieval procedure may not suffice for RAG systems. To address this issue, we can further use iterative retrieval augmentation and adaptive retrieval aug- mentation. Iterative retrieval augmentation aims to itera- tively refine the initial query based on the model’s generated results to achieve a comprehensive coverage of the required information. As it involves accumulating multiple rounds of retrieval information, the performance of RAG systems may be affected by redundant or conflicting information. To address this issue, stop mechanism has been introduced for retrieval iteration, using the LLM to evaluate the confidence of the current generation results to determine whether to continue the iteration process [662]. Additionally, for more complex scenarios, iterative retrieval can be combined with the LLM’s own CoT reasoning capability.\nref_doc_id: general_surveys\/2303.18223v16.pdf\n\nDocument 2022: Unnamed: 0: 2022\ntext: As another query enhancment technique, query rewriting focuses on modifying the query content to highlight key information and eliminate potential ambiguities, facilitating the retrieval of related documents [1033]. LLMs can be ap- plied directly to query rewriting, transforming the original query into a more suitable form through well-designed prompts [1034]. To reduce inference overhead, the query optimization capabilities of LLMs can also be transferred to smaller models through knowledge distillation [1035].\n\nIn addition to the initial retrieval methods, the refinement of retrieval results also plays an important role in RAG systems, since the retrieved documents may be not best suited for RAG systems, e.g., LLMs might have difficulty in utilizing long contexts or be affected by irrelevant information in the retrieved docu- ments. As a solution, the documents returned during the re- trieval stage can be reranked according to their relevance to the input [1036], filtering out low-quality or irrelevant doc- uments or placing less relevant documents in non-optimal positions within the prompt. Furthermore, both generation and reranking tasks [1027] can be jointly optimized to facil- iate better utilize of context documents. Additionally, LLMs\n\nRetrieval results refinement.\n\n89\n\ncan be directly used for document re-ranking by designing specific prompts or using context examples to accomplish this task [777]. In addition to document filtering or rerank- information extraction or automatic summarization ing, techniques can be employed to refine the retrieved content by extracting more concise and query-relevant content from the retrieved documents. Furthermore, existing research has proposed token-level compression strategies [1037], which select important tokens and remove unimportant parts from the candidate documents.\n\nIterative retrieval enhancement.\nref_doc_id: general_surveys\/2303.18223v16.pdf",
  "conversation_history": [],
  "metadata": {
    "question_type": "complex",
    "seed_document_id": 2023,
    "topic": "Retrieval-Augmented Language Models"
  }
}
{
  "id": "b2cfc408-5542-4b6c-babe-acdb42ec6e24",
  "question": "Welche spezifische Metrik wird verwendet, um die Zeitspanne zu messen, die vom Beginn einer Anfrage bis zur Generierung des ersten Tokens einer Antwort vergeht, unter der Bedingung, dass die Datenübertragung nicht viel Zeit in Anspruch nimmt?",
  "reference_answer": "Time to First Token (TTFT).",
  "reference_context": "Document 1056: Unnamed: 0: 1056\ntext: Additionally, usability can be assessed by measuring how well the generated outputs align with user expec- tations in terms of fluency, coherence, relevance, and diversity. Human evaluators can rate the naturalness of the text or assess whether the responses are contextually appropriate and logically consistent. Ethical and fairness metrics can also be included to ensure LLMs avoid perpetuating biases or generating harmful content.\n\n221\n\n222\n\nInference\n\nAll of the evaluation metrics mentioned above essentially focus on assessing the quality of the outputs. Given the high cost of deploying and applying LLMs, efficiency metrics are also very important for practitioners. Below are some commonly used efficiency metrics [Nvidia, 2025]:\n\nRequest Latency. This metric measures the total time taken from when a request is sent to the LLM until the complete response is received. This includes the time taken for data transmission, processing by the model, and the return of the output to the user.\n\nThroughput. It refers to the number of tokens or requests the model can process per second.\n\nTime to First Token (TTFT). This metric measures the time it takes from the beginning of a request being sent to the generation of the first token of the response. If data transmission does not consume too much time, then TTFT is mainly the time for prefilling and predicting the first token.\n\nInter-token Latency (ITL). This metric refers to the time taken to generate each subsequent\n\ntoken after the first one. It reflects the efficiency of the decoding process.\n\nTokens Per Second (TPS). This metric quantifies the number of tokens that the model can\n\ngenerate per second.\n\nResource Utilization. This involves measuring the computational resource usage (e.g.,\n\nCPU and GPU utilization) and memory consumption of the model during inference.\nref_doc_id: general_surveys\/2501.09223v2.pdf",
  "conversation_history": [],
  "metadata": {
    "question_type": "complex",
    "seed_document_id": 1056,
    "topic": "LLM Evaluation and Benchmarking"
  }
}
{
  "id": "8714dc49-3458-487f-847d-58ea19bb938d",
  "question": "Gemäß der Gleichung (5.1) im Dokument, welches Ziel verfolgt die LLM-Inferenz, wenn man die Bedingung Pr(y|x) maximieren möchte, und welche Methode wird dabei bevorzugt, um das Problem zu lösen, ohne auf Sequenz-zu-Sequenz-Modelle zurückzugreifen?",
  "reference_answer": "Das Ziel der LLM-Inferenz ist es, Pr(y|x) zu maximieren.",
  "reference_context": "Document 998: Unnamed: 0: 998\ntext: As described in Eq. (5.1), the goal of LLM inference is to maximize Pr(y|x). Modeling this conditional probability is common in NLP. At first glance, it seems to be a sequence-to-sequence problem, where we transform a sequence into another using encoding-decoding models. How- ever, we are not discussing sequence-to-sequence problems or encoding-decoding architectures. Instead, as discussed in earlier chapters, this modeling problem can be addressed by using decoder- only models. To do this, we can interpret the log-scale probability logPr(y|x) as the difference between logPr([x,y]) and logPr(x)\n\nlogPr(y|x) = logPr([x,y]) − logPr(x)\n\nwhere logPr([x,y]) and logPr(x) can be obtained by running the LLM on the sequences [x,y] and x, respectively. For example, we can calculate the probability of generating x using the chain\n\nInference\n\n(5.2)\n\n5.1 Prefilling and Decoding\n\nrule\n\nlogPr(x) = logPr(x0...xm)\n\n= log (cid:2)Pr(x0)Pr(x1|x0)···Pr(xm|x0...xm−1)(cid:3)\n\n= logPr(x0) }\n\n|\n\n{z =0\n\n+\n\nm X\n\nj=1\n\nlogPr(xj|x<j)\n\n=\n\nm X\n\nlogPr(xj|x<j)\n\nj=1\n\nIn other words, we calculate the token prediction log-probability at each position of x, and sum all these log-probabilities.\nref_doc_id: general_surveys\/2501.09223v2.pdf",
  "conversation_history": [],
  "metadata": {
    "question_type": "complex",
    "seed_document_id": 998,
    "topic": "LLM Inference Optimization"
  }
}
{
  "id": "73b77dba-8535-4f19-a5bd-a0d162a95ebf",
  "question": "Welche Vorteile bietet die Nutzung eines Key-Value-Datenspeichers bei der effizienten Wiederverwendung von Präfixen, insbesondere im Hinblick auf die Verwaltung von Speicherplatz und die Vermeidung der Neuberechnung versteckter Zustände?",
  "reference_answer": "Die Verwendung eines Key-Value-Datenspeichers ermöglicht es, häufig vorkommende Präfixe ihren vorkomputierten KV-Caches zuzuordnen, was einen konstantzeitlichen Zugriff auf die zwischengespeicherten Zustände erlaubt.",
  "reference_context": "Document 1061: Unnamed: 0: 1061\ntext: Thus, we obtain a sequence of prefixes and their corresponding KV cache states:\n\nx0 (x<1) ⇒ cache<1 x0x1 (x<2) ⇒ cache<2\n\n...\n\nx0x1...xm−1 (x<m) ⇒ cache<m\n\nwhere cache<i denotes the KV cache for the prefix x<i (see also Eq. (5.10)). All these mappings can be stored in the prefix cache for efficient reuse.\n\nWhen processing a new sequence that shares a common prefix with a previously seen sequence in D, we can load the corresponding cached hidden states instead of recomputing them. Specifi- cally, if a new input x′ has x<k (i.e., x′ <k = x<k for some k ≤ m), we can initialize the KV cache with cache<k and only compute the hidden states for the remaining tokens x′\n\n≥k.\n\nAs usual, we can maintain a key-value datastore that maps frequently encountered prefixes to their precomputed KV caches. The lookup can be performed using a hash of the prefix tokens, allowing constant-time access to the cached states. Care must be taken to manage memory usage, as storing all possible prefixes may be infeasible for large datasets. Practical systems often employ least recently used (LRU) caching methods or other strategies to balance between computational savings and memory constraints.\n\n5.2.2 Batching\n\nBatching in LLM inference refers to the process of processing multiple input sequences simultane- ously as a group (called a batch) rather than one at a time. Because modern GPUs excel at parallel processing, batching allows them to compute multiple sequences in a single forward pass, keeping the hardware fully occupied.\nref_doc_id: general_surveys\/2501.09223v2.pdf",
  "conversation_history": [],
  "metadata": {
    "question_type": "complex",
    "seed_document_id": 1061,
    "topic": "Efficient Inference and Batching Techniques"
  }
}
{
  "id": "6668b91e-7748-4164-b35b-6a1fbfd6836f",
  "question": "Welche Modelle, die als Decoder fungieren, sind in der Liste der großen Sprachmodelle enthalten, und welche spezifischen Eigenschaften oder Verwendungszwecke zeichnen sie aus?",
  "reference_answer": "PaLM, LaMDA, OPT, InstructGPT, Sparrow, BLOOM, ExT5",
  "reference_context": "Document 239: Unnamed: 0: 239\ntext: com\/huggingface\/transformers BART [34] Transformer https:\/\/github.com\/lucidrains\/PaLM-pytorch Decoder PaLM [209] Decoder Gopher [39] - Transformer Encoder-Decoder Transformer https:\/\/github.com\/tensorflow\/mesh Switch [67] Decoder LaMDA [236] Decoder OPT [45] Decoder InstructGPT [10] Decoder Sparrow [46] Decoder BLOOM [64] Decoder MT-NLG [66] Encoder-Decoder Transformer HTLM [69] Encoder-Decoder Transformer https:\/\/github.com\/amazon-research\/dq-bart DQ-BART [70] Encoder-Decoder Transformer https:\/\/github.com\/google-research\/text-to-text-transfer-transformer ExT5 [68] Decoder LLaMA [277] Table 2. Major natural language models.\n\nBackbone Word2Vec Word2Vec LSTM Word2Vec LSTM Transformer https:\/\/github.com\/huggingface\/transformers Transformer https:\/\/github.com\/...\/ERNIE Transformer https:\/\/github.com\/...\/transformer-xl Transformer https:\/\/github.com\/...\/unilm Transformer https:\/\/github.com\/google-research\/bert Transformer https:\/\/github.com\/pytorch\/fairseq Transformer https:\/\/github.com\/zihangdai\/xlnet Transformer https:\/\/github.com\/huggingface\/transformers Transformer https:\/\/github.com\/microsoft\/MASS Transformer Transformer https:\/\/github.com\/...\/kb Transformer https:\/\/github.com\/openai\/gpt-2\n\nCode https:\/\/github.com\/...\/models https:\/\/github.com\/...\/GloVe - https:\/\/github.com\/...\/fastText https:\/\/allennlp.org\/elmo\n\nICML ICLR EMNLP - JMLR -\nref_doc_id: general_surveys\/2303.04226v1.pdf\n\nDocument 238: Unnamed: 0: 238\ntext: com\/google-research\/text-to-text-transfer-transformer T5 [56] General Megatron [65] Encoder fastBERT [272] Encoder spanBERT [273] Encoder Reformer [274] Encoder TinyBERT [275] Encoder ALBERT [276] Encoder ELECTRA [213] Decoder GPT-3 [245] Encoder-Decoder Transformer https:\/\/github.com\/huggingface\/transformers BART [34] Transformer https:\/\/github.com\/lucidrains\/PaLM-pytorch Decoder PaLM [209] Decoder Gopher [39] - Transformer Encoder-Decoder Transformer https:\/\/github.com\/tensorflow\/mesh Switch [67] Decoder LaMDA [236] Decoder OPT [45] Decoder InstructGPT [10] Decoder Sparrow [46] Decoder BLOOM [64] Decoder MT-NLG [66] Encoder-Decoder Transformer HTLM [69] Encoder-Decoder Transformer https:\/\/github.com\/amazon-research\/dq-bart DQ-BART [70] Encoder-Decoder Transformer https:\/\/github.com\/google-research\/text-to-text-transfer-transformer ExT5 [68] Decoder LLaMA [277] Table 2. Major natural language models.\nref_doc_id: general_surveys\/2303.04226v1.pdf",
  "conversation_history": [],
  "metadata": {
    "question_type": "complex",
    "seed_document_id": 239,
    "topic": "Others"
  }
}
